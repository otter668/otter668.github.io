<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">


  <link rel="manifest" href="/images/manifest.json">


  <meta name="msapplication-config" content="/images/browserconfig.xml" />



  <meta name="keywords" content="DeepLearning,RNN," />





  <link rel="alternate" href="/atom.xml" title="NLPer" type="application/atom+xml" />






<meta name="description" content="循环神经网络 往期回顾 在前面的文章系列文章中，我们介绍了全连接神经网络和卷积神经网络，以及它们的训练和使用。他们都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不">
<meta name="keywords" content="DeepLearning,RNN">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习系列（5）">
<meta property="og:url" content="http://otter668@github.io/2018/01/08/循环神经网络/index.html">
<meta property="og:site_name" content="NLPer">
<meta property="og:description" content="循环神经网络 往期回顾 在前面的文章系列文章中，我们介绍了全连接神经网络和卷积神经网络，以及它们的训练和使用。他们都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153794987536.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153797340945.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153801003416.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153808522012.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153809848245.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153820155319.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153952393256.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153953639317.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153954152014.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153954619609.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15153973768854.jpg">
<meta property="og:updated_time" content="2018-01-08T07:54:40.802Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习系列（5）">
<meta name="twitter:description" content="循环神经网络 往期回顾 在前面的文章系列文章中，我们介绍了全连接神经网络和卷积神经网络，以及它们的训练和使用。他们都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不">
<meta name="twitter:image" content="http://p082waf5e.bkt.clouddn.com/15153794987536.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://otter668@github.io/2018/01/08/循环神经网络/"/>





  <title>深度学习系列（5） | NLPer</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">NLPer</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">coding</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://otter668@github.io/2018/01/08/循环神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Gaochao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars1.githubusercontent.com/u/2328224?s=400&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NLPer">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习系列（5）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-08T10:40:03+08:00">
                2018-01-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/科研/" itemprop="url" rel="index">
                    <span itemprop="name">科研</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  9,852
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  43
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="循环神经网络">循环神经网络</h1>
<h2 id="往期回顾">往期回顾</h2>
<p>在前面的文章系列文章中，我们介绍了全连接神经网络和卷积神经网络，以及它们的训练和使用。他们都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。这时，就需要用到深度学习领域中另一类非常重要神经网络：<strong>循环神经网络(Recurrent Neural Network)</strong>。RNN种类很多，也比较绕脑子。不过读者不用担心，本文将一如既往的对复杂的东西剥茧抽丝，帮助您理解RNNs以及它的训练算法，并动手实现一个<strong>循环神经网络</strong>。</p>
<a id="more"></a>
<h2 id="语言模型">语言模型</h2>
<p>RNN是在<strong>自然语言处理</strong>领域中最先被用起来的，比如，RNN可以为<strong>语言模型</strong>来建模。那么，什么是语言模型呢？</p>
<p>我们可以和电脑玩一个游戏，我们写出一个句子前面的一些词，然后，让电脑帮我们写下接下来的一个词。比如下面这句：</p>
<blockquote>
<p>我昨天上学迟到了，老师批评了____。</p>
</blockquote>
<p>我们给电脑展示了这句话前面这些词，然后，让电脑写下接下来的一个词。在这个例子中，接下来的这个词最有可能是『我』，而不太可能是『小明』，甚至是『吃饭』。</p>
<p><strong>语言模型</strong>就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。</p>
<p><strong>语言模型</strong>是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要<strong>语言模型</strong>来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。</p>
<p>使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的<strong>概率</strong>只与前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词：</p>
<blockquote>
<p>我 昨天 上学 迟到 了 ，老师 批评 了 ____。</p>
</blockquote>
<p>如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『了』，然后，电脑会在语料库中，搜索『了』后面最可能的一个词。不管最后电脑选的是不是『我』，我们都知道这个模型是不靠谱的，因为『了』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『批评了』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『我』，远在9个词之前！</p>
<p>现在读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram…….。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用<del>海量的存储空间</del>。</p>
<p>所以，该轮到RNN出场了，RNN理论上可以往前看(往后看)任意多个词。</p>
<h2 id="循环神经网络是啥">循环神经网络是啥</h2>
<p>循环神经网络种类繁多，我们先从最简单的基本循环神经网络开始吧。</p>
<h3 id="基本循环神经网络">基本循环神经网络</h3>
<p>下图是一个简单的循环神经网络图，它由输入层、一个隐藏层和一个输出层组成</p>
<figure>
<img src="http://p082waf5e.bkt.clouddn.com/15153794987536.jpg" alt="循环神经网络"><figcaption>循环神经网络</figcaption>
</figure>
<p>纳尼？！相信第一次看到这个玩意的读者内心和我一样是崩溃的。因为<strong>循环神经网络</strong>实在是太难画出来了，网上所有大神们都不得不用了这种抽象艺术手法。不过，静下心来仔细看看的话，其实也是很好理解的。如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的<strong>全连接神经网络</strong>。<span class="math inline">\(\mathrm{x}\)</span>是一个向量，它表示<strong>输入层</strong>的值（这里面没有画出来表示神经元节点的圆圈）；<span class="math inline">\(\mathrm{s}\)</span>是一个向量，它表示<strong>隐藏层</strong>的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量<span class="math inline">\(\mathrm{s}\)</span>的维度相同）；U是输入层到隐藏层的<strong>权重矩阵</strong>（读者可以回到<a href="https://otter668.github.io/2017/12/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">深度学习系列（3）</a>，看看我们是怎样用矩阵来表示<strong>全连接神经网络</strong>的计算的）；<span class="math inline">\(\mathrm{o}\)</span>也是一个向量，它表示<strong>输出层</strong>的值；V是隐藏层到输出层的<strong>权重矩阵</strong>。那么，现在我们来看看W是什么。<strong>循环神经网络</strong>的<strong>隐藏层</strong>的值<span class="math inline">\(\mathrm{s}\)</span>不仅仅取决于当前这次的输入<span class="math inline">\(\mathrm{x}\)</span>，还取决于上一次<strong>隐藏层</strong>的值<span class="math inline">\(\mathrm{s}\)</span>。<strong>权重矩阵</strong> W就是<strong>隐藏层上一次</strong>的值作为这一次的输入的权重。</p>
<p>如果我们把上面的图展开，<strong>循环神经网络</strong>也可以画成下面这个样子：</p>
<figure>
<img src="http://p082waf5e.bkt.clouddn.com/15153797340945.jpg" alt="循环神经网络"><figcaption>循环神经网络</figcaption>
</figure>
<p>现在看上去就比较清楚了，这个网络在t时刻接收到输入<span class="math inline">\(\mathrm{x}_t\)</span>之后，隐藏层的值是<span class="math inline">\(\mathrm{s}_t\)</span>，输出值是<span class="math inline">\(\mathrm{o}_t\)</span>。关键一点是，<span class="math inline">\(\mathrm{s}_t\)</span>的值不仅仅取决于<span class="math inline">\(\mathrm{x}_t\)</span>，还取决于<span class="math inline">\(\mathrm{s}_{t-1}\)</span>。我们可以用下面的公式来表示循环神经网络的计算方法：</p>
<p><span class="math display">\[\begin{align}
\mathrm{o}_t&amp;=g(V\mathrm{s}_t)\qquad\qquad\quad(式1)\\
\mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})\qquad(式2)\\
\end{align}\]</span></p>
<p>式1是<strong>输出层</strong>的计算公式，输出层是一个<strong>全连接层</strong>，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的<strong>权重矩阵</strong>，g是<strong>激活函数</strong>。式2是<strong>隐藏层</strong>的计算公式，它是<strong>循环层</strong>。U是输入x的权重矩阵，W是上一次的值<span class="math inline">\(\mathrm{s}_{t-1}\)</span>作为这一次的输入的<strong>权重矩阵</strong>，f是<strong>激活函数</strong>。</p>
<p>从上面的公式我们可以看出，<strong>循环层</strong>和<strong>全连接层</strong>的区别就是<strong>循环层</strong>多了一个<strong>权重矩阵</strong> W。</p>
<p>如果反复<strong>把式2</strong>带入到<strong>式1</strong>，我们将得到：</p>
<p><span class="math display">\[\begin{align}
\mathrm{o}_t&amp;=g(V\mathrm{s}_t)\\
&amp;=Vf(U\mathrm{x}_t+W\mathrm{s}_{t-1})\\
&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+W\mathrm{s}_{t-2}))\\
&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+W\mathrm{s}_{t-3})))\\
&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+Wf(U\mathrm{x}_{t-3}+...))))
\end{align}\]</span></p>
<p>从上面可以看出，<strong>循环神经网络</strong>的输出值<span class="math inline">\(\mathrm{o}_t\)</span>，是受前面历次输入值<span class="math inline">\(\mathrm{x}_t\)</span>、<span class="math inline">\(\mathrm{x}_{t-1}\)</span>、<span class="math inline">\(\mathrm{x}_{t-2}\)</span>、<span class="math inline">\(\mathrm{x}_{t-3}\)</span>、…影响的，这就是为什么<strong>循环神经网络</strong>可以<strong>往前看任意多</strong>个<strong>输入值</strong>的原因。</p>
<h3 id="双向循环神经网络">双向循环神经网络</h3>
<p>对于<strong>语言模型</strong>来说，很多时候光看前面的词是不够的，比如下面这句话：</p>
<blockquote>
<p>我的手机坏了，我打算____一部新手机。</p>
</blockquote>
<p>可以想象，如果我们只看横线前面的词，手机坏了，那么我是打算修一修？换一部新的？还是大哭一场？这些都是无法确定的。但如果我们也看到了横线后面的词是『一部新手机』，那么，横线上的词填『买』的概率就大得多了。</p>
<p>在上一小节中的<strong>基本循环神经</strong>网络是无法对此进行建模的，因此，我们需要<strong>双向循环神经网络</strong>，如下图所示：</p>
<figure>
<img src="http://p082waf5e.bkt.clouddn.com/15153801003416.jpg" alt="双向循环神经网络"><figcaption>双向循环神经网络</figcaption>
</figure>
<p>当遇到这种从未来穿越回来的场景时，难免处于懵逼的状态。不过我们还是可以用屡试不爽的老办法：先分析一个特殊场景，然后再总结一般规律。我们先考虑上图中，<span class="math inline">\(\mathrm{y}_2\)</span>的计算。</p>
<p>从上图可以看出，<strong>双向卷积神经网络</strong>的隐藏层要保存两个值，一个<span class="math inline">\(A\)</span>参与正向计算，另一个值<span class="math inline">\(A&#39;\)</span>参与反向计算。最终的输出值<span class="math inline">\(\mathrm{y}_2\)</span>取决于<span class="math inline">\(A_2\)</span>和<span class="math inline">\(A&#39;_2\)</span>。其计算方法为：</p>
<p><span class="math display">\[\mathrm{y}_2=g(VA_2+V&#39;A_2&#39;)\]</span></p>
<p><span class="math inline">\(A_2\)</span>和<span class="math inline">\(A&#39;_2\)</span>则分别计算：</p>
<p><span class="math display">\[\begin{align}
A_2&amp;=f(WA_1+U\mathrm{x}_2)\\
A_2&#39;&amp;=f(W&#39;A_3&#39;+U&#39;\mathrm{x}_2)\\
\end{align}\]</span></p>
<p>现在，我们已经可以看出一般的规律：正向计算时，隐藏层的值<span class="math inline">\(\mathrm{s}_t\)</span>与<span class="math inline">\(\mathrm{s}_{t-1}\)</span>有关；反向计算时，隐藏层的值<span class="math inline">\(\mathrm{s}&#39;_t\)</span>与<span class="math inline">\(\mathrm{s}&#39;_{t-1}\)</span>有关；最终的输出取决于正向和反向计算的<strong>加和</strong>。现在，我们仿照<strong>式1</strong>和<strong>式2</strong>，写出<strong>双向循环神经网络</strong>的计算方法：</p>
<p><span class="math display">\[\begin{align}
\mathrm{o}_t&amp;=g(V\mathrm{s}_t+V&#39;\mathrm{s}_t&#39;)\\
\mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})\\
\mathrm{s}_t&#39;&amp;=f(U&#39;\mathrm{x}_t+W&#39;\mathrm{s}_{t+1}&#39;)\\
\end{align}\]</span></p>
<p>从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。</p>
<h3 id="深度循环神经网络">深度循环神经网络</h3>
<p>前面我们介绍的<strong>循环神经网络</strong>只有一个隐藏层，我们当然也可以<strong>堆叠</strong>两个以上的隐藏层，这样就得到了<strong>深度循环神经网络</strong>。如下图所示：</p>
<figure>
<img src="http://p082waf5e.bkt.clouddn.com/15153808522012.jpg" alt="深度循环神经网络"><figcaption>深度循环神经网络</figcaption>
</figure>
<p>我们把第i个隐藏层的值表示为<span class="math inline">\(\mathrm{s}^{(i)}_t\)</span>、<span class="math inline">\(\mathrm{s}&#39;^{(i)}_t\)</span>，则<strong>深度循环神经网络</strong>的计算方式可以表示为：</p>
<p><span class="math display">\[\begin{align}
\mathrm{o}_t&amp;=g(V^{(i)}\mathrm{s}_t^{(i)}+V&#39;^{(i)}\mathrm{s}_t&#39;^{(i)})\\
\mathrm{s}_t^{(i)}&amp;=f(U^{(i)}\mathrm{s}_t^{(i-1)}+W^{(i)}\mathrm{s}_{t-1})\\
\mathrm{s}_t&#39;^{(i)}&amp;=f(U&#39;^{(i)}\mathrm{s}_t&#39;^{(i-1)}+W&#39;^{(i)}\mathrm{s}_{t+1}&#39;)\\
...\\
\mathrm{s}_t^{(1)}&amp;=f(U^{(1)}\mathrm{x}_t+W^{(1)}\mathrm{s}_{t-1})\\
\mathrm{s}_t&#39;^{(1)}&amp;=f(U&#39;^{(1)}\mathrm{x}_t+W&#39;^{(1)}\mathrm{s}_{t+1}&#39;)\\
\end{align}\]</span></p>
<h2 id="循环神经网络的训练">循环神经网络的训练</h2>
<h3 id="循环神经网络的训练算法bptt">循环神经网络的训练算法：BPTT</h3>
<p>BPTT算法是针对<strong>循环层</strong>的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p>
<ol type="1">
<li>前向计算每个神经元的输出值；</li>
<li>反向计算每个神经元的<strong>误差项</strong><span class="math inline">\(\delta_j\)</span>值，它是误差函数E对神经元<span class="math inline">\(j\)</span>的<strong>加权输入</strong><span class="math inline">\(\mathrm{net}_j\)</span>的偏导数；</li>
<li>计算每个权重的梯度。</li>
</ol>
<p>最后再用<strong>随机梯度下降</strong>算法更新权重。</p>
<p>循环层如下图所示：</p>
<figure>
<img src="http://p082waf5e.bkt.clouddn.com/15153809848245.jpg" alt="循环层"><figcaption>循环层</figcaption>
</figure>
<h4 id="前向计算">前向计算</h4>
<p>使用前面的<strong>式2</strong>对循环层进行前向计算：</p>
<p><span class="math display">\[\mathrm{s}_t=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})\]</span></p>
<p>注意，上面的<span class="math inline">\(\mathrm{s}_t\)</span>、<span class="math inline">\(\mathrm{x}_t\)</span>、<span class="math inline">\(\mathrm{s}_{t-1}\)</span>都是向量，用<strong>黑体字母</strong>表示；而U、V是<strong>矩阵</strong>，用大写字母表示。<strong>向量的下标</strong>表示时刻，例如，<span class="math inline">\(\mathrm{s}_t\)</span>表示在t时刻向量s的值。</p>
<p>我们假设输入向量<span class="math inline">\(\mathrm{x}\)</span>的维度是m，输出向量<span class="math inline">\(\mathrm{s}\)</span>的维度是n，则矩阵U的维度是<span class="math inline">\(n\times m\)</span>，矩阵W的维度是<span class="math inline">\(n\times n\)</span>。下面是上式展开成矩阵的样子，看起来更直观一些：</p>
<p><span class="math display">\[\begin{align}
\begin{bmatrix}
s_1^t\\
s_2^t\\
.\\.\\
s_n^t\\
\end{bmatrix}=f(
\begin{bmatrix}
u_{11} u_{12} ... u_{1m}\\
u_{21} u_{22} ... u_{2m}\\
.\\.\\
u_{n1} u_{n2} ... u_{nm}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
.\\.\\
x_m\\
\end{bmatrix}+
\begin{bmatrix}
w_{11} w_{12} ... w_{1n}\\
w_{21} w_{22} ... w_{2n}\\
.\\.\\
w_{n1} w_{n2} ... w_{nn}\\
\end{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
s_2^{t-1}\\
.\\.\\
s_n^{t-1}\\
\end{bmatrix})
\end{align}\]</span></p>
<p>在这里我们用<strong>手写体字母</strong>表示向量的一个<strong>元素</strong>，它的下标表示它是这个向量的第几个元素，它的上标表示第几个时刻。例如，<span class="math inline">\(s_j^t\)</span>表示向量s的第j个元素在t时刻的值。<span class="math inline">\(u_{ji}\)</span>表示<strong>输入层</strong>第<span class="math inline">\(i\)</span>个神经元到<strong>循环层</strong>第<span class="math inline">\(j\)</span>个神经元的权重。<span class="math inline">\(w_{ji}\)</span>表示<strong>循环层</strong>第<span class="math inline">\(t-1\)</span>时刻的第<span class="math inline">\(i\)</span>个神经元到<strong>循环层</strong>第<span class="math inline">\(t\)</span>个时刻的第<span class="math inline">\(j\)</span>个神经元的权重。</p>
<h4 id="误差项的计算">误差项的计算</h4>
<p>BTPP算法将第<span class="math inline">\(l\)</span>层<span class="math inline">\(t\)</span>时刻的<strong>误差项</strong><span class="math inline">\(\delta^l_t\)</span>值沿两个方向传播，一个方向是其传递到上一层网络，得到<span class="math inline">\(\delta^{l-1}_t\)</span>，这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始<span class="math inline">\(t_1\)</span>时刻，得到<span class="math inline">\(\delta^l_1\)</span>，这部分只和权重矩阵W有关。</p>
<p>我们用向量<span class="math inline">\(\mathrm{net}_t\)</span>表示神经元在<span class="math inline">\(t\)</span>时刻的<strong>加权输入</strong>，因为：</p>
<p><span class="math display">\[\begin{align}
\mathrm{net}_t&amp;=U\mathrm{x}_t+W\mathrm{s}_{t-1}\\
\mathrm{s}_{t-1}&amp;=f(\mathrm{net}_{t-1})\\
\end{align}\]</span></p>
<p>因此：</p>
<p><span class="math display">\[\begin{align}
\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}&amp;=\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{s}_{t-1}}}\frac{\partial{\mathrm{s}_{t-1}}}{\partial{\mathrm{net}_{t-1}}}\\
\end{align}\]</span></p>
<p>我们用<span class="math inline">\(\mathrm{a}\)</span>表示列向量，用<span class="math inline">\(\mathrm{a}^T\)</span>表示行向量。上式的第一项是向量函数对向量求导，其结果为Jacobian矩阵：</p>
<p><span class="math display">\[\begin{align}
\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{s}_{t-1}}}&amp;=
\begin{bmatrix}
\frac{\partial{\mathrm{net}_1^t}}{\partial{s_1^{t-1}}}&amp; \frac{\partial{\mathrm{net}_1^t}}{\partial{s_2^{t-1}}}&amp; ...&amp;  \frac{\partial{\mathrm{net}_1^t}}{\partial{s_n^{t-1}}}\\
\frac{\partial{\mathrm{net}_2^t}}{\partial{s_1^{t-1}}}&amp; \frac{\partial{\mathrm{net}_2^t}}{\partial{s_2^{t-1}}}&amp; ...&amp;  \frac{\partial{\mathrm{net}_2^t}}{\partial{s_n^{t-1}}}\\
&amp;.\\&amp;.\\
\frac{\partial{\mathrm{net}_n^t}}{\partial{s_1^{t-1}}}&amp; \frac{\partial{\mathrm{net}_n^t}}{\partial{s_2^{t-1}}}&amp; ...&amp;  \frac{\partial{\mathrm{net}_n^t}}{\partial{s_n^{t-1}}}\\
\end{bmatrix}\\
&amp;=\begin{bmatrix}
w_{11} &amp; w_{12} &amp; ... &amp; w_{1n}\\
w_{21} &amp; w_{22} &amp; ... &amp; w_{2n}\\
&amp;.\\&amp;.\\
w_{n1} &amp; w_{n2} &amp; ... &amp; w_{nn}\\
\end{bmatrix}\\
&amp;=W
\end{align}\]</span></p>
<p>同理，上式第二项也是一个Jacobian矩阵：</p>
<p><span class="math display">\[\begin{align}
\frac{\partial{\mathrm{s}_{t-1}}}{\partial{\mathrm{net}_{t-1}}}&amp;=
\begin{bmatrix}
\frac{\partial{s_1^{t-1}}}{\partial{\mathrm{net}_1^{t-1}}}&amp; \frac{\partial{s_1^{t-1}}}{\partial{\mathrm{net}_2^{t-1}}}&amp; ...&amp;  \frac{\partial{s_1^{t-1}}}{\partial{\mathrm{net}_n^{t-1}}}\\
\frac{\partial{s_2^{t-1}}}{\partial{\mathrm{net}_1^{t-1}}}&amp; \frac{\partial{s_2^{t-1}}}{\partial{\mathrm{net}_2^{t-1}}}&amp; ...&amp;  \frac{\partial{s_2^{t-1}}}{\partial{\mathrm{net}_n^{t-1}}}\\
&amp;.\\&amp;.\\
\frac{\partial{s_n^{t-1}}}{\partial{\mathrm{net}_1^{t-1}}}&amp; \frac{\partial{s_n^{t-1}}}{\partial{\mathrm{net}_2^{t-1}}}&amp; ...&amp;  \frac{\partial{s_n^{t-1}}}{\partial{\mathrm{net}_n^{t-1}}}\\
\end{bmatrix}\\
&amp;=\begin{bmatrix}
f&#39;(\mathrm{net}_1^{t-1}) &amp; 0 &amp; ... &amp; 0\\
0 &amp; f&#39;(\mathrm{net}_2^{t-1}) &amp; ... &amp; 0\\
&amp;.\\&amp;.\\
0 &amp; 0 &amp; ... &amp; f&#39;(\mathrm{net}_n^{t-1})\\
\end{bmatrix}\\
&amp;=diag[f&#39;(\mathrm{net}_{t-1})]
\end{align}\]</span></p>
<p>其中，diag[a]表示根据向量a创建一个对角矩阵，即</p>
<p><span class="math display">\[diag(\mathrm{a})=\begin{bmatrix}
a_1 &amp; 0 &amp; ... &amp; 0\\
0 &amp; a_2 &amp; ... &amp; 0\\
&amp;.\\&amp;.\\
0 &amp; 0 &amp; ... &amp; a_n\\
\end{bmatrix}\\\]</span></p>
<p>最后，将两项合在一起，可得：</p>
<p><span class="math display">\[\begin{align}
\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}&amp;=\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{s}_{t-1}}}\frac{\partial{\mathrm{s}_{t-1}}}{\partial{\mathrm{net}_{t-1}}}\\
&amp;=Wdiag[f&#39;(\mathrm{net}_{t-1})]\\
&amp;=\begin{bmatrix}
w_{11}f&#39;(\mathrm{net}_1^{t-1}) &amp; w_{12}f&#39;(\mathrm{net}_2^{t-1}) &amp; ... &amp; w_{1n}f(\mathrm{net}_n^{t-1})\\
w_{21}f&#39;(\mathrm{net}_1^{t-1}) &amp; w_{22} f&#39;(\mathrm{net}_2^{t-1}) &amp; ... &amp; w_{2n}f(\mathrm{net}_n^{t-1})\\
&amp;.\\&amp;.\\
w_{n1}f&#39;(\mathrm{net}_1^{t-1}) &amp; w_{n2} f&#39;(\mathrm{net}_2^{t-1}) &amp; ... &amp; w_{nn} f&#39;(\mathrm{net}_n^{t-1})\\
\end{bmatrix}\\
\end{align}\]</span></p>
<p>上式描述了将<span class="math inline">\(\delta\)</span>沿时间往前传递一个时刻的规律，有了这个规律，我们就可以求得任意时刻k的<strong>误差项</strong><span class="math inline">\(\delta_k\)</span>：</p>
<p><span class="math display">\[\begin{align}
\delta_k^T=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_k}}\\
=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_k}}\\
=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}\frac{\partial{\mathrm{net}_{t-1}}}{\partial{\mathrm{net}_{t-2}}}...\frac{\partial{\mathrm{net}_{k+1}}}{\partial{\mathrm{net}_{k}}}\\
=&amp;Wdiag[f&#39;(\mathrm{net}_{t-1})]
Wdiag[f&#39;(\mathrm{net}_{t-2})]
...
Wdiag[f&#39;(\mathrm{net}_{k})]
\delta_t^l\\
=&amp;\delta_t^T\prod_{i=k}^{t-1}Wdiag[f&#39;(\mathrm{net}_{i})]\qquad(式3)
\end{align}\]</span></p>
<p><strong>式3</strong>就是将误差项沿时间反向传播的算法。</p>
<p><strong>循环层</strong>将<strong>误差项</strong>反向传递到上一层网络，与普通的<strong>全连接层</strong>是完全一样的，这在前面的文章中<a href="https://otter668.github.io/2017/12/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">深度学习系列（3）</a>已经详细讲过了，在此仅简要描述一下。</p>
<p><strong>循环层</strong>的<strong>加权输入</strong><span class="math inline">\(\mathrm{net}^l\)</span>与上一层的<strong>加权输入</strong><span class="math inline">\(\mathrm{net}^{l-1}\)</span>关系如下：</p>
<p><span class="math display">\[\begin{align}
\mathrm{net}_t^l=&amp;U\mathrm{a}_t^{l-1}+W\mathrm{s}_{t-1}\\
\mathrm{a}_t^{l-1}=&amp;f^{l-1}(\mathrm{net}_t^{l-1})
\end{align}\]</span></p>
<p>上式中<span class="math inline">\(\mathrm{net}^l_t\)</span>是第<span class="math inline">\(l\)</span>层神经元的<strong>加权输入</strong>(假设第<span class="math inline">\(l\)</span>层是<strong>循环层</strong>)；<span class="math inline">\(\mathrm{net}^{l-1}_t\)</span>是第<span class="math inline">\(l-1\)</span>层神经元的<strong>加权输入</strong>；<span class="math inline">\(\mathrm{a}_t^{l-1}\)</span>是第<span class="math inline">\(l-1\)</span>层神经元的输出；<span class="math inline">\(f^{l-1}\)</span>是第<span class="math inline">\(l-1\)</span>层的<strong>激活函数</strong>。</p>
<p><span class="math display">\[\begin{align}
\frac{\partial{\mathrm{net}_t^l}}{\partial{\mathrm{net}_t^{l-1}}}=&amp;\frac{\partial{\mathrm{net}^l}}{\partial{\mathrm{a}_t^{l-1}}}\frac{\partial{\mathrm{a}_t^{l-1}}}{\partial{\mathrm{net}_t^{l-1}}}\\
=&amp;Udiag[f&#39;^{l-1}(\mathrm{net}_t^{l-1})]
\end{align}\]</span></p>
<p>所以，</p>
<p><span class="math display">\[\begin{align}
(\delta_t^{l-1})^T=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t^{l-1}}}\\
=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t^l}}\frac{\partial{\mathrm{net}_t^l}}{\partial{\mathrm{net}_t^{l-1}}}\\
=&amp;(\delta_t^l)^TUdiag[f&#39;^{l-1}(\mathrm{net}_t^{l-1})]\qquad(式4)
\end{align}\]</span></p>
<p><strong>式4</strong>就是将误差项传递到上一层算法。</p>
<h4 id="权重梯度的计算">权重梯度的计算</h4>
<p>现在，我们终于来到了BPTT算法的最后一步：计算每个权重的梯度。</p>
<p>首先，我们计算误差函数E对权重矩阵W的梯度<span class="math inline">\(\frac{\partial{E}}{\partial{W}}\)</span>。</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15153820155319.jpg"></p>
<p>上图展示了我们到目前为止，在前两步中已经计算得到的量，包括每个时刻<span class="math inline">\(t\)</span>循环层的输出值<span class="math inline">\(\mathrm{s}_t\)</span>，以及误差项<span class="math inline">\(\delta_t\)</span>。</p>
<p>回忆一下我们在文章<a href="https://otter668.github.io/2017/12/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">深度学习系列（3）</a>介绍的全连接网络的权重梯度计算算法：只要知道了任意一个时刻的<strong>误差项</strong><span class="math inline">\(\delta_t\)</span>，以及上一个时刻循环层的输出值<span class="math inline">\(\mathrm{s}_{t-1}\)</span>，就可以按照下面的公式求出权重矩阵在<span class="math inline">\(t\)</span>时刻的梯度<span class="math inline">\(\nabla_{Wt}E\)</span>：</p>
<p><span class="math display">\[\nabla_{W_t}E=\begin{bmatrix}
\delta_1^t\mathrm{s}_1^{t-1} &amp; \delta_1^t\mathrm{s}_2^{t-1} &amp; ... &amp;  \delta_1^t\mathrm{s}_n^{t-1}\\
\delta_2^t\mathrm{s}_1^{t-1} &amp; \delta_2^t\mathrm{s}_2^{t-1} &amp; ... &amp;  \delta_2^t\mathrm{s}_n^{t-1}\\
.\\.\\
\delta_n^t\mathrm{s}_1^{t-1} &amp; \delta_n^t\mathrm{s}_2^{t-1} &amp; ... &amp;  \delta_n^t\mathrm{s}_n^{t-1}\\
\end{bmatrix}\qquad(式5)\]</span></p>
<p>在<strong>式5</strong>中，表示<span class="math inline">\(t\)</span>时刻误差项向量的第<span class="math inline">\(i\)</span>个分量；<span class="math inline">\(\mathrm{s}^{t-1}_i\)</span>表示<span class="math inline">\(t-1\)</span>时刻<strong>循环层</strong>第<span class="math inline">\(i\)</span>个神经元的输出值。</p>
<p>我们下面可以简单推导一下式5。</p>
<p>我们知道：</p>
<p><span class="math display">\[\begin{align}
\mathrm{net}_t=&amp;U\mathrm{x}_t+W\mathrm{s}_{t-1}\\
\begin{bmatrix}
\mathrm{net}_1^t\\
\mathrm{net}_2^t\\
.\\.\\
\mathrm{net}_n^t\\
\end{bmatrix}=&amp;U\mathrm{x}_t+
\begin{bmatrix}
w_{11} &amp; w_{12} &amp; ... &amp; w_{1n}\\
w_{21} &amp; w_{22} &amp; ... &amp; w_{2n}\\
.\\.\\
w_{n1} &amp; w_{n2} &amp; ... &amp; w_{nn}\\
\end{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
s_2^{t-1}\\
.\\.\\
s_n^{t-1}\\
\end{bmatrix}\\
=&amp;U\mathrm{x}_t+
\begin{bmatrix}
w_{11}s_1^{t-1}+w_{12}s_2^{t-1}...w_{1n}s_n^{t-1}\\
w_{21}s_1^{t-1}+w_{22}s_2^{t-1}...w_{2n}s_n^{t-1}\\
.\\.\\
w_{n1}s_1^{t-1}+w_{n2}s_2^{t-1}...w_{nn}s_n^{t-1}\\
\end{bmatrix}\\
\end{align}\]</span></p>
<p>因为对W求导与<span class="math inline">\(U\mathrm{x}_t\)</span>无关，我们不再考虑。现在，我们考虑对<strong>权重项</strong><span class="math inline">\(w_{ji}\)</span>求导。通过观察上式我们可以看到<span class="math inline">\(w_{ji}\)</span>只与<span class="math inline">\(\mathrm{net}^t_j\)</span>有关，所以：</p>
<p><span class="math display">\[\begin{align}
\frac{\partial{E}}{\partial{w_{ji}}}=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_j^t}}\frac{\partial{\mathrm{net}_j^t}}{\partial{w_{ji}}}\\
=&amp;\delta_j^ts_i^{t-1}
\end{align}\]</span></p>
<p>按照上面的规律就可以生成<strong>式5</strong>里面的矩阵。</p>
<p>我们已经求得了权重矩阵W在<span class="math inline">\(t\)</span>时刻的梯度<span class="math inline">\(\nabla_{Wt}E\)</span>，最终的梯度<span class="math inline">\(\nabla_{W}E\)</span>是各个时刻的<strong>梯度之和</strong>：</p>
<p><span class="math display">\[\begin{align}
\nabla_WE=&amp;\sum_{i=1}^t\nabla_{W_i}E\\
=&amp;\begin{bmatrix}
\delta_1^t\mathrm{s}_1^{t-1} &amp; \delta_1^t\mathrm{s}_2^{t-1} &amp; ... &amp;  \delta_1^t\mathrm{s}_n^{t-1}\\
\delta_2^t\mathrm{s}_1^{t-1} &amp; \delta_2^t\mathrm{s}_2^{t-1} &amp; ... &amp;  \delta_2^t\mathrm{s}_n^{t-1}\\
.\\.\\
\delta_n^t\mathrm{s}_1^{t-1} &amp; \delta_n^t\mathrm{s}_2^{t-1} &amp; ... &amp;  \delta_n^t\mathrm{s}_n^{t-1}\\
\end{bmatrix}
+...+
\begin{bmatrix}
\delta_1^1\mathrm{s}_1^0 &amp; \delta_1^1\mathrm{s}_2^0 &amp; ... &amp;  \delta_1^1\mathrm{s}_n^0\\
\delta_2^1\mathrm{s}_1^0 &amp; \delta_2^1\mathrm{s}_2^0 &amp; ... &amp;  \delta_2^1\mathrm{s}_n^0\\
.\\.\\
\delta_n^1\mathrm{s}_1^0 &amp; \delta_n^1\mathrm{s}_2^0 &amp; ... &amp;  \delta_n^1\mathrm{s}_n^0\\
\end{bmatrix}\qquad(式6)
\end{align}\]</span></p>
<p><strong>式6</strong>就是计算<strong>循环层</strong>权重矩阵W的梯度的公式。</p>
<p><code>----------数学公式超高能预警----------</code></p>
<p>前面已经介绍了<span class="math inline">\(\nabla_{W}E\)</span>的计算方法，看上去还是比较直观的。然而，读者也许会困惑，为什么最终的梯度是各个时刻的梯度<strong>之和</strong>呢？我们前面只是直接用了这个结论，实际上这里面是有道理的，只是这个数学推导比较绕脑子。感兴趣的同学可以仔细阅读接下来这一段，它用到了矩阵对矩阵求导、张量与向量相乘运算的一些法则。</p>
<p>我们还是从这个式子开始：</p>
<p><span class="math display">\[\mathrm{net}_t=U\mathrm{x}_t+Wf(\mathrm{net}_{t-1})\]</span></p>
<p>因为<span class="math inline">\(U\mathrm{x}_t\)</span>与<span class="math inline">\(W\)</span>完全无关，我们把它看做常量。现在，考虑第一个式子加号右边的部分，因为<span class="math inline">\(W\)</span>和<span class="math inline">\(f(\mathrm{net}_{t-1})\)</span>都是<span class="math inline">\(W\)</span>的函数，因此我们要用到大学里面都学过的导数乘法运算：</p>
<p><span class="math display">\[(uv)&#39;=u&#39;v+uv&#39;\]</span></p>
<p>因此，上面第一个式子写成：</p>
<p><span class="math display">\[\frac{\partial{\mathrm{net}_t}}{\partial{W}}=\frac{\partial{W}}{\partial{W}}f(\mathrm{net}_{t-1})+W\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{W}}\\\]</span></p>
<p>我们最终需要计算的是<span class="math inline">\(\nabla_{W}E\)</span>：</p>
<p><span class="math display">\[\begin{align}
\nabla_WE=&amp;\frac{\partial{E}}{\partial{W}}\\
=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{W}}\\
=&amp;\delta_t^T\frac{\partial{W}}{\partial{W}}f(\mathrm{net}_{t-1})+ \delta_t^TW\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{W}}\qquad(式7)\\
\end{align}\]</span></p>
<p>我们先计算<strong>式7</strong>加号左边的部分。<span class="math inline">\(\frac{\partial{W}}{\partial{W}}\)</span>是<strong>矩阵对矩阵</strong>求导，其结果是一个四维<strong>张量(tensor)</strong>，如下所示：</p>
<p><span class="math display">\[\begin{align}
\frac{\partial{W}}{\partial{W}}=&amp;
\begin{bmatrix}
\frac{\partial{w_{11}}}{\partial{W}} &amp; \frac{\partial{w_{12}}}{\partial{W}} &amp; ... &amp; \frac{\partial{w_{1n}}}{\partial{W}}\\
\frac{\partial{w_{21}}}{\partial{W}} &amp; \frac{\partial{w_{22}}}{\partial{W}} &amp; ... &amp; \frac{\partial{w_{2n}}}{\partial{W}}\\
.\\.\\
\frac{\partial{w_{n1}}}{\partial{W}} &amp; \frac{\partial{w_{n2}}}{\partial{W}} &amp; ... &amp; \frac{\partial{w_{nn}}}{\partial{W}}\\
\end{bmatrix}\\
=&amp;
\begin{bmatrix}
\begin{bmatrix}
\frac{\partial{w_{11}}}{\partial{w_{11}}} &amp; \frac{\partial{w_{11}}}{\partial{w_{12}}} &amp; ... &amp; \frac{\partial{w_{11}}}{\partial{_{1n}}}\\
\frac{\partial{w_{11}}}{\partial{w_{21}}} &amp; \frac{\partial{w_{11}}}{\partial{w_{22}}} &amp; ... &amp; \frac{\partial{w_{11}}}{\partial{_{2n}}}\\
.\\.\\
\frac{\partial{w_{11}}}{\partial{w_{n1}}} &amp; \frac{\partial{w_{11}}}{\partial{w_{n2}}} &amp; ... &amp; \frac{\partial{w_{11}}}{\partial{_{nn}}}\\
\end{bmatrix} &amp;
\begin{bmatrix}
\frac{\partial{w_{12}}}{\partial{w_{11}}} &amp; \frac{\partial{w_{12}}}{\partial{w_{12}}} &amp; ... &amp; \frac{\partial{w_{12}}}{\partial{_{1n}}}\\
\frac{\partial{w_{12}}}{\partial{w_{21}}} &amp; \frac{\partial{w_{12}}}{\partial{w_{22}}} &amp; ... &amp; \frac{\partial{w_{12}}}{\partial{_{2n}}}\\
.\\.\\
\frac{\partial{w_{12}}}{\partial{w_{n1}}} &amp; \frac{\partial{w_{12}}}{\partial{w_{n2}}} &amp; ... &amp; \frac{\partial{w_{12}}}{\partial{_{nn}}}\\
\end{bmatrix}&amp;...\\
.\\.\\
\end{bmatrix}\\
=&amp;
\begin{bmatrix}
\begin{bmatrix}
1 &amp; 0 &amp; ... &amp; 0\\
0 &amp; 0 &amp; ... &amp; 0\\
.\\.\\
0 &amp; 0 &amp; ... &amp; 0\\
\end{bmatrix} &amp;
\begin{bmatrix}
0 &amp; 1 &amp; ... &amp; 0\\
0 &amp; 0 &amp; ... &amp; 0\\
.\\.\\
0 &amp; 0 &amp; ... &amp; 0\\
\end{bmatrix}&amp;...\\
.\\.\\
\end{bmatrix}\\
\end{align}\]</span></p>
<p>接下来，我们知道<span class="math inline">\(\mathrm{s}_{t-1}=f({\mathrm{net}_{t-1}})\)</span>，它是一个<strong>列向量</strong>。我们让上面的四维张量与这个向量相乘，得到了一个三维张量，再左乘行向量<span class="math inline">\(\delta_t^T\)</span>，最终得到一个矩阵：</p>
<p><span class="math display">\[\begin{align}
\delta_t^T\frac{\partial{W}}{\partial{W}}f({\mathrm{net}_{t-1}})=&amp;
\delta_t^T\frac{\partial{W}}{\partial{W}}{\mathrm{s}_{t-1}}\\
=&amp;\delta_t^T
\begin{bmatrix}
\begin{bmatrix}
1 &amp; 0 &amp; ... &amp; 0\\
0 &amp; 0 &amp; ... &amp; 0\\
.\\.\\
0 &amp; 0 &amp; ... &amp; 0\\
\end{bmatrix} &amp;
\begin{bmatrix}
0 &amp; 1 &amp; ... &amp; 0\\
0 &amp; 0 &amp; ... &amp; 0\\
.\\.\\
0 &amp; 0 &amp; ... &amp; 0\\
\end{bmatrix}&amp;...\\
.\\.\\
\end{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
s_2^{t-1}\\
.\\.\\
s_n^{t-1}\\
\end{bmatrix}\\
=&amp;\delta_t^T
\begin{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
0\\
.\\.\\
0\\
\end{bmatrix} &amp;
\begin{bmatrix}
s_2^{t-1}\\
0\\
.\\.\\
0\\
\end{bmatrix}&amp;...\\
.\\.\\
\end{bmatrix}\\
=&amp;
\begin{bmatrix}
\delta_1^t &amp; \delta_2^t &amp; ... &amp;\delta_n^t
\end{bmatrix}
\begin{bmatrix}
\begin{bmatrix}
s_1^{t-1}\\
0\\
.\\.\\
0\\
\end{bmatrix} &amp;
\begin{bmatrix}
s_2^{t-1}\\
0\\
.\\.\\
0\\
\end{bmatrix}&amp;...\\
.\\.\\
\end{bmatrix}\\
=&amp;
\begin{bmatrix}
\delta_1^ts_1^{t-1} &amp; \delta_1^ts_2^{t-1} &amp; ... &amp;  \delta_1^ts_n^{t-1}\\
\delta_2^ts_1^{t-1} &amp; \delta_2^ts_2^{t-1} &amp; ... &amp;  \delta_2^ts_n^{t-1}\\
.\\.\\
\delta_n^ts_1^{t-1} &amp; \delta_n^ts_2^{t-1} &amp; ... &amp;  \delta_n^ts_n^{t-1}\\
\end{bmatrix}\\
=&amp;\nabla_{Wt}E
\end{align}\]</span></p>
<p>接下来，我们计算<strong>式7</strong>加号右边的部分：</p>
<p><span class="math display">\[\begin{align}
\delta_t^TW\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{W}}=&amp;
\delta_t^TW\frac{\partial{f(\mathrm{net}_{t-1})}}{\partial{\mathrm{net}_{t-1}}}\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\
=&amp;\delta_t^TWf&#39;(\mathrm{net}_{t-1})\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\
=&amp;\delta_t^T\frac{\partial{\mathrm{net}_t}}{\partial{\mathrm{net}_{t-1}}}\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\
=&amp;\delta_{t-1}^T\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\
\end{align}\]</span></p>
<p>于是，我们得到了如下递推公式：</p>
<p><span class="math display">\[\begin{align}
\nabla_WE=&amp;\frac{\partial{E}}{\partial{W}}\\
=&amp;\frac{\partial{E}}{\partial{\mathrm{net}_t}}\frac{\partial{\mathrm{net}_t}}{\partial{W}}\\
=&amp;\nabla_{Wt}E+\delta_{t-1}^T\frac{\partial{\mathrm{net}_{t-1}}}{\partial{W}}\\
=&amp;\nabla_{Wt}E+\nabla_{Wt-1}E+\delta_{t-2}^T\frac{\partial{\mathrm{net}_{t-2}}}{\partial{W}}\\
=&amp;\nabla_{Wt}E+\nabla_{Wt-1}E+...+\nabla_{W1}E\\
=&amp;\sum_{k=1}^t\nabla_{Wk}E
\end{align}\]</span></p>
<p>这样，我们就证明了：最终的梯度<span class="math inline">\(\nabla_WE\)</span>是各个时刻的梯度之和。</p>
<p><code>----------数学公式超高能预警解除----------</code></p>
<p>同权重矩阵W类似，我们可以得到权重矩阵U的计算方法。</p>
<p><span class="math display">\[\nabla_{U_t}E=\begin{bmatrix}
\delta_1^tx_1^t &amp; \delta_1^tx_2^t &amp; ... &amp;  \delta_1^tx_m^t\\
\delta_2^tx_1^t &amp; \delta_2^tx_2^t &amp; ... &amp;  \delta_2^tx_m^t\\
.\\.\\
\delta_n^tx_1^t &amp; \delta_n^tx_2^t &amp; ... &amp;  \delta_n^tx_m^t\\
\end{bmatrix}\qquad(式8)\]</span></p>
<p><strong>式8</strong>是误差函数在t时刻对权重矩阵U的梯度。和权重矩阵W一样，最终的梯度也是各个时刻的梯度之和：</p>
<p><span class="math display">\[\nabla_UE=\sum_{i=1}^t\nabla_{U_i}E\]</span></p>
<p>具体的证明这里就不再赘述了，感兴趣的读者可以练习推导一下。</p>
<h3 id="rnn的梯度爆炸和消失问题">RNN的梯度爆炸和消失问题</h3>
<p>不幸的是，实践中前面介绍的几种RNNs并不能很好的处理较长的序列。一个主要的原因是，RNN在训练中很容易发生<strong>梯度爆炸</strong>和<strong>梯度消失</strong>，这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。</p>
<p>为什么RNN会产生梯度爆炸和消失问题呢？我们接下来将详细分析一下原因。我们根据<strong>式3</strong>可得：</p>
<p><span class="math display">\[\begin{align}
\delta_k^T=&amp;\delta_t^T\prod_{i=k}^{t-1}Wdiag[f&#39;(\mathrm{net}_{i})]\\
\|\delta_k^T\|\leqslant&amp;\|\delta_t^T\|\prod_{i=k}^{t-1}\|W\|\|diag[f&#39;(\mathrm{net}_{i})]\|\\
\leqslant&amp;\|\delta_t^T\|(\beta_W\beta_f)^{t-k}
\end{align}\]</span></p>
<p>上式的<span class="math inline">\(\beta\)</span>定义为矩阵的模的上界。因为上式是一个指数函数，如果<span class="math inline">\(t-k\)</span>很大的话（也就是向前看很远的时候），会导致对应的<strong>误差项</strong>的值增长或缩小的非常快，这样就会导致相应的<strong>梯度爆炸</strong>和<strong>梯度消失</strong>问题（取决于<span class="math inline">\(\beta\)</span>大于1还是小于1）。</p>
<p>通常来说，<strong>梯度爆炸</strong>更容易处理一些。因为梯度爆炸的时候，我们的程序会收到<em>NaN错误</em>。我们也可以设置一个<em>梯度阈值</em>，当梯度超过这个阈值的时候可以<em>直接截取</em>。</p>
<p><strong>梯度消失</strong>更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：</p>
<ol type="1">
<li>合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。</li>
<li>使用relu代替sigmoid和tanh作为激活函数。原理请参考上一篇文章<a href="https://otter668.github.io/2018/01/03/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">深度学习系列（4）</a>一节。</li>
<li>使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。我们将在以后的文章中介绍这两种网络。</li>
</ol>
<h2 id="rnn的应用举例基于rnn的语言模型">RNN的应用举例——基于RNN的语言模型</h2>
<p>现在，我们介绍一下基于RNN语言模型。我们首先把词依次输入到循环神经网络中，每输入一个词，循环神经网络就输出截止到目前为止，下一个最可能的词。例如，当我们依次输入：</p>
<blockquote>
<p>我 昨天 上学 迟到 了</p>
</blockquote>
<p>神经网络的输出如下图所示：</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15153952393256.jpg"></p>
<p>其中，s和e是两个特殊的词，分别表示一个序列的开始和结束。</p>
<h3 id="向量化">向量化</h3>
<p>我们知道，神经网络的输入和输出都是<strong>向量</strong>，为了让语言模型能够被神经网络处理，我们必须把词表达为向量的形式，这样神经网络才能处理它。</p>
<p>神经网络的输入是词，我们可以用下面的步骤对输入进行<strong>向量化</strong>：</p>
<ol type="1">
<li>建立一个包含所有词的词典，每个词在词典里面有一个唯一的编号。</li>
<li>任意一个词都可以用一个N维的one-hot向量来表示。其中，N是词典中包含的词的个数。假设一个词在词典中的编号是i，v是表示这个词的向量，<span class="math inline">\(v_j\)</span>是向量的第j个元素，则：</li>
</ol>
<p><span class="math display">\[v_j=\begin{equation}\begin{cases}1\qquad j=i\\0\qquad j\ne i\end{cases}\end{equation}\]</span></p>
<p>上面这个公式的含义，可以用下面的图来直观的表示：</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15153953639317.jpg"></p>
<p>使用这种向量化方法，我们就得到了一个<strong>高维</strong>、<strong>稀疏</strong>的向量（稀疏是指绝大部分元素的值都是0）。处理这样的向量会导致我们的神经网络有很多的参数，带来庞大的计算量。因此，往往会需要使用一些降维方法，将高维的稀疏向量转变为低维的稠密向量。不过这个话题我们就不再这篇文章中讨论了。</p>
<p>语言模型要求的输出是下一个最可能的词，我们可以让循环神经网络计算计算词典中每个词是下一个词的概率，这样，概率最大的词就是下一个最可能的词。因此，神经网络的输出向量也是一个N维向量，向量中的每个元素对应着词典中相应的词是下一个词的概率。如下图所示：</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15153954152014.jpg"></p>
<h3 id="softmax层">Softmax层</h3>
<p>前面提到，<strong>语言模型</strong>是对下一个词出现的<strong>概率</strong>进行建模。那么，怎样让神经网络输出概率呢？方法就是用softmax层作为神经网络的输出层。</p>
<p>我们先来看一下softmax函数的定义：</p>
<p><span class="math display">\[g(z_i)=\frac{e^{z_i}}{\sum_{k}e^{z_k}}\]</span></p>
<p>这个公式看起来可能很晕，我们举一个例子。Softmax层如下图所示：</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15153954619609.jpg"></p>
<p>从上图我们可以看到，softmax layer的输入是一个向量，输出也是一个向量，两个向量的维度是一样的（在这个例子里面是4）。输入向量x=[1 2 3 4]经过softmax层之后，经过上面的softmax函数计算，转变为输出向量y=[0.03 0.09 0.24 0.64]。计算过程为：</p>
<p><span class="math display">\[\begin{align}
y_1&amp;=\frac{e^{x_1}}{\sum_{k}e^{x_k}}\\
&amp;=\frac{e^1}{e^1+e^2+e^3+e^4}\\
&amp;=0.03\\
y_2&amp;=\frac{e^2}{e^1+e^2+e^3+e^4}\\
&amp;=0.09\\
y_3&amp;=\frac{e^3}{e^1+e^2+e^3+e^4}\\
&amp;=0.24\\
y_4&amp;=\frac{e^4}{e^1+e^2+e^3+e^4}\\
&amp;=0.64\\
\end{align}\]</span></p>
<p>我们来看看输出向量y的特征：</p>
<ol type="1">
<li>每一项为取值为0-1之间的正数；</li>
<li>所有项的总和是1。</li>
</ol>
<p>我们不难发现，这些特征和概率的特征是一样的，因此我们可以把它们看做是概率。对于<strong>语言模型</strong>来说，我们可以认为模型预测下一个词是词典中第一个词的概率是0.03，是词典中第二个词的概率是0.09，以此类推。</p>
<h3 id="语言模型的训练">语言模型的训练</h3>
<p>可以使用<strong>监督学习</strong>的方法对语言模型进行训练，首先，需要准备训练数据集。接下来，我们介绍怎样把语料</p>
<blockquote>
<p>我 昨天 上学 迟到 了</p>
</blockquote>
<p>转换成语言模型的训练数据集。</p>
<p>首先，我们获取输入-标签对：</p>
<table>
<thead>
<tr class="header">
<th>输入</th>
<th>标签</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>s</td>
<td>我</td>
</tr>
<tr class="even">
<td>我</td>
<td>昨天</td>
</tr>
<tr class="odd">
<td>昨天</td>
<td>上学</td>
</tr>
<tr class="even">
<td>上学</td>
<td>迟到</td>
</tr>
<tr class="odd">
<td>迟到</td>
<td>了</td>
</tr>
<tr class="even">
<td>了</td>
<td>e</td>
</tr>
</tbody>
</table>
<p>然后，使用前面介绍过的<strong>向量化</strong>方法，对输入x和标签y进行<strong>向量化</strong>。这里面有意思的是，对标签y进行向量化，其结果也是一个one-hot向量。例如，我们对标签『我』进行向量化，得到的向量中，只有第2019个元素的值是1，其他位置的元素的值都是0。它的含义就是下一个词是『我』的概率是1，是其它词的概率都是0。</p>
<p>最后，我们使用<strong>交叉熵误差函数</strong>作为优化目标，对模型进行优化。</p>
<p>在实际工程中，我们可以使用大量的语料来对模型进行训练，获取训练数据和训练的方法都是相同的。</p>
<h3 id="交叉熵误差">交叉熵误差</h3>
<p>一般来说，当神经网络的输出层是softmax层时，对应的误差函数E通常选择交叉熵误差函数，其定义如下：</p>
<p><span class="math display">\[L(y,o)=-\frac{1}{N}\sum_{n\in{N}}{y_nlogo_n}\]</span></p>
<p>在上式中，N是训练样本的个数，向量<span class="math inline">\(y_n\)</span>是样本的标记，向<span class="math inline">\(o_n\)</span>量是网络的输出。标记<span class="math inline">\(y_n\)</span>是一个one-hot向量，例如<span class="math inline">\(y_1=[1, 0, 0, 0]\)</span>，如果网络的输出<span class="math inline">\(o=[0.03,0.09,0.24,0.64]\)</span>，那么，交叉熵误差是（假设只有一个训练样本，即N=1）：</p>
<p><span class="math display">\[\begin{align}
L&amp;=-\frac{1}{N}\sum_{n\in{N}}{y_nlogo_n}\\
&amp;=-y_1logo_1\\
&amp;=-(1*log0.03+0*log0.09+0*log0.24+0*log0.64)\\
&amp;=3.51
\end{align}\]</span></p>
<p>我们当然可以选择其他函数作为我们的误差函数，比如最小平方误差函数(MSE)。不过对概率进行建模时，选择交叉熵误差函数更make sense。具体原因，感兴趣的读者请阅读<a href="https://jamesmccaffrey.wordpress.com/2011/12/17/neural-network-classification-categorical-data-softmax-activation-and-cross-entropy-error/" target="_blank" rel="noopener">参考文献7</a>。</p>
<h2 id="rnn的实现">RNN的实现</h2>
<blockquote>
<p>完整代码请参考<a href="https://github.com/hanbt/learn_dl/blob/master/rnn.py" target="_blank" rel="noopener">GitHub:RNN</a> (python2.7)</p>
</blockquote>
<p>为了加深我们对前面介绍的知识的理解，我们来动手实现一个RNN层。我们复用了上一篇文章<a href="https://otter668.github.io/2018/01/03/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" target="_blank" rel="noopener">深度学习系列（4）</a>中的一些代码，所以先把它们导入进来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> cnn <span class="keyword">import</span> ReluActivator, IdentityActivator, element_wise_op</span><br></pre></td></tr></table></figure>
<p>我们用RecurrentLayer类来实现一个<strong>循环层</strong>。下面的代码是初始化一个循环层，可以在构造函数中设置卷积层的超参数。我们注意到，循环层有两个权重数组，U和W。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RecurrentLayer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_width, state_width,</span></span></span><br><span class="line"><span class="function"><span class="params">                 activator, learning_rate)</span>:</span></span><br><span class="line">        self.input_width = input_width</span><br><span class="line">        self.state_width = state_width</span><br><span class="line">        self.activator = activator</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.times = <span class="number">0</span>       <span class="comment"># 当前时刻初始化为t0</span></span><br><span class="line">        self.state_list = [] <span class="comment"># 保存各个时刻的state</span></span><br><span class="line">        self.state_list.append(np.zeros(</span><br><span class="line">            (state_width, <span class="number">1</span>)))           <span class="comment"># 初始化s0</span></span><br><span class="line">        self.U = np.random.uniform(<span class="number">-1e-4</span>, <span class="number">1e-4</span>,</span><br><span class="line">            (state_width, input_width))  <span class="comment"># 初始化U</span></span><br><span class="line">        self.W = np.random.uniform(<span class="number">-1e-4</span>, <span class="number">1e-4</span>,</span><br><span class="line">            (state_width, state_width))  <span class="comment"># 初始化W</span></span><br></pre></td></tr></table></figure>
<p>在forward方法中，实现循环层的前向计算，这部分比较简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_array)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    根据『式2』进行前向计算</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    self.times += <span class="number">1</span></span><br><span class="line">    state = (np.dot(self.U, input_array) +</span><br><span class="line">             np.dot(self.W, self.state_list[<span class="number">-1</span>]))</span><br><span class="line">    element_wise_op(state, self.activator.forward)</span><br><span class="line">    self.state_list.append(state)</span><br></pre></td></tr></table></figure>
<p>在backword方法中，实现BPTT算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, sensitivity_array, </span></span></span><br><span class="line"><span class="function"><span class="params">             activator)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    实现BPTT算法</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    self.calc_delta(sensitivity_array, activator)</span><br><span class="line">    self.calc_gradient()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_delta</span><span class="params">(self, sensitivity_array, activator)</span>:</span></span><br><span class="line">    self.delta_list = []  <span class="comment"># 用来保存各个时刻的误差项</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(self.times):</span><br><span class="line">        self.delta_list.append(np.zeros(</span><br><span class="line">            (self.state_width, <span class="number">1</span>)))</span><br><span class="line">    self.delta_list.append(sensitivity_array)</span><br><span class="line">    <span class="comment"># 迭代计算每个时刻的误差项</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(self.times - <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        self.calc_delta_k(k, activator)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_delta_k</span><span class="params">(self, k, activator)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    根据k+1时刻的delta计算k时刻的delta</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    state = self.state_list[k+<span class="number">1</span>].copy()</span><br><span class="line">    element_wise_op(self.state_list[k+<span class="number">1</span>],</span><br><span class="line">                activator.backward)</span><br><span class="line">    self.delta_list[k] = np.dot(</span><br><span class="line">        np.dot(self.delta_list[k+<span class="number">1</span>].T, self.W),</span><br><span class="line">        np.diag(state[:,<span class="number">0</span>])).T</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_gradient</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.gradient_list = [] <span class="comment"># 保存各个时刻的权重梯度</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(self.times + <span class="number">1</span>):</span><br><span class="line">        self.gradient_list.append(np.zeros(</span><br><span class="line">            (self.state_width, self.state_width)))</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(self.times, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        self.calc_gradient_t(t)</span><br><span class="line">    <span class="comment"># 实际的梯度是各个时刻梯度之和</span></span><br><span class="line">    self.gradient = reduce(</span><br><span class="line">        <span class="keyword">lambda</span> a, b: a + b, self.gradient_list,</span><br><span class="line">        self.gradient_list[<span class="number">0</span>]) <span class="comment"># [0]被初始化为0且没有被修改过</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_gradient_t</span><span class="params">(self, t)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算每个时刻t权重的梯度</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    gradient = np.dot(self.delta_list[t],</span><br><span class="line">        self.state_list[t<span class="number">-1</span>].T)</span><br><span class="line">    self.gradient_list[t] = gradient</span><br></pre></td></tr></table></figure>
<p>有意思的是，BPTT算法虽然数学推导的过程很麻烦，但是写成代码却并不复杂。</p>
<p>在update方法中，实现梯度下降算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    按照梯度下降，更新权重</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    self.W -= self.learning_rate * self.gradient</span><br></pre></td></tr></table></figure>
<p>上面的代码不包含权重U的更新。这部分实际上和全连接神经网络是一样的，留给感兴趣的读者自己来完成吧。</p>
<p><strong>循环层</strong>是一个带状态的层，每次forword都会改变循环层的内部状态，这给梯度检查带来了麻烦。因此，我们需要一个reset_state方法，来重置循环层的内部状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_state</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.times = <span class="number">0</span>       <span class="comment"># 当前时刻初始化为t0</span></span><br><span class="line">    self.state_list = [] <span class="comment"># 保存各个时刻的state</span></span><br><span class="line">    self.state_list.append(np.zeros(</span><br><span class="line">        (self.state_width, <span class="number">1</span>)))      <span class="comment"># 初始化s0</span></span><br></pre></td></tr></table></figure>
<p>最后，是梯度检查的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    梯度检查</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 设计一个误差函数，取所有节点输出项之和</span></span><br><span class="line">    error_function = <span class="keyword">lambda</span> o: o.sum()</span><br><span class="line">    rl = RecurrentLayer(<span class="number">3</span>, <span class="number">2</span>, IdentityActivator(), <span class="number">1e-3</span>)</span><br><span class="line">    <span class="comment"># 计算forward值</span></span><br><span class="line">    x, d = data_set()</span><br><span class="line">    rl.forward(x[<span class="number">0</span>])</span><br><span class="line">    rl.forward(x[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 求取sensitivity map</span></span><br><span class="line">    sensitivity_array = np.ones(rl.state_list[<span class="number">-1</span>].shape,</span><br><span class="line">                                dtype=np.float64)</span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    rl.backward(sensitivity_array, IdentityActivator())</span><br><span class="line">    <span class="comment"># 检查梯度</span></span><br><span class="line">    epsilon = <span class="number">10e-4</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(rl.W.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(rl.W.shape[<span class="number">1</span>]):</span><br><span class="line">            rl.W[i,j] += epsilon</span><br><span class="line">            rl.reset_state()</span><br><span class="line">            rl.forward(x[<span class="number">0</span>])</span><br><span class="line">            rl.forward(x[<span class="number">1</span>])</span><br><span class="line">            err1 = error_function(rl.state_list[<span class="number">-1</span>])</span><br><span class="line">            rl.W[i,j] -= <span class="number">2</span>*epsilon</span><br><span class="line">            rl.reset_state()</span><br><span class="line">            rl.forward(x[<span class="number">0</span>])</span><br><span class="line">            rl.forward(x[<span class="number">1</span>])</span><br><span class="line">            err2 = error_function(rl.state_list[<span class="number">-1</span>])</span><br><span class="line">            expect_grad = (err1 - err2) / (<span class="number">2</span> * epsilon)</span><br><span class="line">            rl.W[i,j] += epsilon</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'weights(%d,%d): expected - actural %f - %f'</span> % (</span><br><span class="line">                i, j, expect_grad, rl.gradient[i,j])</span><br></pre></td></tr></table></figure>
<p>需要注意，每次计算error之前，都要调用reset_state方法重置循环层的内部状态。下面是梯度检查的结果，没问题！</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15153973768854.jpg"></p>
<h2 id="小结">小结</h2>
<p>至此，我们讲完了基本的<strong>循环神经网络</strong>、它的训练算法：<strong>BPTT</strong>，以及在语言模型上的应用。RNN比较烧脑，相信拿下前几篇文章的读者们搞定这篇文章也不在话下吧！然而，<strong>循环神经网络</strong>这个话题并没有完结。我们在前面说到过，基本的循环神经网络存在梯度爆炸和梯度消失问题，并不能真正的处理好长距离的依赖（虽然有一些技巧可以减轻这些问题）。事实上，真正得到广泛的应用的是循环神经网络的一个变体：<strong>长短时记忆网络</strong>。它内部有一些特殊的结构，可以很好的处理长距离的依赖，我们将在下一篇文章中详细的介绍它。现在，让我们稍事休息，准备挑战更为烧脑的<strong>长短时记忆网络</strong>吧。</p>
<h2 id="参考资料">参考资料</h2>
<ol type="1">
<li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="noopener">RECURRENT NEURAL NETWORKS TUTORIAL</a></li>
<li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
<li><a href="http://distill.pub/2016/augmented-rnns/" target="_blank" rel="noopener">Attention and Augmented Recurrent Neural Networks</a></li>
<li><a href="http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf" target="_blank" rel="noopener">On the difficulty of training recurrent neural networks, Bengio et al.</a></li>
<li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" target="_blank" rel="noopener">Recurrent neural network based language model, Mikolov et al.</a></li>
<li><a href="https://jamesmccaffrey.wordpress.com/2011/12/17/neural-network-classification-categorical-data-softmax-activation-and-cross-entropy-error/" target="_blank" rel="noopener">Neural Network Classification, Categorical Data, Softmax Activation, and Cross Entropy Error, McCaffrey</a></li>
</ol>
<hr>
<p>[转载于<a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">零基础入门深度学习(5) - 循环神经网络</a>]</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>感谢您的支持</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block;width:150px;height:150px;margin:auto 10px auto 10px">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="Gaochao 微信支付"/>
        <p style="text-align:center">WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block;width:150px;height:150px;margin:auto 10px auto 10px">
        <img id="alipay_qr" src="/images/alipay.png" alt="Gaochao 支付宝"/>
        <p style="text-align:center">Alipay</p>
      </div>
    

    

    
      <div id="alireward" style="display: inline-block;width:150px;height:150px;margin:auto 10px auto 10px">
        <img id="alireward_qr" src="/images/alireward.png" alt="Gaochao 支付宝"/>
        <p style="text-align:center">支付宝红包码</p>
      </div>
    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DeepLearning/" rel="tag"># DeepLearning</a>
          
            <a href="/tags/RNN/" rel="tag"># RNN</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/03/卷积神经网络/" rel="next" title="深度学习系列（4）">
                <i class="fa fa-chevron-left"></i> 深度学习系列（4）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars1.githubusercontent.com/u/2328224?s=400&v=4"
                alt="Gaochao" />
            
              <p class="site-author-name" itemprop="name">Gaochao</p>
              <p class="site-description motion-element" itemprop="description">Learning NLP by Python</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="mailto:chaogao@aiit.edu.cn" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://github.com/otter668" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://weibo.com/54262426" target="_blank" title="微博">
                    
                      <i class="fa fa-fw fa-weibo"></i>微博</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://www.zhihu.com/people/otter668" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-globe"></i>知乎</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#循环神经网络"><span class="nav-number">1.</span> <span class="nav-text">&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#往期回顾"><span class="nav-number">1.1.</span> <span class="nav-text">&#x5F80;&#x671F;&#x56DE;&#x987E;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#语言模型"><span class="nav-number">1.2.</span> <span class="nav-text">&#x8BED;&#x8A00;&#x6A21;&#x578B;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环神经网络是啥"><span class="nav-number">1.3.</span> <span class="nav-text">&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x662F;&#x5565;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本循环神经网络"><span class="nav-number">1.3.1.</span> <span class="nav-text">&#x57FA;&#x672C;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#双向循环神经网络"><span class="nav-number">1.3.2.</span> <span class="nav-text">&#x53CC;&#x5411;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#深度循环神经网络"><span class="nav-number">1.3.3.</span> <span class="nav-text">&#x6DF1;&#x5EA6;&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#循环神经网络的训练"><span class="nav-number">1.4.</span> <span class="nav-text">&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#循环神经网络的训练算法bptt"><span class="nav-number">1.4.1.</span> <span class="nav-text">&#x5FAA;&#x73AF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;&#x7B97;&#x6CD5;&#xFF1A;BPTT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前向计算"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">&#x524D;&#x5411;&#x8BA1;&#x7B97;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#误差项的计算"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">&#x8BEF;&#x5DEE;&#x9879;&#x7684;&#x8BA1;&#x7B97;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#权重梯度的计算"><span class="nav-number">1.4.1.3.</span> <span class="nav-text">&#x6743;&#x91CD;&#x68AF;&#x5EA6;&#x7684;&#x8BA1;&#x7B97;</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rnn的梯度爆炸和消失问题"><span class="nav-number">1.4.2.</span> <span class="nav-text">RNN&#x7684;&#x68AF;&#x5EA6;&#x7206;&#x70B8;&#x548C;&#x6D88;&#x5931;&#x95EE;&#x9898;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rnn的应用举例基于rnn的语言模型"><span class="nav-number">1.5.</span> <span class="nav-text">RNN&#x7684;&#x5E94;&#x7528;&#x4E3E;&#x4F8B;&#x2014;&#x2014;&#x57FA;&#x4E8E;RNN&#x7684;&#x8BED;&#x8A00;&#x6A21;&#x578B;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#向量化"><span class="nav-number">1.5.1.</span> <span class="nav-text">&#x5411;&#x91CF;&#x5316;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax层"><span class="nav-number">1.5.2.</span> <span class="nav-text">Softmax&#x5C42;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#语言模型的训练"><span class="nav-number">1.5.3.</span> <span class="nav-text">&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x7684;&#x8BAD;&#x7EC3;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#交叉熵误差"><span class="nav-number">1.5.4.</span> <span class="nav-text">&#x4EA4;&#x53C9;&#x71B5;&#x8BEF;&#x5DEE;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rnn的实现"><span class="nav-number">1.6.</span> <span class="nav-text">RNN&#x7684;&#x5B9E;&#x73B0;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">1.7.</span> <span class="nav-text">&#x5C0F;&#x7ED3;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">1.8.</span> <span class="nav-text">&#x53C2;&#x8003;&#x8D44;&#x6599;</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Gaochao</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">41.8k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
  </script>

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
