{"meta":{"title":"NLPer","subtitle":"coding","description":"Python NLP","author":"Gaochao","url":"http://otter668@github.io"},"pages":[{"title":"关于&留言","date":"2017-12-20T01:51:49.000Z","updated":"2017-12-20T03:43:09.764Z","comments":true,"path":"about/index.html","permalink":"http://otter668@github.io/about/index.html","excerpt":"","text":"about me GaoChao (AIIT教师) Email: chaogao#aiit.edu.cn（#-&gt;@） 科研情况： 主要研究方向：自然语言处理（NLP），问答系统 科研项目：863 973项目都没有 科研论文：Nature Science都没发过 教学情况： 编译原理 面向对象程序设计 C语言程序设计基础 计算机应用基础 小结 做你能做到的，接受你不能做到的，然后继续学习、继续提升自己。 你之所以不喜欢现在的我，是因为你还没走过我所走的路。 对未来的真正慷慨，是把一切献给现在。 每一个用真心对待的日子，回想起来都会觉得闪闪发亮。 ​​ 看到一句话，叫：“凡事有交代，件件有着落，事事有回音”。好像也不是很难，其实绝大多数人都做不到。所以你哪怕其它属性不变，只是让自己能做到这些，也会成为稀缺资源，人生会很不一样。"},{"title":"categories","date":"2017-12-20T02:35:36.000Z","updated":"2017-12-20T02:36:14.153Z","comments":false,"path":"categories/index.html","permalink":"http://otter668@github.io/categories/index.html","excerpt":"","text":""},{"title":"Tagcloud","date":"2017-12-20T02:29:41.000Z","updated":"2017-12-20T02:36:18.908Z","comments":false,"path":"tags/index.html","permalink":"http://otter668@github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"深度学习系列（3）","slug":"神经网络和反向传播算法","date":"2017-12-19T12:45:23.490Z","updated":"2017-12-20T07:33:39.709Z","comments":true,"path":"2017/12/19/神经网络和反向传播算法/","link":"","permalink":"http://otter668@github.io/2017/12/19/神经网络和反向传播算法/","excerpt":"神经网络和反向传播算法 0. 往期回顾 在上一篇文章中，我们已经掌握了机器学习的基本套路，对模型、目标函数、优化算法这些概念有了一定程度的理解，而且已经会训练单个的感知器或者线性单元了。在这篇文章中，我们将把这些单独的单元按照一定的规则相互连接在一起形成神经网络，从而奇迹般的获得了强大的学习能力。我们还将介绍这种网络的训练算法：反向传播算法。最后，我们依然用代码实现一个神经网络。如果您能坚持到本文的结尾，将会看到我们用自己实现的神经网络去识别手写数字。现在请做好准备，您即将双手触及到深度学习的大门。","text":"神经网络和反向传播算法 0. 往期回顾 在上一篇文章中，我们已经掌握了机器学习的基本套路，对模型、目标函数、优化算法这些概念有了一定程度的理解，而且已经会训练单个的感知器或者线性单元了。在这篇文章中，我们将把这些单独的单元按照一定的规则相互连接在一起形成神经网络，从而奇迹般的获得了强大的学习能力。我们还将介绍这种网络的训练算法：反向传播算法。最后，我们依然用代码实现一个神经网络。如果您能坚持到本文的结尾，将会看到我们用自己实现的神经网络去识别手写数字。现在请做好准备，您即将双手触及到深度学习的大门。 1. 神经元 神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是阶跃函数；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。如下图所示： 计算一个神经元的输出的方法和计算一个感知器的输出是一样的。假设神经元的输入是向量\\(\\vec{x}\\)，权重向量是\\(\\vec{w}\\)(偏置项是\\(w_0\\))，激活函数是sigmoid函数，则其输出\\(y\\)： \\[y=sigmoid(\\vec{w}^T\\centerdot\\vec{x})\\qquad(式1)\\] sigmoid函数的定义如下： \\[sigmoid(x)=\\frac{1}{1+e^{-x}}\\] sigmoid函数是一个非线性函数，值域是(0,1)。函数图像如下图所示 sigmoid函数的导数是： \\[\\begin{align} &amp;令y=sigmoid(x)\\\\ &amp;则y&#39;=y(1-y) \\end{align}\\] 可以看到，sigmoid函数的导数非常有趣，它可以用sigmoid函数自身来表示。这样，一旦计算出sigmoid函数的值，计算它的导数的值就非常方便。 2. 神经网络是啥 神经网络其实就是按照一定规则连接起来的多个神经元。上图展示了一个全连接(full connected, FC)神经网络，通过观察上面的图，我们可以发现它的规则包括： 神经元按照层来布局。最左边的层叫做输入层，负责接收输入数据；最右边的层叫输出层，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做隐藏层，因为它们对于外部来说是不可见的。 同一层的神经元之间没有连接。 第N层的每个神经元和第N-1层的所有神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。 每个连接都有一个权值。 上面这些规则定义了全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。 3. 计算神经网络的输出 神经网络实际上就是一个输入向量\\(\\vec{x}\\)到输出向量\\(\\vec{y}\\)的函数，即： \\[\\vec{y} = f_{network}(\\vec{x})\\] 根据输入计算神经网络的输出，需要首先将输入向量\\(\\vec{x}\\)的每个元素\\(x_i\\)的值赋给神经网络的输入层的对应神经元，然后根据式1依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量\\(\\vec{y}\\)。 接下来举一个例子来说明这个过程，我们先给神经网络的每个单元写上编号。 如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是全连接网络，所以可以看到每个节点都和上一层的所有节点有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为\\(w_{41}\\)，\\(w_{42}\\)，\\(w_{43}\\)。那么，我们怎样计算节点4的输出值\\(a_4\\)呢？ 为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是输入层的节点，所以，他们的输出值就是输入向量\\(\\vec{x}\\)本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是\\(x_1\\),\\(x_2\\),\\(x_3\\)。我们要求输入向量的维度和输入层神经元个数相同，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把\\(x_1\\)赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值。 一旦我们有了节点1、2、3的输出值，我们就可以根据式1计算节点4的输出值\\(a_4\\)： \\[\\begin{align} a_4&amp;=sigmoid(\\vec{w}^T\\centerdot\\vec{x})\\\\ &amp;=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b}) \\end{align}\\] 上式的\\(w_{4b}\\)是节点4的偏置项，图中没有画出来。而\\(w_{41}\\),\\(w_{42}\\),\\(w_{43}\\)分别为节点1、2、3到节点4连接的权重，在给权重\\(w_{ji}\\)编号时，我们把目标节点的编号\\(j\\)放在前面，把源节点的编号\\(i\\)放在后面。 同样，我们可以继续计算出节点5、6、7的输出值\\(a_5\\),\\(a_6\\),\\(a_7\\)。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值\\(y_1\\)： \\[\\begin{align} y_1&amp;=sigmoid(\\vec{w}^T\\centerdot\\vec{a})\\\\ &amp;=sigmoid(w_{84}a_4+w_{85}a_5+w_{86}a_6+w_{87}a_7+w_{8b}) \\end{align}\\] 同理，我们还可以计算出\\(y_2\\)的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量\\(\\vec{x}=\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}\\)时，神经网络的输出向量\\(\\vec{y}=\\begin{bmatrix}y_1\\\\y_2\\end{bmatrix}\\)。这里我们也看到，输出向量的维度和输出层神经元个数相同。 4. 神经网络的矩阵表示 神经网络的计算如果用矩阵来表示会很方便（当然逼格也更高），我们先来看看隐藏层的矩阵表示。 首先我们把隐藏层4个节点的计算依次排列出来： \\[a_4=sigmoid(w_{41}x_1+w_{42}x_2+w_{43}x_3+w_{4b})\\\\ a_5=sigmoid(w_{51}x_1+w_{52}x_2+w_{53}x_3+w_{5b})\\\\ a_6=sigmoid(w_{61}x_1+w_{62}x_2+w_{63}x_3+w_{6b})\\\\ a_7=sigmoid(w_{71}x_1+w_{72}x_2+w_{73}x_3+w_{7b})\\\\\\] 接着，定义网络的输入向量\\(\\vec{x}\\)和隐藏层每个节点的权重向量\\(\\vec{w_j}\\)。令 \\[\\begin{align} \\vec{x}&amp;=\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\1\\end{bmatrix}\\\\ \\vec{w}_4&amp;=[w_{41},w_{42},w_{43},w_{4b}]\\\\ \\vec{w}_5&amp;=[w_{51},w_{52},w_{53},w_{5b}]\\\\ \\vec{w}_6&amp;=[w_{61},w_{62},w_{63},w_{6b}]\\\\ \\vec{w}_7&amp;=[w_{71},w_{72},w_{73},w_{7b}]\\\\ f&amp;=sigmoid \\end{align}\\] 代入到前面的一组式子，得到： \\[\\begin{align} a_4&amp;=f(\\vec{w_4}\\centerdot\\vec{x})\\\\ a_5&amp;=f(\\vec{w_5}\\centerdot\\vec{x})\\\\ a_6&amp;=f(\\vec{w_6}\\centerdot\\vec{x})\\\\ a_7&amp;=f(\\vec{w_7}\\centerdot\\vec{x}) \\end{align}\\] 现在，我们把上述计算\\(a_4\\),\\(a_5\\),\\(a_6\\),\\(a_7\\)的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示它们的计算了。令 \\[\\vec{a}= \\begin{bmatrix} a_4 \\\\ a_5 \\\\ a_6 \\\\ a_7 \\\\ \\end{bmatrix},\\qquad W= \\begin{bmatrix} \\vec{w}_4 \\\\ \\vec{w}_5 \\\\ \\vec{w}_6 \\\\ \\vec{w}_7 \\\\ \\end{bmatrix}= \\begin{bmatrix} w_{41},w_{42},w_{43},w_{4b} \\\\ w_{51},w_{52},w_{53},w_{5b} \\\\ w_{61},w_{62},w_{63},w_{6b} \\\\ w_{71},w_{72},w_{73},w_{7b} \\\\ \\end{bmatrix} ,\\qquad f( \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ .\\\\ .\\\\ .\\\\ \\end{bmatrix})= \\begin{bmatrix} f(x_1)\\\\ f(x_2)\\\\ f(x_3)\\\\ .\\\\ .\\\\ .\\\\ \\end{bmatrix}\\] 带入前面的一组式子，得到 \\[\\vec{a}=f(W\\centerdot\\vec{x})\\qquad (式2)\\] 在式2中，是\\(f\\)激活函数，在本例中是\\(sigmoid\\)函数；\\(W\\)是某一层的权重矩阵；\\(\\vec{x}\\)是某层的输入向量；\\(\\vec{a}\\)是某层的输出向量。式2说明神经网络的每一层的作用实际上就是先将输入向量左乘一个数组进行线性变换，得到一个新的向量，然后再对这个向量逐元素应用一个激活函数。 每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为\\(W_1\\),\\(W_2\\),\\(W_3\\),\\(W_4\\)，每个隐藏层的输出分别是\\(\\vec{a_1}\\),\\(\\vec{a_2}\\),\\(\\vec{a_3}\\)，神经网络的输入为\\(\\vec{x}\\)，神经网络的输出为\\(\\vec{y}\\)，如下图所示： 则每一层的输出向量的计算可以表示为： \\[\\begin{align} &amp;\\vec{a}_1=f(W_1\\centerdot\\vec{x})\\\\ &amp;\\vec{a}_2=f(W_2\\centerdot\\vec{a}_1)\\\\ &amp;\\vec{a}_3=f(W_3\\centerdot\\vec{a}_2)\\\\ &amp;\\vec{y}=f(W_4\\centerdot\\vec{a}_3)\\\\ \\end{align}\\] 这就是神经网络输出值的计算方法。 5. 神经网络的训练 现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个模型，那么这些权值就是模型的参数，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为超参数(Hyper-Parameters)。 接下来，我们将要介绍神经网络的训练算法：反向传播算法。 5.1 反向传播算法(Back Propagation) 我们首先直观的介绍反向传播算法，最后再来介绍这个算法的推导。当然读者也可以完全跳过推导部分，因为即使不知道如何推导，也不影响你写出来一个神经网络的训练代码。事实上，现在神经网络成熟的开源实现多如牛毛，除了练手之外，你可能都没有机会需要去写一个神经网络。 我们以监督学习为例来解释反向传播算法。在线性单元和梯度下降一文中我们介绍了什么是监督学习，如果忘记了可以再看一下。另外，我们设神经元的激活函数\\(f\\)为\\(sigmoid\\)函数(不同激活函数的计算公式不同，详情见反向传播算法的推导一节)。 我们假设每个训练样本为\\((\\vec{x},\\vec{t})\\)，其中向量\\(\\vec{x}\\)是训练样本的特征，而\\(\\vec{t}\\)是样本的目标值。 首先，我们根据上一节介绍的算法，用样本的特征\\(\\vec{x}\\)，计算出神经网络中每个隐藏层节点的输出\\(a_i\\)，以及输出层每个节点的输出\\(y_i\\)。 然后，我们按照下面的方法计算出每个节点的误差项\\(\\delta_i\\)： 对于输出层节点\\(i\\)， \\[\\delta_i=y_i(1-y_i)(t_i-y_i)\\qquad(式3)\\] 其中，\\(\\delta_i\\)是节点\\(i\\)的误差项，\\(y_i\\)是节点的输出值，\\(t_i\\)是样本对应于节点的目标值。举个例子，根据上图，对于输出层节点8来说，它的输出值是\\(y_1\\)，而样本的目标值是\\(t_1\\)，带入上面的公式得到节点8的误差项\\(\\delta_8\\)应该是： \\[\\delta_8=y_1(1-y_1)(t_1-y_1)\\] 对于隐藏层节点， \\[\\delta_i=a_i(1-a_i)\\sum_{k\\in{outputs}}w_{ki}\\delta_k\\qquad(式4)\\] 其中，\\(a_i\\)是节点\\(i\\)的输出值，\\(w_{ki}\\)是节点\\(i\\)到它的下一层节点\\(k\\)的连接的权重，\\(\\delta_k\\)是节点\\(i\\)的下一层节点\\(k\\)的误差项。例如，对于隐藏层节点4来说，计算方法如下： \\[\\delta_4=a_4(1-a_4)(w_{84}\\delta_8+w_{94}\\delta_9)\\] 最后，更新每个连接上的权值： \\[w_{ji}\\gets w_{ji}+\\eta\\delta_jx_{ji}\\qquad(式5)\\] 其中，\\(w_{ji}\\)是节点\\(i\\)到节点\\(j\\)的权重，\\(\\eta\\)是一个成为学习速率的常数，\\(\\delta_j\\)是节点\\(j\\)的误差项，\\(x_{ij}\\)是节点\\(i\\)传递给节点\\(j\\)的输入。例如，权重\\(w_{84}\\)的更新方法如下： \\[w_{84}\\gets w_{84}+\\eta\\delta_8 a_4\\] 类似的，权重\\(w_{41}\\)的更新方法如下： \\[w_{41}\\gets w_{41}+\\eta\\delta_4 x_1\\] 偏置项的输入值永远为1。例如，节点4的偏置项\\(w_{4b}\\)应该按照下面的方法计算： \\[w_{4b}\\gets w_{4b}+\\eta\\delta_4\\] 我们已经介绍了神经网络每个节点误差项的计算和权重更新方法。显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。当所有节点的误差项计算完毕后，我们就可以根据式5来更新所有的权重。 以上就是基本的反向传播算法，并不是很复杂，您弄清楚了么？ 5.2 反向传播算法的推导 反向传播算法其实就是链式求导法则的应用。然而，这个如此简单且显而易见的方法，却是在Roseblatt提出感知器算法将近30年之后才被发明和普及的。对此，Bengio这样回应道： 很多看似显而易见的想法只有在事后才变得显而易见。 接下来，我们用链式求导法则来推导反向传播算法，也就是上一小节的式3、式4、式5。 前方高能预警——接下来是数学公式重灾区，读者可以酌情阅读，不必强求。 按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用随机梯度下降优化算法去求目标函数最小值时的参数值。 我们取网络所有输出层节点的误差平方和作为目标函数： \\[E_d\\equiv\\frac{1}{2}\\sum_{i\\in outputs}(t_i-y_i)^2\\] 其中，\\(E_d\\)表示是样本\\(d\\)的误差。 然后，我们用文章线性单元和梯度下降中介绍的随机梯度下降算法对目标函数进行优化： \\[w_{ji}\\gets w_{ji}-\\eta\\frac{\\partial{E_d}}{\\partial{w_{ji}}}\\] 随机梯度下降算法也就是需要求出误差\\(E_d\\)对于每个权重\\(w_{ji}\\)的偏导数（也就是梯度），怎么求呢？ 观察上图，我们发现权重\\(w_{ji}\\)仅能通过影响节点\\(j\\)的输入值影响网络的其它部分，设\\(net_j\\)是节点\\(j\\)的加权输入，即 \\[\\begin{align} net_j&amp;=\\vec{w_j}\\centerdot\\vec{x_j}\\\\ &amp;=\\sum_{i}{w_{ji}}x_{ji} \\end{align}\\] \\(E_d\\)是\\(net_j\\)的函数，而\\(net_j\\)是\\(w_{ji}\\)的函数。根据链式求导法则，可以得到： \\[\\begin{align} \\frac{\\partial{E_d}}{\\partial{w_{ji}}}&amp;=\\frac{\\partial{E_d}}{\\partial{net_j}}\\frac{\\partial{net_j}}{\\partial{w_{ji}}}\\\\ &amp;=\\frac{\\partial{E_d}}{\\partial{net_j}}\\frac{\\partial{\\sum_{i}{w_{ji}}x_{ji}}}{\\partial{w_{ji}}}\\\\ &amp;=\\frac{\\partial{E_d}}{\\partial{net_j}}x_{ji} \\end{align}\\] 上式中，\\(x_{ji}\\)是节点\\(i\\)传递给节点\\(j\\)的输入值，也就是节点\\(i\\)的输出值。 对于\\(\\frac{\\partial{E_d}}{\\partial{net_j}}\\)的推导，需要区分输出层和隐藏层两种情况。 #### 5.2.1 输出层权值训练 对于输出层来说，\\(net_j\\)仅能通过节点\\(j\\)的输出值\\(y_j\\)来影响网络其它部分，也就是说\\(E_d\\)是\\(y_j\\)的函数，而\\(y_j\\)是\\(net_j\\)的函数，其中\\(y_j=sigmoid(net_j)\\)。所以我们可以再次使用链式求导法则： \\[\\begin{align} \\frac{\\partial{E_d}}{\\partial{net_j}}&amp;=\\frac{\\partial{E_d}}{\\partial{y_j}}\\frac{\\partial{y_j}}{\\partial{net_j}}\\\\ \\end{align}\\] 考虑上式第一项: \\[\\begin{align} \\frac{\\partial{E_d}}{\\partial{y_j}}&amp;=\\frac{\\partial}{\\partial{y_j}}\\frac{1}{2}\\sum_{i\\in outputs}(t_i-y_i)^2\\\\ &amp;=\\frac{\\partial}{\\partial{y_j}}\\frac{1}{2}(t_j-y_j)^2\\\\ &amp;=-(t_j-y_j) \\end{align}\\] 考虑上式第二项： \\[\\begin{align} \\frac{\\partial{y_j}}{\\partial{net_j}}&amp;=\\frac{\\partial sigmoid(net_j)}{\\partial{net_j}}\\\\ &amp;=y_j(1-y_j)\\\\ \\end{align}\\] 将第一项和第二项带入，得到： \\[\\frac{\\partial{E_d}}{\\partial{net_j}}=-(t_j-y_j)y_j(1-y_j)\\] 如果令\\(\\delta_j=-\\frac{\\partial{E_d}}{\\partial{net_j}}\\)，也就是一个节点的误差项\\(\\delta\\)是网络误差对这个节点输入的偏导数的相反数。带入上式，得到： \\[\\delta_j=(t_j-y_j)y_j(1-y_j)\\] 上式就是式3。 将上述推导带入随机梯度下降公式，得到： \\[\\begin{align} w_{ji}&amp;\\gets w_{ji}-\\eta\\frac{\\partial{E_d}}{\\partial{w_{ji}}}\\\\ &amp;=w_{ji}+\\eta(t_j-y_j)y_j(1-y_j)x_{ji}\\\\ &amp;=w_{ji}+\\eta\\delta_jx_{ji} \\end{align}\\] 上式就是式5。 5.2.2 隐藏层权值训练 现在我们要推导出隐藏层的\\(\\frac{\\partial{E_d}}{\\partial{net_j}}\\)。 首先，我们需要定义节点的所有直接下游节点的集合\\(Downstream(j)\\)。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到\\(net_j\\)只能通过影响\\(Downstream(j)\\)再影响\\(E_d\\)。设\\(net_k\\)是节点\\(j\\)的下游节点的输入，则\\(E_d\\)是\\(net_k\\)的函数，而\\(net_k\\)是\\(net_j\\)的函数。因为\\(net_k\\)有多个，我们应用全导数公式，可以做出如下推导： \\[\\begin{align} \\frac{\\partial{E_d}}{\\partial{net_j}}&amp;=\\sum_{k\\in Downstream(j)}\\frac{\\partial{E_d}}{\\partial{net_k}}\\frac{\\partial{net_k}}{\\partial{net_j}}\\\\ &amp;=\\sum_{k\\in Downstream(j)}-\\delta_k\\frac{\\partial{net_k}}{\\partial{net_j}}\\\\ &amp;=\\sum_{k\\in Downstream(j)}-\\delta_k\\frac{\\partial{net_k}}{\\partial{a_j}}\\frac{\\partial{a_j}}{\\partial{net_j}}\\\\ &amp;=\\sum_{k\\in Downstream(j)}-\\delta_kw_{kj}\\frac{\\partial{a_j}}{\\partial{net_j}}\\\\ &amp;=\\sum_{k\\in Downstream(j)}-\\delta_kw_{kj}a_j(1-a_j)\\\\ &amp;=-a_j(1-a_j)\\sum_{k\\in Downstream(j)}\\delta_kw_{kj} \\end{align}\\] 因为\\(\\delta_j=-\\frac{\\partial{E_d}}{\\partial{net_j}}\\)，带入上式得到： \\[\\delta_j=a_j(1-a_j)\\sum_{k\\in Downstream(j)}\\delta_kw_{kj}\\] 上式就是式4。 数学公式警报解除 至此，我们已经推导出了反向传播算法。需要注意的是，我们刚刚推导出的训练规则是根据激活函数是\\(sigmoid\\)函数、平方和误差、全连接网络、随机梯度下降优化算法。如果激活函数不同、误差计算方式不同、网络连接结构不同、优化算法不同，则具体的训练规则也会不一样。但是无论怎样，训练规则的推导方式都是一样的，应用链式求导法则进行推导即可。 6 神经网络的实现 完整代码请参考GitHub: GitHub-BP (python2.7) 现在，我们要根据前面的算法，实现一个基本的全连接神经网络，这并不需要太多代码。我们在这里依然采用面向对象设计。 首先，我们先做一个基本的模型： 如上图，可以分解出5个领域对象来实现神经网络： Network 神经网络对象，提供API接口。它由若干层对象组成以及连接对象组成。 Layer 层对象，由多个节点组成。 Node 节点对象计算和记录节点自身的信息(比如输出值\\(a\\)、误差项\\(\\delta\\)等)，以及与这个节点相关的上下游的连接。 Connection 每个连接对象都要记录该连接的权重。 Connections 仅仅作为Connection的集合对象，提供一些集合操作。 Node实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 节点类，负责记录和维护节点自身信息以及与这个节点相关的上下游连接，实现输出值和误差项的计算。class Node(object): def __init__(self, layer_index, node_index): ''' 构造节点对象。 layer_index: 节点所属的层的编号 node_index: 节点的编号 ''' self.layer_index = layer_index self.node_index = node_index self.downstream = [] self.upstream = [] self.output = 0 self.delta = 0 def set_output(self, output): ''' 设置节点的输出值。如果节点属于输入层会用到这个函数。 ''' self.output = output def append_downstream_connection(self, conn): ''' 添加一个到下游节点的连接 ''' self.downstream.append(conn) def append_upstream_connection(self, conn): ''' 添加一个到上游节点的连接 ''' self.upstream.append(conn) def calc_output(self): ''' 根据式1计算节点的输出 ''' output = reduce(lambda ret, conn: ret + conn.upstream_node.output * conn.weight, self.upstream, 0) self.output = sigmoid(output) def calc_hidden_layer_delta(self): ''' 节点属于隐藏层时，根据式4计算delta ''' downstream_delta = reduce( lambda ret, conn: ret + conn.downstream_node.delta * conn.weight, self.downstream, 0.0) self.delta = self.output * (1 - self.output) * downstream_delta def calc_output_layer_delta(self, label): ''' 节点属于输出层时，根据式3计算delta ''' self.delta = self.output * (1 - self.output) * (label - self.output) def __str__(self): ''' 打印节点的信息 ''' node_str = '%u-%u: output: %f delta: %f' % (self.layer_index, self.node_index, self.output, self.delta) downstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.downstream, '') upstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.upstream, '') return node_str + '\\n\\tdownstream:' + downstream_str + '\\n\\tupstream:' + upstream_str ConstNode对象，为了实现一个输出恒为1的节点(计算偏置项\\(w_b\\)时需要) 12345678910111213141516171819202122232425262728293031class ConstNode(object): def __init__(self, layer_index, node_index): ''' 构造节点对象。 layer_index: 节点所属的层的编号 node_index: 节点的编号 ''' self.layer_index = layer_index self.node_index = node_index self.downstream = [] self.output = 1 def append_downstream_connection(self, conn): ''' 添加一个到下游节点的连接 ''' self.downstream.append(conn) def calc_hidden_layer_delta(self): ''' 节点属于隐藏层时，根据式4计算delta ''' downstream_delta = reduce( lambda ret, conn: ret + conn.downstream_node.delta * conn.weight, self.downstream, 0.0) self.delta = self.output * (1 - self.output) * downstream_delta def __str__(self): ''' 打印节点的信息 ''' node_str = '%u-%u: output: 1' % (self.layer_index, self.node_index) downstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.downstream, '') return node_str + '\\n\\tdownstream:' + downstream_str Layer对象，负责初始化一层。此外，作为Node的集合对象，提供对Node集合的操作。 123456789101112131415161718192021222324252627282930class Layer(object): def __init__(self, layer_index, node_count): ''' 初始化一层 layer_index: 层编号 node_count: 层所包含的节点个数 ''' self.layer_index = layer_index self.nodes = [] for i in range(node_count): self.nodes.append(Node(layer_index, i)) self.nodes.append(ConstNode(layer_index, node_count)) def set_output(self, data): ''' 设置层的输出。当层是输入层时会用到。 ''' for i in range(len(data)): self.nodes[i].set_output(data[i]) def calc_output(self): ''' 计算层的输出向量 ''' for node in self.nodes[:-1]: node.calc_output() def dump(self): ''' 打印层的信息 ''' for node in self.nodes: print node Connection对象，主要职责是记录连接的权重，以及这个连接所关联的上下游节点。 12345678910111213141516171819202122232425262728293031323334353637class Connection(object): def __init__(self, upstream_node, downstream_node): ''' 初始化连接，权重初始化为是一个很小的随机数 upstream_node: 连接的上游节点 downstream_node: 连接的下游节点 ''' self.upstream_node = upstream_node self.downstream_node = downstream_node self.weight = random.uniform(-0.1, 0.1) self.gradient = 0.0 def calc_gradient(self): ''' 计算梯度 ''' self.gradient = self.downstream_node.delta * self.upstream_node.output def get_gradient(self): ''' 获取当前的梯度 ''' return self.gradient def update_weight(self, rate): ''' 根据梯度下降算法更新权重 ''' self.calc_gradient() self.weight += rate * self.gradient def __str__(self): ''' 打印连接信息 ''' return '(%u-%u) -&gt; (%u-%u) = %f' % ( self.upstream_node.layer_index, self.upstream_node.node_index, self.downstream_node.layer_index, self.downstream_node.node_index, self.weight) Connections对象，提供Connection集合操作。 12345678class Connections(object): def __init__(self): self.connections = [] def add_connection(self, connection): self.connections.append(connection) def dump(self): for conn in self.connections: print conn Network对象，提供API。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class Network(object): def __init__(self, layers): ''' 初始化一个全连接神经网络 layers: 二维数组，描述神经网络每层节点数 ''' self.connections = Connections() self.layers = [] layer_count = len(layers) node_count = 0; for i in range(layer_count): self.layers.append(Layer(i, layers[i])) for layer in range(layer_count - 1): connections = [Connection(upstream_node, downstream_node) for upstream_node in self.layers[layer].nodes for downstream_node in self.layers[layer + 1].nodes[:-1]] for conn in connections: self.connections.add_connection(conn) conn.downstream_node.append_upstream_connection(conn) conn.upstream_node.append_downstream_connection(conn) def train(self, labels, data_set, rate, iteration): ''' 训练神经网络 labels: 数组，训练样本标签。每个元素是一个样本的标签。 data_set: 二维数组，训练样本特征。每个元素是一个样本的特征。 ''' for i in range(iteration): for d in range(len(data_set)): self.train_one_sample(labels[d], data_set[d], rate) def train_one_sample(self, label, sample, rate): ''' 内部函数，用一个样本训练网络 ''' self.predict(sample) self.calc_delta(label) self.update_weight(rate) def calc_delta(self, label): ''' 内部函数，计算每个节点的delta ''' output_nodes = self.layers[-1].nodes for i in range(len(label)): output_nodes[i].calc_output_layer_delta(label[i]) for layer in self.layers[-2::-1]: for node in layer.nodes: node.calc_hidden_layer_delta() def update_weight(self, rate): ''' 内部函数，更新每个连接权重 ''' for layer in self.layers[:-1]: for node in layer.nodes: for conn in node.downstream: conn.update_weight(rate) def calc_gradient(self): ''' 内部函数，计算每个连接的梯度 ''' for layer in self.layers[:-1]: for node in layer.nodes: for conn in node.downstream: conn.calc_gradient() def get_gradient(self, label, sample): ''' 获得网络在一个样本下，每个连接上的梯度 label: 样本标签 sample: 样本输入 ''' self.predict(sample) self.calc_delta(label) self.calc_gradient() def predict(self, sample): ''' 根据输入的样本预测输出值 sample: 数组，样本的特征，也就是网络的输入向量 ''' self.layers[0].set_output(sample) for i in range(1, len(self.layers)): self.layers[i].calc_output() return map(lambda node: node.output, self.layers[-1].nodes[:-1]) def dump(self): ''' 打印网络信息 ''' for layer in self.layers: layer.dump() 至此，实现了一个基本的全连接神经网络。可以看到，同神经网络的强大学习能力相比，其实现还算是很容易的。 7 梯度检查 怎么保证自己写的神经网络没有BUG呢？事实上这是一个非常重要的问题。一方面，千辛万苦想到一个算法，结果效果不理想，那么是算法本身错了还是代码实现错了呢？定位这种问题肯定要花费大量的时间和精力。另一方面，由于神经网络的复杂性，我们几乎无法事先知道神经网络的输入和输出，因此类似TDD(测试驱动开发)这样的开发方法似乎也不可行。 办法还是有滴，就是利用梯度检查来确认程序是否正确。梯度检查的思路如下： 对于梯度下降算法： \\[w_{ji}\\gets w_{ji}-\\eta\\frac{\\partial{E_d}}{\\partial{w_{ji}}}\\] 来说，这里关键之处在于\\(\\frac{\\partial{E_d}}{\\partial{w_{ji}}}\\)的计算一定要正确，而它是\\(E_d\\)对\\(w_{ji}\\)的偏导数。而根据导数的定义： \\[f&#39;(\\theta)=\\lim_{\\epsilon-&gt;0}\\frac{f(\\theta+\\epsilon)-f(\\theta-\\epsilon)}{2\\epsilon}\\] 对于任意的\\(\\theta\\)导数值，我们都可以用等式右边来近似计算。我们把\\(E_d\\)看做是\\(w_{ji}\\)的函数，即\\(E_d(w_{ji})\\)，那么根据导数定义，\\(\\frac{\\partial{E_d(w_{ji})}}{\\partial{w_{ji}}}\\)应该等于： \\[\\frac{\\partial{E_d(w_{ji})}}{\\partial{w_{ji}}}=\\lim_{\\epsilon-&gt;0}\\frac{f(w_{ji}+\\epsilon)-f(w_{ji}-\\epsilon)}{2\\epsilon}\\] 如果把\\(\\epsilon\\)设置为一个很小的数（比如\\(10^{-4}\\)），那么上式可以写成： \\[\\frac{\\partial{E_d(w_{ji})}}{\\partial{w_{ji}}}\\approx\\frac{f(w_{ji}+\\epsilon)-f(w_{ji}-\\epsilon)}{2\\epsilon}\\qquad(式6)\\] 我们就可以利用式6，来计算梯度\\(\\frac{\\partial{E_d}}{\\partial{w_{ji}}}\\)的值，然后同我们神经网络代码中计算出来的梯度值进行比较。如果两者的差别非常的小，那么就说明我们的代码是正确的。 下面是梯度检查的代码。如果我们想检查参数\\(w_{ji}\\)的梯度是否正确，我们需要以下几个步骤： 首先使用一个样本\\(d\\)对神经网络进行训练，这样就能获得每个权重的梯度。 将\\(w_{ji}\\)加上一个很小的值(\\(10^{-4}\\))，重新计算神经网络在这个样本\\(d\\)下的\\(E_{d+}\\)。 将\\(w_{ji}\\)减上一个很小的值(\\(10^{-4}\\))，重新计算神经网络在这个样本\\(d\\)下的\\(E_{d-}\\)。 根据式6计算出期望的梯度值，和第一步获得的梯度值进行比较，它们应该几乎想等(至少4位有效数字相同)。 当然，我们可以重复上面的过程，对每个权重\\(w_{ji}\\)都进行检查。也可以使用多个样本重复检查。 123456789101112131415161718192021222324252627282930def gradient_check(network, sample_feature, sample_label): ''' 梯度检查 network: 神经网络对象 sample_feature: 样本的特征 sample_label: 样本的标签 ''' # 计算网络误差 network_error = lambda vec1, vec2: \\ 0.5 * reduce(lambda a, b: a + b, map(lambda v: (v[0] - v[1]) * (v[0] - v[1]), zip(vec1, vec2))) # 获取网络在当前样本下每个连接的梯度 network.get_gradient(sample_feature, sample_label) # 对每个权重做梯度检查 for conn in network.connections.connections: # 获取指定连接的梯度 actual_gradient = conn.get_gradient() # 增加一个很小的值，计算网络的误差 epsilon = 0.0001 conn.weight += epsilon error1 = network_error(network.predict(sample_feature), sample_label) # 减去一个很小的值，计算网络的误差 conn.weight -= 2 * epsilon # 刚才加过了一次，因此这里需要减去2倍 error2 = network_error(network.predict(sample_feature), sample_label) # 根据式6计算期望的梯度值 expected_gradient = (error2 - error1) / (2 * epsilon) # 打印 print 'expected gradient: \\t%f\\nactual gradient: \\t%f' % ( expected_gradient, actual_gradient) 至此，会推导、会实现、会抓BUG，你已经摸到深度学习的大门了。接下来还需要不断的实践，我们用刚刚写过的神经网络去识别手写数字。 8 神经网络实战——手写数字识别 针对这个任务，我们采用业界非常流行的MNIST数据集。MNIST大约有60000个手写字母的训练样本，我们使用它训练我们的神经网络，然后再用训练好的网络去识别手写数字。 手写数字识别是个比较简单的任务，数字只可能是0-9中的一个，这是个10分类问题。 8.1 超参数的确定 我们首先需要确定网络的层数和每层的节点数。关于第一个问题，实际上并没有什么理论化的方法，大家都是根据经验来拍，如果没有经验的话就随便拍一个。然后，你可以多试几个值，训练不同层数的神经网络，看看哪个效果最好就用哪个。嗯，现在你可能明白为什么说深度学习是个手艺活了，有些手艺很让人无语，而有些手艺还是很有技术含量的。 不过，有些基本道理我们还是明白的，我们知道网络层数越多越好，也知道层数越多训练难度越大。对于全连接网络，隐藏层最好不要超过三层。那么，我们可以先试试仅有一个隐藏层的神经网络效果怎么样。毕竟模型小的话，训练起来也快些(刚开始玩模型的时候，都希望快点看到结果)。 输入层节点数是确定的。因为MNIST数据集每个训练数据是28*28的图片，共784个像素，因此，输入层节点数应该是784，每个像素对应一个输入节点。 输出层节点数也是确定的。因为是10分类，我们可以用10个节点，每个节点对应一个分类。输出层10个节点中，输出最大值的那个节点对应的分类，就是模型的预测结果。 隐藏层节点数量是不好确定的，从1到100万都可以。下面有几个经验公式： \\[\\begin{align} &amp;m=\\sqrt{n+l}+\\alpha\\\\ &amp;m=log_2n\\\\ &amp;m=\\sqrt{nl}\\\\ &amp;m:隐藏层节点数\\\\ &amp;n:输入层节点数\\\\ &amp;l:输出层节点数\\\\ &amp;\\alpha:1到10之间的常数 \\end{align}\\] 因此，我们可以先根据上面的公式设置一个隐藏层节点数。如果有时间，我们可以设置不同的节点数，分别训练，看看哪个效果最好就用哪个。我们先拍一个，设隐藏层节点数为300吧。 对于3层\\(784*300*10\\)的全连接网络，总共有\\(300*(784+1)+10*(300+1)=238510\\)个参数！神经网络之所以强大，是它提供了一种非常简单的方法去实现大量的参数。目前百亿参数、千亿样本的超大规模神经网络也是有的。因为MNIST只有6万个训练样本，参数太多了很容易过拟合，效果反而不好。 8.2 模型的训练和评估 MNIST数据集包含10000个测试样本。我们先用60000个训练样本训练我们的网络，然后再用测试样本对网络进行测试，计算识别错误率： \\[错误率=\\frac{错误预测样本数}{总样本数}\\] 我们每训练10轮，评估一次准确率。当准确率开始下降时（出现了过拟合）终止训练。 8.3 代码实现 首先，我们需要把MNIST数据集处理为神经网络能够接受的形式。MNIST训练集的文件格式可以参考官方网站，这里不在赘述。每个训练样本是一个28*28的图像，我们按照行优先，把它转化为一个784维的向量。每个标签是0-9的值，我们将其转换为一个10维的one-hot向量：如果标签值为\\(n\\)，我们就把向量的第\\(n\\)维（从0开始编号）设置为0.9，而其它维设置为0.1。例如，向量[0.1,0.1,0.9,0.1,0.1,0.1,0.1,0.1,0.1,0.1]表示值2。 下面是处理MNIST数据的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899#!/usr/bin/env Python# -*- coding: UTF-8 -*-import structfrom bp import *from datetime import datetime# 数据加载器基类class Loader(object): def __init__(self, path, count): ''' 初始化加载器 path: 数据文件路径 count: 文件中的样本个数 ''' self.path = path self.count = count def get_file_content(self): ''' 读取文件内容 ''' f = open(self.path, 'rb') content = f.read() f.close() return content def to_int(self, byte): ''' 将unsigned byte字符转换为整数 ''' return struct.unpack('B', byte)[0]# 图像数据加载器class ImageLoader(Loader): def get_picture(self, content, index): ''' 内部函数，从文件中获取图像 ''' start = index * 28 * 28 + 16 picture = [] for i in range(28): picture.append([]) for j in range(28): picture[i].append( self.to_int(content[start + i * 28 + j])) return picture def get_one_sample(self, picture): ''' 内部函数，将图像转化为样本的输入向量 ''' sample = [] for i in range(28): for j in range(28): sample.append(picture[i][j]) return sample def load(self): ''' 加载数据文件，获得全部样本的输入向量 ''' content = self.get_file_content() data_set = [] for index in range(self.count): data_set.append( self.get_one_sample( self.get_picture(content, index))) return data_set# 标签数据加载器class LabelLoader(Loader): def load(self): ''' 加载数据文件，获得全部样本的标签向量 ''' content = self.get_file_content() labels = [] for index in range(self.count): labels.append(self.norm(content[index + 8])) return labels def norm(self, label): ''' 内部函数，将一个值转换为10维标签向量 ''' label_vec = [] label_value = self.to_int(label) for i in range(10): if i == label_value: label_vec.append(0.9) else: label_vec.append(0.1) return label_vecdef get_training_data_set(): ''' 获得训练数据集 ''' image_loader = ImageLoader('train-images-idx3-ubyte', 60000) label_loader = LabelLoader('train-labels-idx1-ubyte', 60000) return image_loader.load(), label_loader.load()def get_test_data_set(): ''' 获得测试数据集 ''' image_loader = ImageLoader('t10k-images-idx3-ubyte', 10000) label_loader = LabelLoader('t10k-labels-idx1-ubyte', 10000) return image_loader.load(), label_loader.load() 网络的输出是一个10维向量，这个向量第\\(n\\)个(从0开始编号)元素的值最大，那么\\(n\\)就是网络的识别结果。下面是代码实现： 12345678def get_result(vec): max_value_index = 0 max_value = 0 for i in range(len(vec)): if vec[i] &gt; max_value: max_value = vec[i] max_value_index = i return max_value_index 我们使用错误率来对网络进行评估，下面是代码实现： 123456789def evaluate(network, test_data_set, test_labels): error = 0 total = len(test_data_set) for i in range(total): label = get_result(test_labels[i]) predict = get_result(network.predict(test_data_set[i])) if label != predict: error += 1 return float(error) / float(total) 最后实现我们的训练策略：每训练10轮，评估一次准确率，当准确率开始下降时终止训练。下面是代码实现： 12345678910111213141516171819def train_and_evaluate(): last_error_ratio = 1.0 epoch = 0 train_data_set, train_labels = get_training_data_set() test_data_set, test_labels = get_test_data_set() network = Network([784, 300, 10]) while True: epoch += 1 network.train(train_labels, train_data_set, 0.3, 1) print '%s epoch %d finished' % (now(), epoch) if epoch % 10 == 0: error_ratio = evaluate(network, test_data_set, test_labels) print '%s after epoch %d, error ratio is %f' % (now(), epoch, error_ratio) if error_ratio &gt; last_error_ratio: break else: last_error_ratio = error_ratioif __name__ == '__main__': train_and_evaluate() 在我的机器上测试了一下，1个epoch大约需要9000多秒，所以要对代码做很多的性能优化工作（比如用向量化编程）。训练要很久很久，可以把它上传到服务器上，在tmux的session里面去运行。为了防止异常终止导致前功尽弃，我们每训练10轮，就把获得参数值保存在磁盘上，以便后续可以恢复。(代码略) 9 向量化编程 完整代码请参考GitHub: GitHub-fc (python2.7) 在经历了漫长的训练之后，我们可能会想到，肯定有更好的办法！是的，程序员们，现在我们需要告别面向对象编程了，转而去使用另外一种更适合深度学习算法的编程方式：向量化编程。主要有两个原因：一个是我们事实上并不需要真的去定义Node、Connection这样的对象，直接把数学计算实现了就可以了；另一个原因，是底层算法库会针对向量运算做优化（甚至有专用的硬件，比如GPU），程序效率会提升很多。所以，在深度学习的世界里，我们总会想法设法的把计算表达为向量的形式。我相信优秀的程序员不会把自己拘泥于某种（自己熟悉的）编程范式上，而会去学习并使用最为合适的范式。 下面，我们用向量化编程的方法，重新实现前面的全连接神经网络。 首先，我们需要把所有的计算都表达为向量的形式。对于全连接神经网络来说，主要有三个计算公式。 前向计算，我们发现式2已经是向量化的表达了： \\[\\vec{a}=\\sigma(W\\centerdot\\vec{x})\\qquad (式2)\\] 上式中的\\(\\sigma\\)表示sigmoid函数。 反向计算，我们需要把式3和式4使用向量来表示： \\[\\vec{\\delta}=\\vec{y}(1-\\vec{y})(\\vec{t}-\\vec{y})\\qquad(式7)\\\\ \\vec{\\delta^{(l)}}=\\vec{a}^{(l)}(1-\\vec{a}^{(l)})W^T\\delta^{(l+1)}\\qquad(式8)\\] 在式8中，\\(\\delta^{(l)}\\)表示第\\(l\\)层的误差项；\\(W^T\\)表示矩阵\\(W\\)的转置。 我们还需要权重数组\\(W\\)和偏置项b的梯度计算的向量化表示。也就是需要把式5使用向量化表示： \\[w_{ji}\\gets w_{ji}+\\eta\\delta_jx_{ji}\\qquad(式5)\\] 其对应的向量化表示为： \\[W \\gets W + \\eta\\vec{\\delta}\\vec{x}^T\\qquad(式9)\\] 更新偏置项的向量化表示为： \\[\\vec{b} \\gets \\vec{b} + \\eta\\vec{\\delta}\\qquad(式10)\\] 现在，我们根据上面几个公式，重新实现一个类：FullConnectedLayer。它实现了全连接层的前向和后向计算： 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 全连接层实现类class FullConnectedLayer(object): def __init__(self, input_size, output_size, activator): ''' 构造函数 input_size: 本层输入向量的维度 output_size: 本层输出向量的维度 activator: 激活函数 ''' self.input_size = input_size self.output_size = output_size self.activator = activator # 权重数组W self.W = np.random.uniform(-0.1, 0.1, (output_size, input_size)) # 偏置项b self.b = np.zeros((output_size, 1)) # 输出向量 self.output = np.zeros((output_size, 1)) def forward(self, input_array): ''' 前向计算 input_array: 输入向量，维度必须等于input_size ''' # 式2 self.input = input_array self.output = self.activator.forward( np.dot(self.W, input_array) + self.b) def backward(self, delta_array): ''' 反向计算W和b的梯度 delta_array: 从上一层传递过来的误差项 ''' # 式8 self.delta = self.activator.backward(self.input) * np.dot( self.W.T, delta_array) self.W_grad = np.dot(delta_array, self.input.T) self.b_grad = delta_array def update(self, learning_rate): ''' 使用梯度下降算法更新权重 ''' self.W += learning_rate * self.W_grad self.b += learning_rate * self.b_grad 上面这个类一举取代了原先的Layer、Node、Connection等类，不但代码更加容易理解，而且运行速度也快了几百倍。 现在，我们对Network类稍作修改，使之用到FullConnectedLayer： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# Sigmoid激活函数类class SigmoidActivator(object): def forward(self, weighted_input): return 1.0 / (1.0 + np.exp(-weighted_input)) def backward(self, output): return output * (1 - output)# 神经网络类class Network(object): def __init__(self, layers): ''' 构造函数 ''' self.layers = [] for i in range(len(layers) - 1): self.layers.append( FullConnectedLayer( layers[i], layers[i+1], SigmoidActivator() ) ) def predict(self, sample): ''' 使用神经网络实现预测 sample: 输入样本 ''' output = sample for layer in self.layers: layer.forward(output) output = layer.output return output def train(self, labels, data_set, rate, epoch): ''' 训练函数 labels: 样本标签 data_set: 输入样本 rate: 学习速率 epoch: 训练轮数 ''' for i in range(epoch): for d in range(len(data_set)): self.train_one_sample(labels[d], data_set[d], rate) def train_one_sample(self, label, sample, rate): self.predict(sample) self.calc_gradient(label) self.update_weight(rate) def calc_gradient(self, label): delta = self.layers[-1].activator.backward( self.layers[-1].output ) * (label - self.layers[-1].output) for layer in self.layers[::-1]: layer.backward(delta) delta = layer.delta return delta def update_weight(self, rate): for layer in self.layers: layer.update(rate) 现在，Network类也清爽多了，用我们的新代码再次训练一下MNIST数据集吧。 10 小结 至此，你已经完成了又一次漫长的学习之旅。你现在应该已经明白了神经网络的基本原理，高兴的话，你甚至有能力去动手实现一个，并用它解决一些问题。如果感到困难也不要气馁，这篇文章是一个重要的分水岭，如果你完全弄明白了的话，在真正的『小白』和装腔作势的『大牛』面前吹吹牛是完全没有问题的。 作为深度学习入门的系列文章，本文也是上半场的结束。在这个半场，你掌握了机器学习、神经网络的基本概念，并且有能力去动手解决一些简单的问题（例如手写数字识别，如果用传统的观点来看，其实这些问题也不简单）。而且，一旦掌握基本概念，后面的学习就容易多了。 在下半场，我们讲介绍更多『深度』学习的内容，我们已经讲了神经网络(Neutrol Network)，但是并没有讲深度神经网络(Deep Neutrol Network)。Deep会带来更加强大的能力，同时也带来更多的问题。如果不理解这些问题和它们的解决方案，也不能说你入门了『深度』学习。 目前业界有很多开源的神经网络实现，它们的功能也要强大的多，因此你并不需要事必躬亲的去实现自己的神经网络。我们在上半场不断的从头发明轮子，是为了让你明白神经网络的基本原理，这样你就能非常迅速的掌握这些工具。在下半场的文章中，我们改变了策略：不会再去从头开始去实现，而是尽可能应用现有的工具。 下一篇文章，我们介绍不同结构的神经网络，比如鼎鼎大名的卷积神经网络，它在图像和语音领域已然创造了诸多奇迹，在自然语言处理领域的研究也如火如荼。某种意义上说，它的成功大大提升了人们对于深度学习的信心。","categories":[{"name":"科研","slug":"科研","permalink":"http://otter668@github.io/categories/科研/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://otter668@github.io/tags/DeepLearning/"},{"name":"ANN","slug":"ANN","permalink":"http://otter668@github.io/tags/ANN/"},{"name":"BP","slug":"BP","permalink":"http://otter668@github.io/tags/BP/"}]},{"title":"深度学习系列（2）","slug":"线性单元和梯度下降","date":"2017-12-19T12:44:32.090Z","updated":"2017-12-20T07:33:32.514Z","comments":true,"path":"2017/12/19/线性单元和梯度下降/","link":"","permalink":"http://otter668@github.io/2017/12/19/线性单元和梯度下降/","excerpt":"线性单元和梯度下降 0. 往期回顾 在上一篇文章中，我们已经学会了编写一个简单的感知器，并用它来实现一个线性分类器。你应该还记得用来训练感知器的『感知器规则』。然而，我们并没有关心这个规则是怎么得到的。本文通过介绍另外一种『感知器』，也就是『线性单元』，来说明关于机器学习一些基本的概念，比如模型、目标函数、优化算法等等。这些概念对于所有的机器学习算法来说都是通用的，掌握了这些概念，就掌握了机器学习的基本套路。","text":"线性单元和梯度下降 0. 往期回顾 在上一篇文章中，我们已经学会了编写一个简单的感知器，并用它来实现一个线性分类器。你应该还记得用来训练感知器的『感知器规则』。然而，我们并没有关心这个规则是怎么得到的。本文通过介绍另外一种『感知器』，也就是『线性单元』，来说明关于机器学习一些基本的概念，比如模型、目标函数、优化算法等等。这些概念对于所有的机器学习算法来说都是通用的，掌握了这些概念，就掌握了机器学习的基本套路。 1. 线性单元是啥 感知器有一个问题，当面对的数据集不是线性可分的时候，『感知器规则』可能无法收敛，这意味着我们永远也无法完成一个感知器的训练。为了解决这个问题，我们使用一个可导的线性函数来替代感知器的阶跃函数，这种感知器就叫做线性单元。线性单元在面对线性不可分的数据集时，会收敛到一个最佳的近似上。 为了简单起见，我们可以设置线性单元的激活函数\\(f\\)为 \\[f(x)=x\\] 这样的线性单元如下图所示 对比此前我们讲过的感知器 这样替换了激活函数\\(f\\)之后，线性单元将返回一个实数值而不是0,1分类。因此线性单元用来解决回归问题而不是分类问题。 ## 2. 线性单元的模型 当我们说模型时，我们实际上在谈论根据输入\\(x\\)预测输出\\(y\\)的算法。比如，\\(x\\)可以是一个人的工作年限，\\(y\\)可以是他的月薪，我们可以用某种算法来根据一个人的工作年限来预测他的收入。比如： \\[y=h(x)=w*x+b\\] 函数\\(h(x)\\)叫做假设，而\\(w\\)、\\(b\\)是它的参数。我们假设参数\\(w=1000\\)，参数\\(b=500\\)，如果一个人的工作年限是5年的话，我们的模型会预测他的月薪为 \\[y=h(x)=1000*5+500=5500(元)\\] 你也许会说，这个模型太不靠谱了。是这样的，因为我们考虑的因素太少了，仅仅包含了工作年限。如果考虑更多的因素，比如所处的行业、公司、职级等等，可能预测就会靠谱的多。我们把工作年限、行业、公司、职级这些信息，称之为特征。对于一个工作了5年，在IT行业，百度工作，职级T6这样的人，我们可以用这样的一个特征向量来表示他 \\[x = (5, IT, 百度, T6)\\] 既然输入\\(x\\)变成了一个具备四个特征的向量，相对应的，仅仅一个参数\\(w\\)就不够用了，我们应该使用4个参数\\(w_1\\),\\(w_2\\),\\(w_3\\),\\(w_4\\)，每个特征对应一个。这样，我们的模型就变成 \\[y=h(x)=w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4+b\\] 其中，\\(x_1\\)对应工作年限，\\(x_2\\)对应行业，\\(x_3\\)对应公司，\\(x_4\\)对应职级。 为了书写和计算方便，我们可以令\\(w_0\\)等于\\(b\\)，同时令\\(w_0\\)对应于特征\\(x_0\\)。由于\\(x_0\\)其实并不存在，我们可以令它的值永远为1。也就是说 \\[b=w_0*x_0\\qquad 其中x_0=1\\] 这样上面的式子就可以写成 \\[\\begin{align} y=h(x)&amp;=w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4+b\\\\ &amp;=w_0*x_0+w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4 \\end{align}\\] 我们还可以把上式写成向量的形式 \\[y=h(x)=\\mathrm{w}^T\\mathrm{x}\\qquad\\qquad(式1)\\] 长成这种样子模型就叫做线性模型，因为输出\\(y\\)就是输入特征\\(x_1\\),\\(x_2\\),\\(x_3\\),…的线性组合。 ## 3. 监督学习和无监督学习 接下来，我们需要关心的是这个模型如何训练，也就是参数\\(w\\)取什么值最合适。 机器学习有一类学习方法叫做监督学习，它是说为了训练一个模型，我们要提供这样一堆训练样本：每个训练样本既包括输入特征\\(x\\)，也包括对应的输出\\(y\\)(\\(y\\)也叫做标记，label)。也就是说，我们要找到很多人，我们既知道他们的特征(工作年限，行业…)，也知道他们的收入。我们用这样的样本去训练模型，让模型既看到我们提出的每个问题(输入特征\\(x\\))，也看到对应问题的答案(标记\\(y\\))。当模型看到足够多的样本之后，它就能总结出其中的一些规律。然后，就可以预测那些它没看过的输入所对应的答案了。 另外一类学习方法叫做无监督学习，这种方法的训练样本中只有\\(x\\)而没有\\(y\\)。模型可以总结出特征\\(x\\)的一些规律，但是无法知道其对应的答案\\(y\\)。 很多时候，既有\\(x\\)又有\\(y\\)的训练样本是很少的，大部分样本都只有\\(x\\)。比如在语音到文本(STT)的识别任务中，\\(x\\)是语音，\\(y\\)是这段语音对应的文本。我们很容易获取大量的语音录音，然而把语音一段一段切分好并标注上对应文字则是非常费力气的事情。这种情况下，为了弥补带标注样本的不足，我们可以用无监督学习方法先做一些聚类，让模型总结出哪些音节是相似的，然后再用少量的带标注的训练样本，告诉模型其中一些音节对应的文字。这样模型就可以把相似的音节都对应到相应文字上，完成模型的训练。 ## 4. 线性单元的目标函数 现在，让我们只考虑监督学习。 在监督学习下，对于一个样本，我们知道它的特征\\(x\\)，以及标记\\(y\\)。同时，我们还可以根据模型\\(h(x)\\)计算得到输出\\(\\bar{y}\\)。注意这里面我们用\\(y\\)表示训练样本里面的标记，也就是实际值；用带上划线\\(\\bar{y}\\)的表示模型计算的出来的预测值。我们当然希望模型计算出来的\\(\\bar{y}\\)和\\(y\\)越接近越好。 数学上有很多方法来表示的\\(\\bar{y}\\)和\\(y\\)的接近程度，比如我们可以用\\(\\bar{y}\\)和\\(y\\)的差的平方的\\(\\frac{1}{2}\\)来表示它们的接近程度 \\[e=\\frac{1}{2}(y-\\bar{y})^2\\] 我们把\\(e\\)叫做单个样本的误差。至于为什么前面要乘\\(\\frac{1}{2}\\)，是为了后面计算方便。 训练数据中会有很多样本，比如\\(N\\)个，我们可以用训练数据中所有样本的误差的和，来表示模型的误差\\(E\\)，也就是 \\[E=e^{(1)}+e^{(2)}+e^{(3)}+...+e^{(n)}\\] 上式的\\(e^{(1)}\\)表示第一个样本的误差，\\(e^{(2)}\\)表示第二个样本的误差……。 我们还可以把上面的式子写成和式的形式。使用和式，不光书写起来简单，逼格也跟着暴涨，一举两得。所以一定要写成下面这样 \\[\\begin{align} E&amp;=e^{(1)}+e^{(2)}+e^{(3)}+...+e^{(n)}\\\\ &amp;=\\sum_{i=1}^{n}e^{(i)}\\\\ &amp;=\\frac{1}{2}\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})^2\\qquad\\qquad(式2) \\end{align}\\] 其中 \\[\\begin{align} \\bar{y}^{(i)}&amp;=h(\\mathrm{x}^{(i)})\\\\ &amp;=\\mathrm{w}^T\\mathrm{x^{(i)}} \\end{align}\\] (式2)中，\\(x^{(i)}\\)表示第个\\(i\\)训练样本的特征，\\(y^{(i)}\\)表示第\\(i\\)个样本的标记，我们也可以用元组\\((x^{(i)},y^{(i)})\\)表示第\\(i\\)训练样本。\\(\\bar{y}^{(i)}\\)则是模型对第\\(i\\)个样本的预测值。 我们当然希望对于一个训练数据集来说，误差最小越好，也就是(式2)的值越小越好。对于特定的训练数据集来说，\\((x^{(i)},y^{(i)})\\)的值都是已知的，所以(式2)其实是参数\\(w\\)的函数。 \\[\\begin{align} E(\\mathrm{w})&amp;=\\frac{1}{2}\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})^2\\\\ &amp;=\\frac{1}{2}\\sum_{i=1}^{n}(\\mathrm{y^{(i)}-\\mathrm{w}^Tx^{(i)}})^2 \\end{align}\\] 由此可见，模型的训练，实际上就是求取到合适的\\(w\\)，使(式2)取得最小值。这在数学上称作优化问题，而\\(E(w)\\)就是我们优化的目标，称之为目标函数。 5. 梯度下降优化算法 大学时我们学过怎样求函数的极值。函数\\(y=f(x)\\)的极值点，就是它的导数\\(f&#39;(x)=0\\)的那个点。因此我们可以通过解方程\\(f&#39;(x)=0\\)，求得函数的极值点\\((x_0,y_0)\\)。 不过对于计算机来说，它可不会解方程。但是它可以凭借强大的计算能力，一步一步的去把函数的极值点『试』出来。如下图所示： 首先，我们随便选择一个点开始，比如上图的\\(x_0\\)点。接下来，每次迭代修改\\(x\\)的为\\(x_1\\),\\(x_2\\),\\(x_3\\),…，经过数次迭代后最终达到函数最小值点。 你可能要问了，为啥每次修改\\(x\\)的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数\\(y=f(x)\\)的梯度的相反方向来修改\\(x\\)。什么是梯度呢？翻开大学高数课的课本，我们会发现梯度是一个向量，它指向函数值上升最快的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改\\(x\\)的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。 按照上面的讨论，我们就可以写出梯度下降算法的公式 \\[\\mathrm{x}_{new}=\\mathrm{x}_{old}-\\eta\\nabla{f(x)}\\] 其中，\\(\\nabla\\)是梯度算子，\\(\\nabla{f(x)}\\)就是指\\(f(x)\\)的梯度。\\(\\eta\\)是步长，也称作学习速率。 对于上一节列出的目标函数(式2) \\[E(\\mathrm{w})=\\frac{1}{2}\\sum_{i=1}^{n}(\\mathrm{y^{(i)}-\\bar{y}^{(i)}})^2\\] 梯度下降算法可以写成 \\[\\mathrm{w}_{new}=\\mathrm{w}_{old}-\\eta\\nabla{E(\\mathrm{w})}\\] 聪明的你应该能想到，如果要求目标函数的最大值，那么我们就应该用梯度上升算法，它的参数修改规则是 \\[\\mathrm{w}_{new}=\\mathrm{w}_{old}+\\eta\\nabla{E(\\mathrm{w})}\\] 下面，请先做几次深呼吸，让你的大脑补充足够的新鲜的氧气，我们要来求取\\(\\nabla{E}(\\mathrm{w})\\)，然后带入上式，就能得到线性单元的参数修改规则。 关于\\(\\nabla{E}(\\mathrm{w})\\)的推导过程，我单独把它们放到一节中。您既可以选择慢慢看，也可以选择无视。在这里，您只需要知道，经过一大串推导，目标函数\\(E(w)\\)的梯度是 \\[\\nabla{E(\\mathrm{w})}=-\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})\\mathrm{x}^{(i)}\\] 因此，线性单元的参数修改规则最后是这个样子 \\[\\mathrm{w}_{new}=\\mathrm{w}_{old}+\\eta\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})\\mathrm{x}^{(i)}\\qquad\\qquad(式3)\\] 有了上面这个式子，我们就可以根据它来写出训练线性单元的代码了。 需要说明的是，如果每个样本有M个特征，则上式中的\\(\\mathrm{x},\\mathrm{w}\\)都是M+1维向量(因为我们加上了一个恒为1的虚拟特征\\(x_0\\)，参考前面的内容)，而\\(y\\)是标量。用高逼格的数学符号表示，就是 \\[\\mathrm{x},\\mathrm{w}\\in\\Re^{(M+1)}\\\\ y\\in\\Re^1\\] 为了让您看明白说的是啥，我吐血写下下面这个解释(写这种公式可累可累了)。因为\\(\\mathrm{w},\\mathrm{x}\\)是M+1维列向量，所以(式3)可以写成 \\[\\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ ... \\\\ w_m \\\\ \\end{bmatrix}_{new}= \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ ... \\\\ w_m \\\\ \\end{bmatrix}_{old}+\\eta\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)}) \\begin{bmatrix} 1 \\\\ x_1^{(i)} \\\\ x_2^{(i)} \\\\ ... \\\\ x_m^{(i)} \\\\ \\end{bmatrix}\\] 如果您还是没看明白，建议您也吐血再看一下大学时学过的《线性代数》吧。 6. \\(\\nabla{E}(\\mathrm{w})\\)的推导 这一节你尽可以跳过它，并不太会影响到全文的理解。当然如果你非要弄明白每个细节，那恭喜你骚年，机器学习的未来一定是属于你的。 首先，我们先做一个简单的前戏。我们知道函数的梯度的定义就是它相对于各个变量的偏导数，所以我们写下下面的式子 \\[\\begin{align} \\nabla{E(\\mathrm{w})}&amp;=\\frac{\\partial}{\\partial\\mathrm{w}}E(\\mathrm{w})\\\\ &amp;=\\frac{\\partial}{\\partial\\mathrm{w}}\\frac{1}{2}\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})^2\\\\ \\end{align}\\] 可接下来怎么办呢？我们知道和的导数等于导数的和，所以我们可以先把求和符号\\(\\sum\\)里面的导数求出来，然后再把它们加在一起就行了，也就是 \\[\\begin{align} &amp;\\frac{\\partial}{\\partial\\mathrm{w}}\\frac{1}{2}\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})^2\\\\ =&amp;\\frac{1}{2}\\sum_{i=1}^{n}\\frac{\\partial}{\\partial\\mathrm{w}}(y^{(i)}-\\bar{y}^{(i)})^2\\\\ \\end{align}\\] 现在我们可以不管高大上的\\(\\sum\\)了，先专心把里面的导数求出来。 \\[\\begin{align} &amp;\\frac{\\partial}{\\partial\\mathrm{w}}(y^{(i)}-\\bar{y}^{(i)})^2\\\\ =&amp;\\frac{\\partial}{\\partial\\mathrm{w}}(y^{(i)2}-2\\bar{y}^{(i)}y^{(i)}+\\bar{y}^{(i)2})\\\\ \\end{align}\\] 我们知道，\\(y\\)是与\\(w\\)无关的常数，而\\(\\bar{y}=\\mathrm{w}^T\\mathrm{x}\\)，下面我们根据链式求导法则来求导(上大学时好像叫复合函数求导法则) \\[\\frac{\\partial{E(\\mathrm{w})}}{\\partial\\mathrm{w}}=\\frac{\\partial{E(\\bar{y})}}{\\partial\\bar{y}}\\frac{\\partial{\\bar{y}}}{\\partial\\mathrm{w}}\\] 我们分别计算上式等号右边的两个偏导数 \\[\\begin{align} \\frac{\\partial{E(\\mathrm{w})}}{\\partial\\bar{y}}= &amp;\\frac{\\partial}{\\partial\\bar{y}}(y^{(i)2}-2\\bar{y}^{(i)}y^{(i)}+\\bar{y}^{(i)2})\\\\ =&amp;-2y^{(i)}+2\\bar{y}^{(i)}\\\\\\\\ \\frac{\\partial{\\bar{y}}}{\\partial\\mathrm{w}}= &amp;\\frac{\\partial}{\\partial\\mathrm{w}}\\mathrm{w}^T\\mathrm{x}\\\\ =&amp;\\mathrm{x} \\end{align}\\] 代入，我们求得\\(\\sum\\)里面的偏导数是 \\[\\begin{align} &amp;\\frac{\\partial}{\\partial\\mathrm{w}}(y^{(i)}-\\bar{y}^{(i)})^2\\\\ =&amp;2(-y^{(i)}+\\bar{y}^{(i)})\\mathrm{x} \\end{align}\\] 最后代入\\(\\nabla{E}(\\mathrm{w})\\)，求得 \\[\\begin{align} \\nabla{E(\\mathrm{w})}&amp;=\\frac{1}{2}\\sum_{i=1}^{n}\\frac{\\partial}{\\partial\\mathrm{w}}(y^{(i)}-\\bar{y}^{(i)})^2\\\\ &amp;=\\frac{1}{2}\\sum_{i=1}^{n}2(-y^{(i)}+\\bar{y}^{(i)})\\mathrm{x}\\\\ &amp;=-\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})\\mathrm{x} \\end{align}\\] 至此，大功告成。 7. 随机梯度下降算法(Stochastic Gradient Descent, SGD) 如果我们根据(式3)来训练模型，那么我们每次更新\\(\\mathrm{w}\\)的迭代，要遍历训练数据中所有的样本进行计算，我们称这种算法叫做批梯度下降(Batch Gradient Descent)。如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是SGD算法。在SGD算法中，每次更新\\(\\mathrm{w}\\)的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对\\(\\mathrm{w}\\)更新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新\\(\\mathrm{w}\\)并不一定按照减少\\(E\\)的方向。然而，虽然存在一定随机性，大量的更新总体上沿着\\(E\\)减少的方向前进的，因此最后也能收敛到最小值附近。下图展示了SGD和BGD的区别 如上图，椭圆表示的是函数值的等高线，椭圆中心是函数的最小值点。红色是BGD的逼近曲线，而紫色是SGD的逼近曲线。我们可以看到BGD是一直向着最低点前进的，而SGD明显躁动了许多，但总体上仍然是向最低点逼近的。 最后需要说明的是，SGD不仅仅效率高，而且随机性有时候反而是好事。今天的目标函数是一个『凸函数』，沿着梯度反方向就能找到全局唯一的最小值。然而对于非凸函数来说，存在许多局部最小值。随机性有助于我们逃离某些很糟糕的局部最小值，从而获得一个更好的模型。 8. 实现线性单元 完整代码请参考GitHub: GitHub-linear_unit (python2.7) 接下来，让我们撸一把代码。 因为我们已经写了感知器的代码，因此我们先比较一下感知器模型和线性单元模型，看看哪些代码能够复用。 算法 感知器 线性单元 模型\\(h(x)\\) \\[y=f(\\mathrm{w}^T\\mathrm{x})\\\\f(z)=\\begin{equation}\\begin{cases}1\\qquad z&gt;0\\\\0\\qquad otherwise\\end{cases}\\end{equation}\\] \\[y=f(\\mathrm{w}^T\\mathrm{x})\\\\f(z)=z\\] 训练规则 \\[\\mathrm{w}\\gets\\mathrm{w}+\\eta(y-\\bar{y})\\mathrm{x}\\] \\[\\mathrm{w}\\gets\\mathrm{w}+\\eta(y-\\bar{y})\\mathrm{x}\\] 比较的结果令人震惊，原来除了激活函数\\(f\\)不同之外，两者的模型和训练规则是一样的(在上表中，线性单元的优化算法是SGD算法)。那么，我们只需要把感知器的激活函数进行替换即可。感知器的代码请参考上一篇文章零基础入门深度学习(1) - 感知器，这里就不再重复了。对于一个养成良好习惯的程序员来说，重复代码是不可忍受的。大家应该把代码保存在一个代码库中(比如git)。 1234567from perceptron import Perceptron#定义激活函数ff = lambda x: xclass LinearUnit(Perceptron): def __init__(self, input_num): '''初始化线性单元，设置输入参数的个数''' Perceptron.__init__(self, input_num, f) 通过继承Perceptron，我们仅用几行代码就实现了线性单元。这再次证明了面向对象编程范式的强大。 接下来，我们用简单的数据进行一下测试。 12345678910111213141516171819202122232425262728293031def get_training_dataset(): ''' 捏造5个人的收入数据 ''' # 构建训练数据 # 输入向量列表，每一项是工作年限 input_vecs = [[5], [3], [8], [1.4], [10.1]] # 期望的输出列表，月薪，注意要与输入一一对应 labels = [5500, 2300, 7600, 1800, 11400] return input_vecs, labels def train_linear_unit(): ''' 使用数据训练线性单元 ''' # 创建感知器，输入参数的特征数为1（工作年限） lu = LinearUnit(1) # 训练，迭代10轮, 学习速率为0.01 input_vecs, labels = get_training_dataset() lu.train(input_vecs, labels, 10, 0.01) #返回训练好的线性单元 return luif __name__ == '__main__': '''训练线性单元''' linear_unit = train_linear_unit() # 打印训练获得的权重 print linear_unit # 测试 print 'Work 3.4 years, monthly salary = %.2f' % linear_unit.predict([3.4]) print 'Work 15 years, monthly salary = %.2f' % linear_unit.predict([15]) print 'Work 1.5 years, monthly salary = %.2f' % linear_unit.predict([1.5]) print 'Work 6.3 years, monthly salary = %.2f' % linear_unit.predict([6.3]) 程序运行结果如下图 拟合的直线如下图 9. 小结 事实上，一个机器学习算法其实只有两部分 模型 从输入特征\\(\\mathrm{x}\\)预测输入\\(y\\)的那个函数\\(h(x)\\) 目标函数 目标函数取最小(最大)值时所对应的参数值，就是模型的参数的最优值。很多时候我们只能获得目标函数的局部最小(最大)值，因此也只能得到模型参数的局部最优值。 因此，如果你想最简洁的介绍一个算法，列出这两个函数就行了。 接下来，你会用优化算法去求取目标函数的最小(最大)值。[随机]梯度{下降|上升}算法就是一个优化算法。针对同一个目标函数，不同的优化算法会推导出不同的训练规则。我们后面还会讲其它的优化算法。 其实在机器学习中，算法往往并不是关键，真正的关键之处在于选取特征。选取特征需要我们人类对问题的深刻理解，经验、以及思考。而神经网络算法的一个优势，就在于它能够自动学习到应该提取什么特征，从而使算法不再那么依赖人类，而这也是神经网络之所以吸引人的一个方面。 现在，经过漫长的烧脑，你已经具备了学习神经网络的必备知识。 10. 后记 线性单元也是一种感知器。 线性单元揭示了用神经网络如何来做回归预测。","categories":[{"name":"科研","slug":"科研","permalink":"http://otter668@github.io/categories/科研/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://otter668@github.io/tags/DeepLearning/"},{"name":"linearunit","slug":"linearunit","permalink":"http://otter668@github.io/tags/linearunit/"}]},{"title":"深度学习系列（1）","slug":"感知器","date":"2017-12-19T12:42:02.624Z","updated":"2017-12-20T07:22:53.251Z","comments":true,"path":"2017/12/19/感知器/","link":"","permalink":"http://otter668@github.io/2017/12/19/感知器/","excerpt":"深度学习 1. 感知器 上图就是一个感知器。有如下部分构成：","text":"深度学习 1. 感知器 上图就是一个感知器。有如下部分构成： 输入权值 一个感知器可以接收多个输入\\((x_1, x_2,...,x_n\\mid x_i\\in\\Re)\\)，每个输入上有一个权值\\(w_i\\in\\Re\\)，此外还有一个偏置项\\(b\\in\\Re\\)，就是上图中的\\(w_0\\)。 激活函数 感知器的激活函数可以有很多选择，比如我们可以选择下面这个阶跃函数\\(f\\)来作为激活函数： \\[f(z)=\\begin{equation}\\begin{cases}1\\qquad z&gt;0\\\\0\\qquad otherwise\\end{cases}\\end{equation}\\] 输出 感知器的输出由下面这个公式来计算: \\[y=f(\\mathrm{w}\\bullet\\mathrm{x}+b)\\qquad 公式(1)\\] 2. 感知器的训练 现在，你可能困惑前面的权重项和偏置项的值是如何获得的呢？这就要用到感知器训练算法：将权重项和偏置项初始化为0，然后，利用下面的感知器规则迭代的修改\\(w_i\\)和\\(b\\)，直到训练完成。 \\[\\begin{align} w_i&amp;\\gets w_i+\\Delta w_i \\\\ b&amp;\\gets b+\\Delta b \\end{align}\\] 其中: \\[\\begin{align} \\Delta w_i&amp;=\\eta(t-y)x_i \\\\ \\Delta b&amp;=\\eta(t-y) \\end{align}\\] \\(w_i\\)是与输入\\(x_i\\)对应的权重项，\\(b\\)是偏置项。事实上，可以把\\(b\\)看作是值永远为1的输入\\(x_b\\)所对应的权重。\\(t\\)是训练样本的实际值，一般称之为label。而\\(y\\)是感知器的输出值，它是根据公式(1)计算得出。\\(\\eta\\)是一个称为学习速率的常数，其作用是控制每一步调整权的幅度。 每次从训练数据中取出一个样本的输入向量\\(x\\)，使用感知器计算其输出\\(y\\)，再根据上面的规则来调整权重。每处理一个样本就调整一次权重。经过多轮迭代后（即全部的训练数据被反复处理多轮），就可以训练出感知器的权重，使之实现目标函数。 3. 编程实战 完整代码请参考GitHub: GitHub-Perceptron (python2.7) 我们来用Python3实现一个感知器，并训练实现一个and函数。 例子：用感知器实现and函数 我们设计一个感知器，让它来实现and运算。程序员都知道，and是一个二元函数（带有两个参数\\(x_1\\)和\\(x_2\\)），下面是它的真值表： \\(x_1\\) \\(x_2\\) \\(y\\) 0 0 0 0 1 0 1 0 0 1 1 1 我们令\\(w_1=0.5;w_2=0.5;b=-0.8\\)，而激活函数\\(f\\)就是前面写出来的阶跃函数，这时，感知器就相当于and函数。 下面是感知器类的实现，非常简单。去掉注释只有27行，而且还包括为了美观（每行不超过60个字符）而增加的很多换行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117#!/usr/bin/env Python# -*- coding: UTF-8 -*-from functools import reduceclass Perceptron(object): def __init__(self, input_num, activator): \"\"\" 初始化感知器，设置输入参数的个数，以及激活函数。 激活函数的类型为double -&gt; double \"\"\" self.activator = activator # 权重向量初始化为0 self.weights = [0.0 for _ in range(input_num)] # 偏置项初始化为0 self.bias = 0.0 def __str__(self): \"\"\" 打印学习到的权重、偏置项 \"\"\" return 'weights\\t:%s\\nbias\\t:%f\\n' % (self.weights, self.bias) def predict(self, input_vec): \"\"\" 输入向量，输出感知器的计算结果 \"\"\" # 把input_vec[x1,x2,x3...]和weights[w1,w2,w3,...]打包在一起 # 变成[(x1,w1),(x2,w2),(x3,w3),...] # 然后利用map函数计算[x1*w1, x2*w2, x3*w3] # 最后利用reduce求和 return self.activator( reduce(lambda a, b: a + b, map(lambda x_w: x_w[0] * x_w[1], zip(input_vec, self.weights)), 0.0) + self.bias) def train(self, input_vecs, labels, iteration, rate): \"\"\" 输入训练数据：一组向量、与每个向量对应的label；以及训练轮数、学习率 \"\"\" for i in range(iteration): self._one_iteration(input_vecs, labels, rate) def _one_iteration(self, input_vecs, labels, rate): \"\"\" 一次迭代，把所有的训练数据过一遍 \"\"\" # 把输入和输出打包在一起，成为样本的列表[(input_vec, label), ...] # 而每个训练样本是(input_vec, label) samples = zip(input_vecs, labels) # 对每个样本，按照感知器规则更新权重 for (input_vec, label) in samples: # 计算感知器在当前权重下的输出 output = self.predict(input_vec) # 更新权重 self._update_weights(input_vec, output, label, rate) def _update_weights(self, input_vec, output, label, rate): \"\"\" 按照感知器规则更新权重 \"\"\" # 把input_vec[x1,x2,x3,...]和weights[w1,w2,w3,...]打包在一起 # 变成[(x1,w1),(x2,w2),(x3,w3),...] # 然后利用感知器规则更新权重 delta = label - output self.weights = list(map( lambda x_w: x_w[1] + rate * delta * x_w[0], zip(input_vec, self.weights))) # 更新bias self.bias += rate * deltadef f(x): \"\"\" 定义激活函数f \"\"\" return 1 if x &gt; 0 else 0def get_training_dataset(): \"\"\" 基于and真值表构建训练数据 \"\"\" # 构建训练数据 # 输入向量列表 input_vecs = [[1, 1], [0, 0], [1, 0], [0, 1]] # 期望的输出列表，注意要与输入一一对应 # [1,1] -&gt; 1, [0,0] -&gt; 0, [1,0] -&gt; 0, [0,1] -&gt; 0 labels = [1, 0, 0, 0] return input_vecs, labelsdef train_and_perceptron(): \"\"\" 使用and真值表训练感知器 \"\"\" # 创建感知器，输入参数个数为2（因为and是二元函数），激活函数为f p = Perceptron(2, f) # 训练，迭代10轮, 学习速率为0.1 input_vecs, labels = get_training_dataset() p.train(input_vecs, labels, 10, 0.1) # 返回训练好的感知器 return pif __name__ == '__main__': # 训练and感知器 and_perceptron = train_and_perceptron() # 打印训练获得的权重 print(and_perceptron) # 测试 print('1 and 1 = %d' % and_perceptron.predict([1, 1])) print('0 and 0 = %d' % and_perceptron.predict([0, 0])) print('1 and 0 = %d' % and_perceptron.predict([1, 0])) print('0 and 1 = %d' % and_perceptron.predict([0, 1])) 4. 后记 感知器算法的流程： 训练模型 获取已知数据 训练，计算\\(y=f(\\mathrm{w}\\bullet\\mathrm{x}+b)\\) 循环多次去根据现有参数预测结果 根据预测结果更新参数 预测数据 一个神经元的感知器模型只能模拟线性函数。","categories":[{"name":"科研","slug":"科研","permalink":"http://otter668@github.io/categories/科研/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://otter668@github.io/tags/DeepLearning/"},{"name":"perceptron","slug":"perceptron","permalink":"http://otter668@github.io/tags/perceptron/"}]},{"title":"简单问答系统","slug":"简单问答系统","date":"2017-12-19T12:33:02.647Z","updated":"2017-12-20T07:33:44.997Z","comments":true,"path":"2017/12/19/简单问答系统/","link":"","permalink":"http://otter668@github.io/2017/12/19/简单问答系统/","excerpt":"简单问答系统 整体框架：","text":"简单问答系统 整体框架： 1. 智能对话交互框架 2. 智能对话交互核心技术 2.1 自然语言理解 语义表示1： 分布语义表示(Distributional semantics) 框架语义表示(Frame semantics) 模型论语义表示(Model-theoretic semantics) 注解： 分布语义表示比较简单，目前可以采用Word2Vec来进行词嵌入。 AliMe采用的是框架语义表示。。。 整体自然语言理解的流程： 2.2 意图分类 意图分类是一种文本分类，主要分为 基于规则的方法、基于传统机器学习的方 法和基于深度学习的方法，如 CNN2、LSTM3、RCNN4、C-LSTM5 及 FastText6 等。 同时也提到单纯使用word vector在分类效果上比不上复杂特征工程的SVM分类器，因此需要嵌入其他信息。如文章中提到把分布式表示和符号表示进行融合。 比如对于“刘德华的忘情水”这句话，通过知识库可以标注刘德华为 singer、忘情水为 song，期望能把 singer 和 song 这样的符号表示融入到网络中去。具体融合方法，既可以把符号标签进行 embedding，然后把 embedding 后的 vector 拼接到 word vector 后进行分类，也可以直接用 multi-hot 的方式拼接到 word vector 后面。 具体方案如图： 在以词为输入单位的 CNN 中，经常会遇到 OOV(Out-Of-Vocabulary) 问题，引入了 FastText7 来训练 word vector，对于 OOV，可以用其 subword 向量计算得到，有效地解决了 OOV 的问题。 2.3 属性抽取 属性抽取问题可以抽象为一个序列标注问题，可以以字为单位进行序列标注，也可以以词为单位进行序列标注。 属性抽取的方法，包括基于规则的方法和基于传统统计模型的方法，经典的如 CRF8，以及基于深度学习模型的方法。 2014年， 在 ARTIS 数据集上，RNN9 模型的效果超过了 CRF。此后，R-CRF10、 LSTM11、Bi-RNN12、 Bi-LSTM-CRF13 等各种模型陆续出来。 文中采用了如下结构： 输入层 采用了分布式表示(word vector)和符号表示 (symbol vector)融合的方式，有效利用了分布式的上下文学习能力和符号的抽象知识表示能力； 采用了局部上下文窗口(local context window)，将窗口内的词的表示拼接在一起送入一个非线性映射层，非线性映射具有特征学习和特征降维的作用; 采用了 FastText14进行 word embedding 的学习，可以有效解决 OOV 的问题。 Bi-LSTM层 输出层 在输出层有几种典型的做法，比如 Bi-LSTM+Softmax、Bi-LSTM+CRF 等。Bi-LSTM+Softmax 是把属性抽取在输出层当成了一个分类问题，得到的标注结果是局部最优，Bi-LSTM+CRF 在输出层会综合句子层面的信息得到全局最优结果。 2.4 意图排序 暂时不做 3 智能问答 三种问答任务： 用户提供 QA-Pairs，一问一答; 建立结构化的知识图谱，进行基于知识图谱的问答; 针对非结构化的文本，进行基于阅读理解的问答。 针对斯坦福大学的数据集 SquAD，有大量优秀的方法不断涌现，比如 match-LSTM15、 BiDAF16、DCN17、FastQA18 等。文献 19 给出了目前的通用框架，如图 11 所示，主要分为 4 层: 4 智能聊天 暂不涉及 ## 5 对话管理 暂不涉及 ## 6 结束语 1. 自然语言理解方面，通过 CNN/ Bi-LSTM-CRF 等深度学习模型、分布式表示和符号表示的融合、多粒度的 word embedding、基于上下文的意图排序等方法， 构建了规则和深度学习模型有机融合的自然语言理解系统。 2. 智能问答方面，成功的将机器阅 读理解应用在了AliMe产品中。 Percy Liang, Natural Language Understanding: Foundations and State-of-the-Art, ICML, 2015.↩ Yoon Kim, Neural Networks for Sentence Classification, EMNLP, 2014↩ Suman Ravuri, and Andreas Stolcke, Recurrent Neural Network and LSTM Models for Lexical Utterance Classification, InterSpecch, 2015.↩ Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao, Recurrent Convolutional Neural Networks for Text Classification, AAAI, 2015.↩ Chunting Zhou, Chonglin Sun, Zhiyuan Liu, and Francis C.M. Lau, A C-LSTM Neural Network for Text Classification, arXiv, 2015.↩ Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov, Bag of Tricks for Efficient Text Classification, EACL, 2017.↩ Piotr Bojanowski, Edouard Grave, Armand Joulin and Tomas Mikolov, Enriching Word Vectors with Subword Information, TACL, 2017.↩ C. Raymond, and G. Riccardi, Generative and discriminative algorithms for spoken language understanding, Interspeech, 2007.↩ Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang, Yangyang Shi, and Dong Yu, Recurrent neural networks for language understanding, InterSpeech, 2013.↩ Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Dong Yu, Xiaolong Li, and Feng Gao, Recurrent conditional random field for language understanding, ICASSP, 2014.↩ Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Geoffrey Zweig, and Yangyang Shi, Spoken language understanding using long short-term memory neural networks, 2014 IEEE Spoken Language Technology Workshop (SLT), 2014.↩ Grégoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xiaodong He, Larry Heck, Gokhan Tur, Dong Yu, and Geoffrey Zweig, Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding, TASLP, 2015.↩ Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer, Neural architectures for named entity recognition, NAACL, 2016.↩ Piotr Bojanowski, Edouard Grave, Armand Joulin and Tomas Mikolov, Enriching Word Vectors with Subword Information, TACL, 2017.↩ Shuohang Wang, and Jing Jiang, Machine comprehension using match-lstm and answer pointer, ICLR, 2017.↩ Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hananneh Hajishirzi, Bidirectional attention flow for machine comprehension, ICLR, 2017.↩ Caiming Xiong, Victor Zhong, and Richard Socher, Dynamic coattention networks for question answering, ICLR, 2017.↩ Dirk Weissenborn, Georg Wiese, and Laura Seiffe, Making Neural QA as Simple as Possible but not Simpler, arXiv, 2017.↩ Dirk Weissenborn, Georg Wiese, and Laura Seiffe, Making Neural QA as Simple as Possible but not Simpler, arXiv, 2017.↩","categories":[{"name":"科研","slug":"科研","permalink":"http://otter668@github.io/categories/科研/"}],"tags":[{"name":"DeepLearning","slug":"DeepLearning","permalink":"http://otter668@github.io/tags/DeepLearning/"},{"name":"QA","slug":"QA","permalink":"http://otter668@github.io/tags/QA/"}]},{"title":"Hello World","slug":"hello-world","date":"2017-12-19T10:57:20.719Z","updated":"2017-12-20T07:33:47.737Z","comments":true,"path":"2017/12/19/hello-world/","link":"","permalink":"http://otter668@github.io/2017/12/19/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[{"name":"其他","slug":"其他","permalink":"http://otter668@github.io/categories/其他/"}],"tags":[{"name":"Hello","slug":"Hello","permalink":"http://otter668@github.io/tags/Hello/"},{"name":"World","slug":"World","permalink":"http://otter668@github.io/tags/World/"}]}]}