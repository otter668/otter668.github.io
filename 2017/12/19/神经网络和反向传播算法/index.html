<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">


  <link rel="manifest" href="/images/manifest.json">


  <meta name="msapplication-config" content="/images/browserconfig.xml" />



  <meta name="keywords" content="Python,NLP,Hexo" />




  


  <link rel="alternate" href="/atom.xml" title="coding@AIIT" type="application/atom+xml" />






<meta name="description" content="神经网络和反向传播算法##0. 往期回顾在上一篇文章中，我们已经掌握了机器学习的基本套路，对模型、目标函数、优化算法这些概念有了一定程度的理解，而且已经会训练单个的感知器或者线性单元了。在这篇文章中，我们将把这些单独的单元按照一定的规则相互连接在一起形成神经网络，从而奇迹般的获得了强大的学习能力。我们还将介绍这种网络的训练算法：反向传播算法。最后，我们依然用代码实现一个神经网络。如果您能坚持到本文">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习系列（3）">
<meta property="og:url" content="http://otter668@github.io/2017/12/19/神经网络和反向传播算法/index.html">
<meta property="og:site_name" content="coding@AIIT">
<meta property="og:description" content="神经网络和反向传播算法##0. 往期回顾在上一篇文章中，我们已经掌握了机器学习的基本套路，对模型、目标函数、优化算法这些概念有了一定程度的理解，而且已经会训练单个的感知器或者线性单元了。在这篇文章中，我们将把这些单独的单元按照一定的规则相互连接在一起形成神经网络，从而奇迹般的获得了强大的学习能力。我们还将介绍这种网络的训练算法：反向传播算法。最后，我们依然用代码实现一个神经网络。如果您能坚持到本文">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15135819090375.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15135846758451.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15135847433480.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15135867205049.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15135881706279.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15135886688854.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15135897556797.jpg">
<meta property="og:image" content="http://p082waf5e.bkt.clouddn.com/15136009452670.jpg">
<meta property="og:updated_time" content="2017-12-19T12:46:06.047Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习系列（3）">
<meta name="twitter:description" content="神经网络和反向传播算法##0. 往期回顾在上一篇文章中，我们已经掌握了机器学习的基本套路，对模型、目标函数、优化算法这些概念有了一定程度的理解，而且已经会训练单个的感知器或者线性单元了。在这篇文章中，我们将把这些单独的单元按照一定的规则相互连接在一起形成神经网络，从而奇迹般的获得了强大的学习能力。我们还将介绍这种网络的训练算法：反向传播算法。最后，我们依然用代码实现一个神经网络。如果您能坚持到本文">
<meta name="twitter:image" content="http://p082waf5e.bkt.clouddn.com/15135819090375.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://otter668@github.io/2017/12/19/神经网络和反向传播算法/"/>





  <title>深度学习系列（3） | coding@AIIT</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">coding@AIIT</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">coding</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://otter668@github.io/2017/12/19/神经网络和反向传播算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Gaochao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="coding@AIIT">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习系列（3）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-19T20:45:23+08:00">
                2017-12-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  11,054
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  47
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="神经网络和反向传播算法"><a href="#神经网络和反向传播算法" class="headerlink" title="神经网络和反向传播算法"></a>神经网络和反向传播算法</h1><p>##0. 往期回顾<br>在上一篇文章中，我们已经掌握了机器学习的基本套路，对模型、目标函数、优化算法这些概念有了一定程度的理解，而且已经会训练单个的感知器或者线性单元了。在这篇文章中，我们将把这些单独的单元按照一定的规则相互连接在一起形成神经网络，从而奇迹般的获得了强大的学习能力。我们还将介绍这种网络的训练算法：反向传播算法。最后，我们依然用代码实现一个神经网络。如果您能坚持到本文的结尾，将会看到我们用自己实现的神经网络去识别手写数字。现在请做好准备，您即将双手触及到深度学习的大门。</p>
<h2 id="1-神经元"><a href="#1-神经元" class="headerlink" title="1. 神经元"></a>1. 神经元</h2><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是阶跃函数；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。如下图所示：</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15135819090375.jpg" alt=""></p>
<p>计算一个神经元的输出的方法和计算一个感知器的输出是一样的。假设神经元的输入是向量$\vec{x}$，权重向量是$\vec{w}$(偏置项是$w_0$)，激活函数是sigmoid函数，则其输出$y$：</p>
<p>$$y=sigmoid(\vec{w}^T\centerdot\vec{x})\qquad(式1)$$</p>
<p>sigmoid函数的定义如下：</p>
<p>$$sigmoid(x)=\frac{1}{1+e^{-x}}$$</p>
<p>sigmoid函数是一个非线性函数，值域是(0,1)。函数图像如下图所示</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15135846758451.jpg" alt=""></p>
<p>sigmoid函数的导数是：<br>$$\begin{align}<br>&amp;令y=sigmoid(x)\<br>&amp;则y’=y(1-y)<br>\end{align}$$</p>
<p>可以看到，sigmoid函数的导数非常有趣，它可以用sigmoid函数自身来表示。这样，一旦计算出sigmoid函数的值，计算它的导数的值就非常方便。</p>
<h2 id="2-神经网络是啥"><a href="#2-神经网络是啥" class="headerlink" title="2. 神经网络是啥"></a>2. 神经网络是啥</h2><p><img src="http://p082waf5e.bkt.clouddn.com/15135847433480.jpg" alt=""></p>
<p>神经网络其实就是按照一定规则连接起来的多个神经元。上图展示了一个全连接(full connected, FC)神经网络，通过观察上面的图，我们可以发现它的规则包括：</p>
<ul>
<li>神经元按照<strong>层</strong>来布局。最左边的层叫做<strong>输入层</strong>，负责接收输入数据；最右边的层叫<strong>输出层</strong>，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做<strong>隐藏层</strong>，因为它们对于外部来说是不可见的。</li>
<li>同一层的神经元之间没有连接。</li>
<li>第N层的每个神经元和第N-1层的所有神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。</li>
<li>每个连接都有一个<strong>权值</strong>。</li>
</ul>
<p>上面这些规则定义了全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。</p>
<h2 id="3-计算神经网络的输出"><a href="#3-计算神经网络的输出" class="headerlink" title="3. 计算神经网络的输出"></a>3. 计算神经网络的输出</h2><p>神经网络实际上就是一个输入向量$\vec{x}$到输出向量$\vec{y}$的函数，即：</p>
<p>$$\vec{y} = f_{network}(\vec{x})$$</p>
<p>根据输入计算神经网络的输出，需要首先将输入向量$\vec{x}$的每个元素$x_i$的值赋给神经网络的输入层的对应神经元，然后根据式1依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量$\vec{y}$。</p>
<p>接下来举一个例子来说明这个过程，我们先给神经网络的每个单元写上编号。</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15135867205049.jpg" alt=""></p>
<p>如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是全连接网络，所以可以看到每个节点都和上一层的所有节点有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为$w<em>{41}$,$w</em>{42}$,$w_{43}$。那么，我们怎样计算节点4的输出值$a_4$呢？</p>
<p>为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是输入层的节点，所以，他们的输出值就是输入向量$\vec{x}$本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是$x_1$,$x_2$,$x_3$。我们要求输入向量的维度和输入层神经元个数相同，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把$x_1$赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值。</p>
<p>一旦我们有了节点1、2、3的输出值，我们就可以根据式1计算节点4的输出值$a_4$：</p>
<p>$$\begin{align}<br>a<em>4&amp;=sigmoid(\vec{w}^T\centerdot\vec{x})\<br>&amp;=sigmoid(w</em>{41}x<em>1+w</em>{42}x<em>2+w</em>{43}x<em>3+w</em>{4b})<br>\end{align}$$</p>
<p>上式的$w<em>{4b}$是节点4的偏置项，图中没有画出来。而$w</em>{41}$,$w<em>{42}$,$w</em>{43}$分别为节点1、2、3到节点4连接的权重，在给权重$w_{ji}$编号时，我们把目标节点的编号$j$放在前面，把源节点的编号$i$放在后面。</p>
<p>同样，我们可以继续计算出节点5、6、7的输出值$a_5$,$a_6$,$a_7$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$y_1$：</p>
<p>$$\begin{align}<br>y<em>1&amp;=sigmoid(\vec{w}^T\centerdot\vec{a})\<br>&amp;=sigmoid(w</em>{84}a<em>4+w</em>{85}a<em>5+w</em>{86}a<em>6+w</em>{87}a<em>7+w</em>{8b})<br>\end{align}$$</p>
<p>同理，我们还可以计算出$y_2$的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$\vec{x}=\begin{bmatrix}x_1\x_2\x_3\end{bmatrix}$时，神经网络的输出向量$\vec{y}=\begin{bmatrix}y_1\y_2\end{bmatrix}$。这里我们也看到，输出向量的维度和输出层神经元个数相同。</p>
<h2 id="4-神经网络的矩阵表示"><a href="#4-神经网络的矩阵表示" class="headerlink" title="4. 神经网络的矩阵表示"></a>4. 神经网络的矩阵表示</h2><p>神经网络的计算如果用矩阵来表示会很方便（当然逼格也更高），我们先来看看隐藏层的矩阵表示。</p>
<p>首先我们把隐藏层4个节点的计算依次排列出来：</p>
<p>$$a<em>4=sigmoid(w</em>{41}x<em>1+w</em>{42}x<em>2+w</em>{43}x<em>3+w</em>{4b})\<br>a<em>5=sigmoid(w</em>{51}x<em>1+w</em>{52}x<em>2+w</em>{53}x<em>3+w</em>{5b})\<br>a<em>6=sigmoid(w</em>{61}x<em>1+w</em>{62}x<em>2+w</em>{63}x<em>3+w</em>{6b})\<br>a<em>7=sigmoid(w</em>{71}x<em>1+w</em>{72}x<em>2+w</em>{73}x<em>3+w</em>{7b})\$$</p>
<p>接着，定义网络的输入向量$\vec{x}$和隐藏层每个节点的权重向量$\vec{w_j}$。令</p>
<p>$$\begin{align}<br>\vec{x}&amp;=\begin{bmatrix}x_1\x_2\x_3\1\end{bmatrix}\<br>\vec{w}<em>4&amp;=[w</em>{41},w<em>{42},w</em>{43},w_{4b}]\<br>\vec{w}<em>5&amp;=[w</em>{51},w<em>{52},w</em>{53},w_{5b}]\<br>\vec{w}<em>6&amp;=[w</em>{61},w<em>{62},w</em>{63},w_{6b}]\<br>\vec{w}<em>7&amp;=[w</em>{71},w<em>{72},w</em>{73},w_{7b}]\<br>f&amp;=sigmoid<br>\end{align}$$</p>
<p>代入到前面的一组式子，得到：</p>
<p>$$\begin{align}<br>a_4&amp;=f(\vec{w_4}\centerdot\vec{x})\<br>a_5&amp;=f(\vec{w_5}\centerdot\vec{x})\<br>a_6&amp;=f(\vec{w_6}\centerdot\vec{x})\<br>a_7&amp;=f(\vec{w_7}\centerdot\vec{x})<br>\end{align}$$</p>
<p>现在，我们把上述计算$a_4$,$a_5$,$a_6$,$a_7$的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示它们的计算了。令</p>
<p>$$\vec{a}=<br>\begin{bmatrix}<br>a_4 \<br>a_5 \<br>a_6 \<br>a_7 \<br>\end{bmatrix},\qquad W=<br>\begin{bmatrix}<br>\vec{w}_4 \<br>\vec{w}_5 \<br>\vec{w}_6 \<br>\vec{w}<em>7 \<br>\end{bmatrix}=<br>\begin{bmatrix}<br>w</em>{41},w<em>{42},w</em>{43},w<em>{4b} \<br>w</em>{51},w<em>{52},w</em>{53},w<em>{5b} \<br>w</em>{61},w<em>{62},w</em>{63},w<em>{6b} \<br>w</em>{71},w<em>{72},w</em>{73},w_{7b} \<br>\end{bmatrix}<br>,\qquad f(<br>\begin{bmatrix}<br>x_1\<br>x_2\<br>x_3\<br>.\<br>.\<br>.\<br>\end{bmatrix})=<br>\begin{bmatrix}<br>f(x_1)\<br>f(x_2)\<br>f(x_3)\<br>.\<br>.\<br>.\<br>\end{bmatrix}$$</p>
<p>带入前面的一组式子，得到<br>$$\vec{a}=f(W\centerdot\vec{x})\qquad (式2)$$</p>
<p>在式2中，是$f$激活函数，在本例中是$sigmoid$函数；$W$是某一层的权重矩阵；$\vec{x}$是某层的输入向量；$\vec{a}$是某层的输出向量。式2说明神经网络的每一层的作用实际上就是先将输入向量左乘一个数组进行线性变换，得到一个新的向量，然后再对这个向量逐元素应用一个激活函数。</p>
<p>每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为$W_1$,$W_2$,$W_3$,$W_4$，每个隐藏层的输出分别是$\vec{a_1}$,$\vec{a_2}$,$\vec{a_3}$，神经网络的输入为$\vec{x}$，神经网络的输出为$\vec{y}$，如下图所示：</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15135881706279.jpg" alt=""></p>
<p>则每一层的输出向量的计算可以表示为：</p>
<p>$$\begin{align}<br>&amp;\vec{a}_1=f(W_1\centerdot\vec{x})\<br>&amp;\vec{a}_2=f(W_2\centerdot\vec{a}_1)\<br>&amp;\vec{a}_3=f(W_3\centerdot\vec{a}_2)\<br>&amp;\vec{y}=f(W_4\centerdot\vec{a}_3)\<br>\end{align}$$</p>
<p>这就是神经网络输出值的计算方法。</p>
<h2 id="5-神经网络的训练"><a href="#5-神经网络的训练" class="headerlink" title="5. 神经网络的训练"></a>5. 神经网络的训练</h2><p>现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个模型，那么这些权值就是模型的参数，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为超参数(Hyper-Parameters)。</p>
<p>接下来，我们将要介绍神经网络的训练算法：反向传播算法。</p>
<h3 id="5-1-反向传播算法-Back-Propagation"><a href="#5-1-反向传播算法-Back-Propagation" class="headerlink" title="5.1 反向传播算法(Back Propagation)"></a>5.1 反向传播算法(Back Propagation)</h3><p>我们首先直观的介绍反向传播算法，最后再来介绍这个算法的推导。当然读者也可以完全跳过推导部分，因为即使不知道如何推导，也不影响你写出来一个神经网络的训练代码。事实上，现在神经网络成熟的开源实现多如牛毛，除了练手之外，你可能都没有机会需要去写一个神经网络。</p>
<p>我们以监督学习为例来解释反向传播算法。在<a href="https://www.zybuluo.com/hanbingtao/note/448086" target="_blank" rel="noopener">线性单元和梯度下降</a>一文中我们介绍了什么是监督学习，如果忘记了可以再看一下。另外，我们设神经元的激活函数$f$为$sigmoid$函数(不同激活函数的计算公式不同，详情见反向传播算法的推导一节)。</p>
<p>我们假设每个训练样本为$(\vec{x},\vec{t})$，其中向量$\vec{x}$是训练样本的特征，而$\vec{t}$是样本的目标值。</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15135886688854.jpg" alt=""></p>
<p>首先，我们根据上一节介绍的算法，用样本的特征$\vec{x}$，计算出神经网络中每个隐藏层节点的输出$a_i$，以及输出层每个节点的输出$y_i$。</p>
<p>然后，我们按照下面的方法计算出每个节点的误差项$\delta_i$：</p>
<ul>
<li>对于输出层节点$i$，</li>
</ul>
<p>$$\delta_i=y_i(1-y_i)(t_i-y_i)\qquad(式3)$$</p>
<p>其中，$\delta_i$是节点$i$的误差项，$y_i$是节点的输出值，$t_i$是样本对应于节点的目标值。举个例子，根据上图，对于输出层节点8来说，它的输出值是$y_1$，而样本的目标值是$t_1$，带入上面的公式得到节点8的误差项$\delta_8$应该是：</p>
<p>$$\delta_8=y_1(1-y_1)(t_1-y_1)$$</p>
<ul>
<li>对于隐藏层节点，</li>
</ul>
<p>$$\delta_i=a_i(1-a<em>i)\sum</em>{k\in{outputs}}w_{ki}\delta_k\qquad(式4)$$</p>
<p>其中，$a<em>i$是节点$i$的输出值，$w</em>{ki}$是节点$i$到它的下一层节点$k$的连接的权重，$\delta_k$是节点$i$的下一层节点$k$的误差项。例如，对于隐藏层节点4来说，计算方法如下：</p>
<p>$$\delta_4=a_4(1-a<em>4)(w</em>{84}\delta<em>8+w</em>{94}\delta_9)$$</p>
<p>最后，更新每个连接上的权值：</p>
<p>$$w<em>{ji}\gets w</em>{ji}+\eta\delta<em>jx</em>{ji}\qquad(式5)$$</p>
<p>其中，$w_{ji}$是节点$i$到节点$j$的权重，$\eta$是一个成为学习速率的常数，$\delta<em>j$是节点$j$的误差项，$x</em>{ij}$是节点$i$传递给节点$j$的输入。例如，权重$w_{84}$的更新方法如下：</p>
<p>$$w<em>{84}\gets w</em>{84}+\eta\delta_8 a_4$$</p>
<p>类似的，权重$w_{41}$的更新方法如下：</p>
<p>$$w<em>{41}\gets w</em>{41}+\eta\delta_4 x_1$$</p>
<p>偏置项的输入值永远为1。例如，节点4的偏置项$w_{4b}$应该按照下面的方法计算：</p>
<p>$$w<em>{4b}\gets w</em>{4b}+\eta\delta_4$$</p>
<p>我们已经介绍了神经网络每个节点误差项的计算和权重更新方法。显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。当所有节点的误差项计算完毕后，我们就可以根据式5来更新所有的权重。</p>
<p>以上就是基本的反向传播算法，并不是很复杂，您弄清楚了么？</p>
<h3 id="5-2-反向传播算法的推导"><a href="#5-2-反向传播算法的推导" class="headerlink" title="5.2 反向传播算法的推导"></a>5.2 反向传播算法的推导</h3><p>反向传播算法其实就是链式求导法则的应用。然而，这个如此简单且显而易见的方法，却是在Roseblatt提出感知器算法将近30年之后才被发明和普及的。对此，Bengio这样回应道：</p>
<blockquote>
<p>很多看似显而易见的想法只有在事后才变得显而易见。</p>
</blockquote>
<p>接下来，我们用链式求导法则来推导反向传播算法，也就是上一小节的式3、式4、式5。</p>
<p><em>前方高能预警——接下来是数学公式重灾区，读者可以酌情阅读，不必强求。</em></p>
<p>按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用随机梯度下降优化算法去求目标函数最小值时的参数值。</p>
<p>我们取网络所有输出层节点的误差平方和作为目标函数：</p>
<p>$$E<em>d\equiv\frac{1}{2}\sum</em>{i\in outputs}(t_i-y_i)^2$$</p>
<p>其中，$E_d$表示是样本$d$的误差。</p>
<p>然后，我们用文章<a href="https://www.zybuluo.com/hanbingtao/note/448086" target="_blank" rel="noopener">线性单元和梯度下降</a>中介绍的随机梯度下降算法对目标函数进行优化：</p>
<p>$$w<em>{ji}\gets w</em>{ji}-\eta\frac{\partial{E<em>d}}{\partial{w</em>{ji}}}$$</p>
<p>随机梯度下降算法也就是需要求出误差$E<em>d$对于每个权重$w</em>{ji}$的偏导数（也就是梯度），怎么求呢？</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15135897556797.jpg" alt=""></p>
<p>观察上图，我们发现权重$w_{ji}$仅能通过影响节点$j$的输入值影响网络的其它部分，设$net_j$是节点$j$的加权输入，即</p>
<p>$$\begin{align}<br>net_j&amp;=\vec{w_j}\centerdot\vec{x<em>j}\<br>&amp;=\sum</em>{i}{w<em>{ji}}x</em>{ji}<br>\end{align}$$</p>
<p>$E_d$是$net_j$的函数，而$net<em>j$是$w</em>{ji}$的函数。根据链式求导法则，可以得到：</p>
<p>$$\begin{align}<br>\frac{\partial{E<em>d}}{\partial{w</em>{ji}}}&amp;=\frac{\partial{E_d}}{\partial{net_j}}\frac{\partial{net<em>j}}{\partial{w</em>{ji}}}\<br>&amp;=\frac{\partial{E_d}}{\partial{net<em>j}}\frac{\partial{\sum</em>{i}{w<em>{ji}}x</em>{ji}}}{\partial{w_{ji}}}\<br>&amp;=\frac{\partial{E_d}}{\partial{net<em>j}}x</em>{ji}<br>\end{align}$$</p>
<p>上式中，$x_{ji}$是节点$i$传递给节点$j$的输入值，也就是节点$i$的输出值。<br>对于$\frac{\partial{E_d}}{\partial{net_j}}$的推导，需要区分输出层和隐藏层两种情况。</p>
<h4 id="5-2-1-输出层权值训练"><a href="#5-2-1-输出层权值训练" class="headerlink" title="5.2.1 输出层权值训练"></a>5.2.1 输出层权值训练</h4><p>对于输出层来说，$net_j$仅能通过节点$j$的输出值$y_j$来影响网络其它部分，也就是说$E_d$是$y_j$的函数，而$y_j$是$net_j$的函数，其中$y_j=sigmoid(net_j)$。所以我们可以再次使用链式求导法则：</p>
<p>$$\begin{align}<br>\frac{\partial{E_d}}{\partial{net_j}}&amp;=\frac{\partial{E_d}}{\partial{y_j}}\frac{\partial{y_j}}{\partial{net_j}}\<br>\end{align}$$</p>
<p>考虑上式第一项:</p>
<p>$$\begin{align}<br>\frac{\partial{E_d}}{\partial{y_j}}&amp;=\frac{\partial}{\partial{y<em>j}}\frac{1}{2}\sum</em>{i\in outputs}(t_i-y_i)^2\<br>&amp;=\frac{\partial}{\partial{y_j}}\frac{1}{2}(t_j-y_j)^2\<br>&amp;=-(t_j-y_j)<br>\end{align}$$</p>
<p>考虑上式第二项：</p>
<p>$$\begin{align}<br>\frac{\partial{y_j}}{\partial{net_j}}&amp;=\frac{\partial sigmoid(net_j)}{\partial{net_j}}\<br>&amp;=y_j(1-y_j)\<br>\end{align}$$</p>
<p>将第一项和第二项带入，得到：</p>
<p>$$\frac{\partial{E_d}}{\partial{net_j}}=-(t_j-y_j)y_j(1-y_j)$$</p>
<p>如果令$\delta_j=-\frac{\partial{E_d}}{\partial{net_j}}$，也就是一个节点的误差项$\delta$是网络误差对这个节点输入的偏导数的相反数。带入上式，得到：</p>
<p>$$\delta_j=(t_j-y_j)y_j(1-y_j)$$</p>
<p>上式就是式3。</p>
<p>将上述推导带入随机梯度下降公式，得到：</p>
<p>$$\begin{align}<br>w<em>{ji}&amp;\gets w</em>{ji}-\eta\frac{\partial{E<em>d}}{\partial{w</em>{ji}}}\<br>&amp;=w_{ji}+\eta(t_j-y_j)y_j(1-y<em>j)x</em>{ji}\<br>&amp;=w_{ji}+\eta\delta<em>jx</em>{ji}<br>\end{align}$$</p>
<p>上式就是式5。</p>
<h4 id="5-2-2-隐藏层权值训练"><a href="#5-2-2-隐藏层权值训练" class="headerlink" title="5.2.2 隐藏层权值训练"></a>5.2.2 隐藏层权值训练</h4><p>现在我们要推导出隐藏层的$\frac{\partial{E_d}}{\partial{net_j}}$。</p>
<p>首先，我们需要定义节点的所有直接下游节点的集合$Downstream(j)$。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到$net_j$只能通过影响$Downstream(j)$再影响$E_d$。设$net_k$是节点$j$的下游节点的输入，则$E_d$是$net_k$的函数，而$net_k$是$net_j$的函数。因为$net_k$有多个，我们应用全导数公式，可以做出如下推导：</p>
<p>$$\begin{align}<br>\frac{\partial{E_d}}{\partial{net<em>j}}&amp;=\sum</em>{k\in Downstream(j)}\frac{\partial{E_d}}{\partial{net_k}}\frac{\partial{net_k}}{\partial{net<em>j}}\<br>&amp;=\sum</em>{k\in Downstream(j)}-\delta_k\frac{\partial{net_k}}{\partial{net<em>j}}\<br>&amp;=\sum</em>{k\in Downstream(j)}-\delta_k\frac{\partial{net_k}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{net<em>j}}\<br>&amp;=\sum</em>{k\in Downstream(j)}-\delta<em>kw</em>{kj}\frac{\partial{a_j}}{\partial{net<em>j}}\<br>&amp;=\sum</em>{k\in Downstream(j)}-\delta<em>kw</em>{kj}a_j(1-a_j)\<br>&amp;=-a_j(1-a<em>j)\sum</em>{k\in Downstream(j)}\delta<em>kw</em>{kj}<br>\end{align}$$</p>
<p>因为$\delta_j=-\frac{\partial{E_d}}{\partial{net_j}}$，带入上式得到：</p>
<p>$$\delta_j=a_j(1-a<em>j)\sum</em>{k\in Downstream(j)}\delta<em>kw</em>{kj}$$</p>
<p>上式就是式4。</p>
<p><em>数学公式警报解除</em></p>
<p>至此，我们已经推导出了反向传播算法。需要注意的是，我们刚刚推导出的训练规则是根据激活函数是$sigmoid$函数、平方和误差、全连接网络、随机梯度下降优化算法。如果激活函数不同、误差计算方式不同、网络连接结构不同、优化算法不同，则具体的训练规则也会不一样。但是无论怎样，训练规则的推导方式都是一样的，应用链式求导法则进行推导即可。</p>
<h2 id="6-神经网络的实现"><a href="#6-神经网络的实现" class="headerlink" title="6 神经网络的实现"></a>6 神经网络的实现</h2><blockquote>
<p>完整代码请参考GitHub: <a href="https://github.com/hanbt/learn_dl/blob/master/bp.py" target="_blank" rel="noopener">GitHub-BP</a> (python2.7)</p>
</blockquote>
<p>现在，我们要根据前面的算法，实现一个基本的全连接神经网络，这并不需要太多代码。我们在这里依然采用面向对象设计。</p>
<p>首先，我们先做一个基本的模型：</p>
<p><img src="http://p082waf5e.bkt.clouddn.com/15136009452670.jpg" alt=""></p>
<p>如上图，可以分解出5个领域对象来实现神经网络：</p>
<ul>
<li><em>Network</em> 神经网络对象，提供API接口。它由若干层对象组成以及连接对象组成。</li>
<li><em>Layer</em> 层对象，由多个节点组成。</li>
<li><em>Node</em> 节点对象计算和记录节点自身的信息(比如输出值$a$、误差项$\delta$等)，以及与这个节点相关的上下游的连接。</li>
<li><em>Connection</em> 每个连接对象都要记录该连接的权重。</li>
<li><em>Connections</em> 仅仅作为Connection的集合对象，提供一些集合操作。</li>
</ul>
<p>Node实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 节点类，负责记录和维护节点自身信息以及与这个节点相关的上下游连接，实现输出值和误差项的计算。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer_index, node_index)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        构造节点对象。</span></span><br><span class="line"><span class="string">        layer_index: 节点所属的层的编号</span></span><br><span class="line"><span class="string">        node_index: 节点的编号</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.layer_index = layer_index</span><br><span class="line">        self.node_index = node_index</span><br><span class="line">        self.downstream = []</span><br><span class="line">        self.upstream = []</span><br><span class="line">        self.output = <span class="number">0</span></span><br><span class="line">        self.delta = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_output</span><span class="params">(self, output)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        设置节点的输出值。如果节点属于输入层会用到这个函数。</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.output = output</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append_downstream_connection</span><span class="params">(self, conn)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        添加一个到下游节点的连接</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.downstream.append(conn)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append_upstream_connection</span><span class="params">(self, conn)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        添加一个到上游节点的连接</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.upstream.append(conn)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_output</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        根据式1计算节点的输出</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        output = reduce(<span class="keyword">lambda</span> ret, conn: ret + conn.upstream_node.output * conn.weight, self.upstream, <span class="number">0</span>)</span><br><span class="line">        self.output = sigmoid(output)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_hidden_layer_delta</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        节点属于隐藏层时，根据式4计算delta</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        downstream_delta = reduce(</span><br><span class="line">            <span class="keyword">lambda</span> ret, conn: ret + conn.downstream_node.delta * conn.weight,</span><br><span class="line">            self.downstream, <span class="number">0.0</span>)</span><br><span class="line">        self.delta = self.output * (<span class="number">1</span> - self.output) * downstream_delta</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_output_layer_delta</span><span class="params">(self, label)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        节点属于输出层时，根据式3计算delta</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.delta = self.output * (<span class="number">1</span> - self.output) * (label - self.output)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        打印节点的信息</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        node_str = <span class="string">'%u-%u: output: %f delta: %f'</span> % (self.layer_index, self.node_index, self.output, self.delta)</span><br><span class="line">        downstream_str = reduce(<span class="keyword">lambda</span> ret, conn: ret + <span class="string">'\n\t'</span> + str(conn), self.downstream, <span class="string">''</span>)</span><br><span class="line">        upstream_str = reduce(<span class="keyword">lambda</span> ret, conn: ret + <span class="string">'\n\t'</span> + str(conn), self.upstream, <span class="string">''</span>)</span><br><span class="line">        <span class="keyword">return</span> node_str + <span class="string">'\n\tdownstream:'</span> + downstream_str + <span class="string">'\n\tupstream:'</span> + upstream_str</span><br></pre></td></tr></table></figure>
<p>ConstNode对象，为了实现一个输出恒为1的节点(计算偏置项$w_b$时需要)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConstNode</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer_index, node_index)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        构造节点对象。</span></span><br><span class="line"><span class="string">        layer_index: 节点所属的层的编号</span></span><br><span class="line"><span class="string">        node_index: 节点的编号</span></span><br><span class="line"><span class="string">        '''</span>    </span><br><span class="line">        self.layer_index = layer_index</span><br><span class="line">        self.node_index = node_index</span><br><span class="line">        self.downstream = []</span><br><span class="line">        self.output = <span class="number">1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">append_downstream_connection</span><span class="params">(self, conn)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        添加一个到下游节点的连接</span></span><br><span class="line"><span class="string">        '''</span>       </span><br><span class="line">        self.downstream.append(conn)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_hidden_layer_delta</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        节点属于隐藏层时，根据式4计算delta</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        downstream_delta = reduce(</span><br><span class="line">            <span class="keyword">lambda</span> ret, conn: ret + conn.downstream_node.delta * conn.weight,</span><br><span class="line">            self.downstream, <span class="number">0.0</span>)</span><br><span class="line">        self.delta = self.output * (<span class="number">1</span> - self.output) * downstream_delta</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        打印节点的信息</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        node_str = <span class="string">'%u-%u: output: 1'</span> % (self.layer_index, self.node_index)</span><br><span class="line">        downstream_str = reduce(<span class="keyword">lambda</span> ret, conn: ret + <span class="string">'\n\t'</span> + str(conn), self.downstream, <span class="string">''</span>)</span><br><span class="line">        <span class="keyword">return</span> node_str + <span class="string">'\n\tdownstream:'</span> + downstream_str</span><br></pre></td></tr></table></figure>
<p>Layer对象，负责初始化一层。此外，作为Node的集合对象，提供对Node集合的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Layer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer_index, node_count)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        初始化一层</span></span><br><span class="line"><span class="string">        layer_index: 层编号</span></span><br><span class="line"><span class="string">        node_count: 层所包含的节点个数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.layer_index = layer_index</span><br><span class="line">        self.nodes = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(node_count):</span><br><span class="line">            self.nodes.append(Node(layer_index, i))</span><br><span class="line">        self.nodes.append(ConstNode(layer_index, node_count))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_output</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        设置层的输出。当层是输入层时会用到。</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">            self.nodes[i].set_output(data[i])</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_output</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算层的输出向量</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> self.nodes[:<span class="number">-1</span>]:</span><br><span class="line">            node.calc_output()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dump</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        打印层的信息</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> self.nodes:</span><br><span class="line">            <span class="keyword">print</span> node</span><br></pre></td></tr></table></figure>
<p>Connection对象，主要职责是记录连接的权重，以及这个连接所关联的上下游节点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Connection</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, upstream_node, downstream_node)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        初始化连接，权重初始化为是一个很小的随机数</span></span><br><span class="line"><span class="string">        upstream_node: 连接的上游节点</span></span><br><span class="line"><span class="string">        downstream_node: 连接的下游节点</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.upstream_node = upstream_node</span><br><span class="line">        self.downstream_node = downstream_node</span><br><span class="line">        self.weight = random.uniform(<span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.gradient = <span class="number">0.0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_gradient</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算梯度</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.gradient = self.downstream_node.delta * self.upstream_node.output</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_gradient</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        获取当前的梯度</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.gradient</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weight</span><span class="params">(self, rate)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        根据梯度下降算法更新权重</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.calc_gradient()</span><br><span class="line">        self.weight += rate * self.gradient</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        打印连接信息</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'(%u-%u) -&gt; (%u-%u) = %f'</span> % (</span><br><span class="line">            self.upstream_node.layer_index, </span><br><span class="line">            self.upstream_node.node_index,</span><br><span class="line">            self.downstream_node.layer_index, </span><br><span class="line">            self.downstream_node.node_index, </span><br><span class="line">            self.weight)</span><br></pre></td></tr></table></figure>
<p>Connections对象，提供Connection集合操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Connections</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.connections = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_connection</span><span class="params">(self, connection)</span>:</span></span><br><span class="line">        self.connections.append(connection)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dump</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> conn <span class="keyword">in</span> self.connections:</span><br><span class="line">            <span class="keyword">print</span> conn</span><br></pre></td></tr></table></figure>
<p>Network对象，提供API。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        初始化一个全连接神经网络</span></span><br><span class="line"><span class="string">        layers: 二维数组，描述神经网络每层节点数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.connections = Connections()</span><br><span class="line">        self.layers = []</span><br><span class="line">        layer_count = len(layers)</span><br><span class="line">        node_count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(layer_count):</span><br><span class="line">            self.layers.append(Layer(i, layers[i]))</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> range(layer_count - <span class="number">1</span>):</span><br><span class="line">            connections = [Connection(upstream_node, downstream_node) </span><br><span class="line">                           <span class="keyword">for</span> upstream_node <span class="keyword">in</span> self.layers[layer].nodes</span><br><span class="line">                           <span class="keyword">for</span> downstream_node <span class="keyword">in</span> self.layers[layer + <span class="number">1</span>].nodes[:<span class="number">-1</span>]]</span><br><span class="line">            <span class="keyword">for</span> conn <span class="keyword">in</span> connections:</span><br><span class="line">                self.connections.add_connection(conn)</span><br><span class="line">                conn.downstream_node.append_upstream_connection(conn)</span><br><span class="line">                conn.upstream_node.append_downstream_connection(conn)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, labels, data_set, rate, iteration)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        训练神经网络</span></span><br><span class="line"><span class="string">        labels: 数组，训练样本标签。每个元素是一个样本的标签。</span></span><br><span class="line"><span class="string">        data_set: 二维数组，训练样本特征。每个元素是一个样本的特征。</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(iteration):</span><br><span class="line">            <span class="keyword">for</span> d <span class="keyword">in</span> range(len(data_set)):</span><br><span class="line">                self.train_one_sample(labels[d], data_set[d], rate)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_one_sample</span><span class="params">(self, label, sample, rate)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        内部函数，用一个样本训练网络</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.predict(sample)</span><br><span class="line">        self.calc_delta(label)</span><br><span class="line">        self.update_weight(rate)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_delta</span><span class="params">(self, label)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        内部函数，计算每个节点的delta</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        output_nodes = self.layers[<span class="number">-1</span>].nodes</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(label)):</span><br><span class="line">            output_nodes[i].calc_output_layer_delta(label[i])</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers[<span class="number">-2</span>::<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> layer.nodes:</span><br><span class="line">                node.calc_hidden_layer_delta()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weight</span><span class="params">(self, rate)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        内部函数，更新每个连接权重</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers[:<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> layer.nodes:</span><br><span class="line">                <span class="keyword">for</span> conn <span class="keyword">in</span> node.downstream:</span><br><span class="line">                    conn.update_weight(rate)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_gradient</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        内部函数，计算每个连接的梯度</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers[:<span class="number">-1</span>]:</span><br><span class="line">            <span class="keyword">for</span> node <span class="keyword">in</span> layer.nodes:</span><br><span class="line">                <span class="keyword">for</span> conn <span class="keyword">in</span> node.downstream:</span><br><span class="line">                    conn.calc_gradient()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_gradient</span><span class="params">(self, label, sample)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        获得网络在一个样本下，每个连接上的梯度</span></span><br><span class="line"><span class="string">        label: 样本标签</span></span><br><span class="line"><span class="string">        sample: 样本输入</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.predict(sample)</span><br><span class="line">        self.calc_delta(label)</span><br><span class="line">        self.calc_gradient()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        根据输入的样本预测输出值</span></span><br><span class="line"><span class="string">        sample: 数组，样本的特征，也就是网络的输入向量</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.layers[<span class="number">0</span>].set_output(sample)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(self.layers)):</span><br><span class="line">            self.layers[i].calc_output()</span><br><span class="line">        <span class="keyword">return</span> map(<span class="keyword">lambda</span> node: node.output, self.layers[<span class="number">-1</span>].nodes[:<span class="number">-1</span>])</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dump</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        打印网络信息</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            layer.dump()</span><br></pre></td></tr></table></figure>
<p>至此，实现了一个基本的全连接神经网络。可以看到，同神经网络的强大学习能力相比，其实现还算是很容易的。</p>
<h2 id="7-梯度检查"><a href="#7-梯度检查" class="headerlink" title="7 梯度检查"></a>7 梯度检查</h2><p>怎么保证自己写的神经网络没有BUG呢？事实上这是一个非常重要的问题。一方面，千辛万苦想到一个算法，结果效果不理想，那么是算法本身错了还是代码实现错了呢？定位这种问题肯定要花费大量的时间和精力。另一方面，由于神经网络的复杂性，我们几乎无法事先知道神经网络的输入和输出，因此类似TDD(测试驱动开发)这样的开发方法似乎也不可行。</p>
<p>办法还是有滴，就是利用梯度检查来确认程序是否正确。梯度检查的思路如下：</p>
<p>对于梯度下降算法：</p>
<p>$$w<em>{ji}\gets w</em>{ji}-\eta\frac{\partial{E<em>d}}{\partial{w</em>{ji}}}$$</p>
<p>来说，这里关键之处在于$\frac{\partial{E<em>d}}{\partial{w</em>{ji}}}$的计算一定要正确，而它是$E<em>d$对$w</em>{ji}$的偏导数。而根据导数的定义：</p>
<p>$$f’(\theta)=\lim_{\epsilon-&gt;0}\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}$$</p>
<p>对于任意的$\theta$导数值，我们都可以用等式右边来近似计算。我们把$E<em>d$看做是$w</em>{ji}$的函数，即$E<em>d(w</em>{ji})$，那么根据导数定义，$\frac{\partial{E<em>d(w</em>{ji})}}{\partial{w_{ji}}}$应该等于：</p>
<p>$$\frac{\partial{E<em>d(w</em>{ji})}}{\partial{w<em>{ji}}}=\lim</em>{\epsilon-&gt;0}\frac{f(w<em>{ji}+\epsilon)-f(w</em>{ji}-\epsilon)}{2\epsilon}$$</p>
<p>如果把$\epsilon$设置为一个很小的数（比如$10^{-4}$），那么上式可以写成：</p>
<p>$$\frac{\partial{E<em>d(w</em>{ji})}}{\partial{w<em>{ji}}}\approx\frac{f(w</em>{ji}+\epsilon)-f(w_{ji}-\epsilon)}{2\epsilon}\qquad(式6)$$</p>
<p>我们就可以利用式6，来计算梯度$\frac{\partial{E<em>d}}{\partial{w</em>{ji}}}$的值，然后同我们神经网络代码中计算出来的梯度值进行比较。如果两者的差别非常的小，那么就说明我们的代码是正确的。</p>
<p>下面是梯度检查的代码。如果我们想检查参数$w_{ji}$的梯度是否正确，我们需要以下几个步骤：</p>
<ol>
<li>首先使用一个样本$d$对神经网络进行训练，这样就能获得每个权重的梯度。</li>
<li>将$w<em>{ji}$加上一个很小的值($10^{-4}$)，重新计算神经网络在这个样本$d$下的$E</em>{d+}$。</li>
<li>将$w<em>{ji}$减上一个很小的值($10^{-4}$)，重新计算神经网络在这个样本$d$下的$E</em>{d-}$。</li>
<li>根据式6计算出期望的梯度值，和第一步获得的梯度值进行比较，它们应该几乎想等(至少4位有效数字相同)。</li>
</ol>
<p>当然，我们可以重复上面的过程，对每个权重$w_{ji}$都进行检查。也可以使用多个样本重复检查。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">(network, sample_feature, sample_label)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    梯度检查</span></span><br><span class="line"><span class="string">    network: 神经网络对象</span></span><br><span class="line"><span class="string">    sample_feature: 样本的特征</span></span><br><span class="line"><span class="string">    sample_label: 样本的标签</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 计算网络误差</span></span><br><span class="line">    network_error = <span class="keyword">lambda</span> vec1, vec2: \</span><br><span class="line">            <span class="number">0.5</span> * reduce(<span class="keyword">lambda</span> a, b: a + b, </span><br><span class="line">                      map(<span class="keyword">lambda</span> v: (v[<span class="number">0</span>] - v[<span class="number">1</span>]) * (v[<span class="number">0</span>] - v[<span class="number">1</span>]),</span><br><span class="line">                          zip(vec1, vec2)))</span><br><span class="line">    <span class="comment"># 获取网络在当前样本下每个连接的梯度</span></span><br><span class="line">    network.get_gradient(sample_feature, sample_label)</span><br><span class="line">    <span class="comment"># 对每个权重做梯度检查    </span></span><br><span class="line">    <span class="keyword">for</span> conn <span class="keyword">in</span> network.connections.connections: </span><br><span class="line">        <span class="comment"># 获取指定连接的梯度</span></span><br><span class="line">        actual_gradient = conn.get_gradient()</span><br><span class="line">        <span class="comment"># 增加一个很小的值，计算网络的误差</span></span><br><span class="line">        epsilon = <span class="number">0.0001</span></span><br><span class="line">        conn.weight += epsilon</span><br><span class="line">        error1 = network_error(network.predict(sample_feature), sample_label)</span><br><span class="line">        <span class="comment"># 减去一个很小的值，计算网络的误差</span></span><br><span class="line">        conn.weight -= <span class="number">2</span> * epsilon <span class="comment"># 刚才加过了一次，因此这里需要减去2倍</span></span><br><span class="line">        error2 = network_error(network.predict(sample_feature), sample_label)</span><br><span class="line">        <span class="comment"># 根据式6计算期望的梯度值</span></span><br><span class="line">        expected_gradient = (error2 - error1) / (<span class="number">2</span> * epsilon)</span><br><span class="line">        <span class="comment"># 打印</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'expected gradient: \t%f\nactual gradient: \t%f'</span> % (</span><br><span class="line">            expected_gradient, actual_gradient)</span><br></pre></td></tr></table></figure>
<p>至此，会推导、会实现、会抓BUG，你已经摸到深度学习的大门了。接下来还需要不断的实践，我们用刚刚写过的神经网络去识别手写数字。</p>
<h2 id="8-神经网络实战——手写数字识别"><a href="#8-神经网络实战——手写数字识别" class="headerlink" title="8 神经网络实战——手写数字识别"></a>8 神经网络实战——手写数字识别</h2><p>针对这个任务，我们采用业界非常流行的MNIST数据集。MNIST大约有60000个手写字母的训练样本，我们使用它训练我们的神经网络，然后再用训练好的网络去识别手写数字。</p>
<p>手写数字识别是个比较简单的任务，数字只可能是0-9中的一个，这是个10分类问题。</p>
<h3 id="8-1-超参数的确定"><a href="#8-1-超参数的确定" class="headerlink" title="8.1 超参数的确定"></a>8.1 超参数的确定</h3><p>我们首先需要确定网络的层数和每层的节点数。关于第一个问题，实际上并没有什么理论化的方法，大家都是根据经验来拍，如果没有经验的话就随便拍一个。然后，你可以多试几个值，训练不同层数的神经网络，看看哪个效果最好就用哪个。嗯，现在你可能明白为什么说深度学习是个手艺活了，有些手艺很让人无语，而有些手艺还是很有技术含量的。</p>
<p>不过，有些基本道理我们还是明白的，我们知道网络层数越多越好，也知道层数越多训练难度越大。对于全连接网络，隐藏层最好不要超过三层。那么，我们可以先试试仅有一个隐藏层的神经网络效果怎么样。毕竟模型小的话，训练起来也快些(刚开始玩模型的时候，都希望快点看到结果)。</p>
<p>输入层节点数是确定的。因为MNIST数据集每个训练数据是28*28的图片，共784个像素，因此，输入层节点数应该是784，每个像素对应一个输入节点。</p>
<p>输出层节点数也是确定的。因为是10分类，我们可以用10个节点，每个节点对应一个分类。输出层10个节点中，输出最大值的那个节点对应的分类，就是模型的预测结果。</p>
<p>隐藏层节点数量是不好确定的，从1到100万都可以。下面有几个经验公式：</p>
<p>$$\begin{align}<br>&amp;m=\sqrt{n+l}+\alpha\<br>&amp;m=log_2n\<br>&amp;m=\sqrt{nl}\<br>&amp;m:隐藏层节点数\<br>&amp;n:输入层节点数\<br>&amp;l:输出层节点数\<br>&amp;\alpha:1到10之间的常数<br>\end{align}$$</p>
<p>因此，我们可以先根据上面的公式设置一个隐藏层节点数。如果有时间，我们可以设置不同的节点数，分别训练，看看哪个效果最好就用哪个。我们先拍一个，设隐藏层节点数为300吧。</p>
<p>对于3层$784<em>300</em>10$的全连接网络，总共有$300<em>(784+1)+10</em>(300+1)=238510$个参数！神经网络之所以强大，是它提供了一种非常简单的方法去实现大量的参数。目前百亿参数、千亿样本的超大规模神经网络也是有的。因为MNIST只有6万个训练样本，参数太多了很容易过拟合，效果反而不好。</p>
<h3 id="8-2-模型的训练和评估"><a href="#8-2-模型的训练和评估" class="headerlink" title="8.2 模型的训练和评估"></a>8.2 模型的训练和评估</h3><p>MNIST数据集包含10000个测试样本。我们先用60000个训练样本训练我们的网络，然后再用测试样本对网络进行测试，计算识别错误率：</p>
<p>$$错误率=\frac{错误预测样本数}{总样本数}$$</p>
<p>我们每训练10轮，评估一次准确率。当准确率开始下降时（出现了过拟合）终止训练。</p>
<h3 id="8-3-代码实现"><a href="#8-3-代码实现" class="headerlink" title="8.3 代码实现"></a>8.3 代码实现</h3><p>首先，我们需要把MNIST数据集处理为神经网络能够接受的形式。MNIST训练集的文件格式可以参考官方网站，这里不在赘述。每个训练样本是一个28*28的图像，我们按照行优先，把它转化为一个784维的向量。每个标签是0-9的值，我们将其转换为一个10维的one-hot向量：如果标签值为$n$，我们就把向量的第$n$维（从0开始编号）设置为0.9，而其它维设置为0.1。例如，向量[0.1,0.1,0.9,0.1,0.1,0.1,0.1,0.1,0.1,0.1]表示值2。</p>
<p>下面是处理MNIST数据的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env Python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> struct</span><br><span class="line"><span class="keyword">from</span> bp <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="comment"># 数据加载器基类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Loader</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, path, count)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        初始化加载器</span></span><br><span class="line"><span class="string">        path: 数据文件路径</span></span><br><span class="line"><span class="string">        count: 文件中的样本个数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.path = path</span><br><span class="line">        self.count = count</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_file_content</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        读取文件内容</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        f = open(self.path, <span class="string">'rb'</span>)</span><br><span class="line">        content = f.read()</span><br><span class="line">        f.close()</span><br><span class="line">        <span class="keyword">return</span> content</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_int</span><span class="params">(self, byte)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        将unsigned byte字符转换为整数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> struct.unpack(<span class="string">'B'</span>, byte)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 图像数据加载器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageLoader</span><span class="params">(Loader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_picture</span><span class="params">(self, content, index)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        内部函数，从文件中获取图像</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        start = index * <span class="number">28</span> * <span class="number">28</span> + <span class="number">16</span></span><br><span class="line">        picture = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">28</span>):</span><br><span class="line">            picture.append([])</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">28</span>):</span><br><span class="line">                picture[i].append(</span><br><span class="line">                    self.to_int(content[start + i * <span class="number">28</span> + j]))</span><br><span class="line">        <span class="keyword">return</span> picture</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_one_sample</span><span class="params">(self, picture)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        内部函数，将图像转化为样本的输入向量</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        sample = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">28</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">28</span>):</span><br><span class="line">                sample.append(picture[i][j])</span><br><span class="line">        <span class="keyword">return</span> sample</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        加载数据文件，获得全部样本的输入向量</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        content = self.get_file_content()</span><br><span class="line">        data_set = []</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(self.count):</span><br><span class="line">            data_set.append(</span><br><span class="line">                self.get_one_sample(</span><br><span class="line">                    self.get_picture(content, index)))</span><br><span class="line">        <span class="keyword">return</span> data_set</span><br><span class="line"><span class="comment"># 标签数据加载器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelLoader</span><span class="params">(Loader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        加载数据文件，获得全部样本的标签向量</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        content = self.get_file_content()</span><br><span class="line">        labels = []</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(self.count):</span><br><span class="line">            labels.append(self.norm(content[index + <span class="number">8</span>]))</span><br><span class="line">        <span class="keyword">return</span> labels</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">norm</span><span class="params">(self, label)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        内部函数，将一个值转换为10维标签向量</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        label_vec = []</span><br><span class="line">        label_value = self.to_int(label)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            <span class="keyword">if</span> i == label_value:</span><br><span class="line">                label_vec.append(<span class="number">0.9</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_vec.append(<span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">return</span> label_vec</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_training_data_set</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获得训练数据集</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_loader = ImageLoader(<span class="string">'train-images-idx3-ubyte'</span>, <span class="number">60000</span>)</span><br><span class="line">    label_loader = LabelLoader(<span class="string">'train-labels-idx1-ubyte'</span>, <span class="number">60000</span>)</span><br><span class="line">    <span class="keyword">return</span> image_loader.load(), label_loader.load()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_data_set</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获得测试数据集</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    image_loader = ImageLoader(<span class="string">'t10k-images-idx3-ubyte'</span>, <span class="number">10000</span>)</span><br><span class="line">    label_loader = LabelLoader(<span class="string">'t10k-labels-idx1-ubyte'</span>, <span class="number">10000</span>)</span><br><span class="line">    <span class="keyword">return</span> image_loader.load(), label_loader.load()</span><br></pre></td></tr></table></figure>
<p>网络的输出是一个10维向量，这个向量第$n$个(从0开始编号)元素的值最大，那么$n$就是网络的识别结果。下面是代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_result</span><span class="params">(vec)</span>:</span></span><br><span class="line">    max_value_index = <span class="number">0</span></span><br><span class="line">    max_value = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(vec)):</span><br><span class="line">        <span class="keyword">if</span> vec[i] &gt; max_value:</span><br><span class="line">            max_value = vec[i]</span><br><span class="line">            max_value_index = i</span><br><span class="line">    <span class="keyword">return</span> max_value_index</span><br></pre></td></tr></table></figure>
<p>我们使用错误率来对网络进行评估，下面是代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(network, test_data_set, test_labels)</span>:</span></span><br><span class="line">    error = <span class="number">0</span></span><br><span class="line">    total = len(test_data_set)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(total):</span><br><span class="line">        label = get_result(test_labels[i])</span><br><span class="line">        predict = get_result(network.predict(test_data_set[i]))</span><br><span class="line">        <span class="keyword">if</span> label != predict:</span><br><span class="line">            error += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> float(error) / float(total)</span><br></pre></td></tr></table></figure>
<p>最后实现我们的训练策略：每训练10轮，评估一次准确率，当准确率开始下降时终止训练。下面是代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_evaluate</span><span class="params">()</span>:</span></span><br><span class="line">    last_error_ratio = <span class="number">1.0</span></span><br><span class="line">    epoch = <span class="number">0</span></span><br><span class="line">    train_data_set, train_labels = get_training_data_set()</span><br><span class="line">    test_data_set, test_labels = get_test_data_set()</span><br><span class="line">    network = Network([<span class="number">784</span>, <span class="number">300</span>, <span class="number">10</span>])</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        epoch += <span class="number">1</span></span><br><span class="line">        network.train(train_labels, train_data_set, <span class="number">0.3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'%s epoch %d finished'</span> % (now(), epoch)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            error_ratio = evaluate(network, test_data_set, test_labels)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'%s after epoch %d, error ratio is %f'</span> % (now(), epoch, error_ratio)</span><br><span class="line">            <span class="keyword">if</span> error_ratio &gt; last_error_ratio:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                last_error_ratio = error_ratio</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    train_and_evaluate()</span><br></pre></td></tr></table></figure>
<p>在我的机器上测试了一下，1个epoch大约需要9000多秒，所以要对代码做很多的性能优化工作（比如用向量化编程）。训练要很久很久，可以把它上传到服务器上，在tmux的session里面去运行。为了防止异常终止导致前功尽弃，我们每训练10轮，就把获得参数值保存在磁盘上，以便后续可以恢复。(代码略)</p>
<h2 id="9-向量化编程"><a href="#9-向量化编程" class="headerlink" title="9 向量化编程"></a>9 向量化编程</h2><blockquote>
<p>完整代码请参考GitHub: <a href="https://github.com/hanbt/learn_dl/blob/master/fc.py" target="_blank" rel="noopener">GitHub-fc</a> (python2.7)</p>
</blockquote>
<p>在经历了漫长的训练之后，我们可能会想到，肯定有更好的办法！是的，程序员们，现在我们需要告别面向对象编程了，转而去使用另外一种更适合深度学习算法的编程方式：向量化编程。主要有两个原因：一个是我们事实上并不需要真的去定义Node、Connection这样的对象，直接把数学计算实现了就可以了；另一个原因，是底层算法库会针对向量运算做优化（甚至有专用的硬件，比如GPU），程序效率会提升很多。所以，在深度学习的世界里，我们总会想法设法的把计算表达为向量的形式。我相信优秀的程序员不会把自己拘泥于某种（自己熟悉的）编程范式上，而会去学习并使用最为合适的范式。</p>
<p>下面，我们用向量化编程的方法，重新实现前面的全连接神经网络。</p>
<p>首先，我们需要把所有的计算都表达为向量的形式。对于全连接神经网络来说，主要有三个计算公式。</p>
<p>前向计算，我们发现式2已经是向量化的表达了：</p>
<p>$$\vec{a}=\sigma(W\centerdot\vec{x})\qquad (式2)$$</p>
<p>上式中的$\sigma$表示sigmoid函数。</p>
<p>反向计算，我们需要把式3和式4使用向量来表示：</p>
<p>$$\vec{\delta}=\vec{y}(1-\vec{y})(\vec{t}-\vec{y})\qquad(式7)\<br>\vec{\delta^{(l)}}=\vec{a}^{(l)}(1-\vec{a}^{(l)})W^T\delta^{(l+1)}\qquad(式8)$$</p>
<p>在式8中，$\delta^{(l)}$表示第$l$层的误差项；$W^T$表示矩阵$W$的转置。</p>
<p>我们还需要权重数组$W$和偏置项b的梯度计算的向量化表示。也就是需要把式5使用向量化表示：</p>
<p>$$w<em>{ji}\gets w</em>{ji}+\eta\delta<em>jx</em>{ji}\qquad(式5)$$</p>
<p>其对应的向量化表示为：</p>
<p>$$W \gets W + \eta\vec{\delta}\vec{x}^T\qquad(式9)$$</p>
<p>更新偏置项的向量化表示为：</p>
<p>$$\vec{b} \gets \vec{b} + \eta\vec{\delta}\qquad(式10)$$</p>
<p>现在，我们根据上面几个公式，重新实现一个类：FullConnectedLayer。它实现了全连接层的前向和后向计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全连接层实现类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullConnectedLayer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, output_size, </span></span></span><br><span class="line"><span class="function"><span class="params">                 activator)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        构造函数</span></span><br><span class="line"><span class="string">        input_size: 本层输入向量的维度</span></span><br><span class="line"><span class="string">        output_size: 本层输出向量的维度</span></span><br><span class="line"><span class="string">        activator: 激活函数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.output_size = output_size</span><br><span class="line">        self.activator = activator</span><br><span class="line">        <span class="comment"># 权重数组W</span></span><br><span class="line">        self.W = np.random.uniform(<span class="number">-0.1</span>, <span class="number">0.1</span>,</span><br><span class="line">            (output_size, input_size))</span><br><span class="line">        <span class="comment"># 偏置项b</span></span><br><span class="line">        self.b = np.zeros((output_size, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 输出向量</span></span><br><span class="line">        self.output = np.zeros((output_size, <span class="number">1</span>))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_array)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        前向计算</span></span><br><span class="line"><span class="string">        input_array: 输入向量，维度必须等于input_size</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 式2</span></span><br><span class="line">        self.input = input_array</span><br><span class="line">        self.output = self.activator.forward(</span><br><span class="line">            np.dot(self.W, input_array) + self.b)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, delta_array)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        反向计算W和b的梯度</span></span><br><span class="line"><span class="string">        delta_array: 从上一层传递过来的误差项</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 式8</span></span><br><span class="line">        self.delta = self.activator.backward(self.input) * np.dot(</span><br><span class="line">            self.W.T, delta_array)</span><br><span class="line">        self.W_grad = np.dot(delta_array, self.input.T)</span><br><span class="line">        self.b_grad = delta_array</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, learning_rate)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        使用梯度下降算法更新权重</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.W += learning_rate * self.W_grad</span><br><span class="line">        self.b += learning_rate * self.b_grad</span><br></pre></td></tr></table></figure>
<p>上面这个类一举取代了原先的Layer、Node、Connection等类，不但代码更加容易理解，而且运行速度也快了几百倍。</p>
<p>现在，我们对Network类稍作修改，使之用到FullConnectedLayer：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sigmoid激活函数类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SigmoidActivator</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, weighted_input)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-weighted_input))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, output)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> output * (<span class="number">1</span> - output)</span><br><span class="line"><span class="comment"># 神经网络类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        构造函数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(layers) - <span class="number">1</span>):</span><br><span class="line">            self.layers.append(</span><br><span class="line">                FullConnectedLayer(</span><br><span class="line">                    layers[i], layers[i+<span class="number">1</span>],</span><br><span class="line">                    SigmoidActivator()</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        使用神经网络实现预测</span></span><br><span class="line"><span class="string">        sample: 输入样本</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        output = sample</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            layer.forward(output)</span><br><span class="line">            output = layer.output</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, labels, data_set, rate, epoch)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        训练函数</span></span><br><span class="line"><span class="string">        labels: 样本标签</span></span><br><span class="line"><span class="string">        data_set: 输入样本</span></span><br><span class="line"><span class="string">        rate: 学习速率</span></span><br><span class="line"><span class="string">        epoch: 训练轮数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch):</span><br><span class="line">            <span class="keyword">for</span> d <span class="keyword">in</span> range(len(data_set)):</span><br><span class="line">                self.train_one_sample(labels[d], </span><br><span class="line">                    data_set[d], rate)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_one_sample</span><span class="params">(self, label, sample, rate)</span>:</span></span><br><span class="line">        self.predict(sample)</span><br><span class="line">        self.calc_gradient(label)</span><br><span class="line">        self.update_weight(rate)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_gradient</span><span class="params">(self, label)</span>:</span></span><br><span class="line">        delta = self.layers[<span class="number">-1</span>].activator.backward(</span><br><span class="line">            self.layers[<span class="number">-1</span>].output</span><br><span class="line">        ) * (label - self.layers[<span class="number">-1</span>].output)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers[::<span class="number">-1</span>]:</span><br><span class="line">            layer.backward(delta)</span><br><span class="line">            delta = layer.delta</span><br><span class="line">        <span class="keyword">return</span> delta</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_weight</span><span class="params">(self, rate)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            layer.update(rate)</span><br></pre></td></tr></table></figure>
<p>现在，Network类也清爽多了，用我们的新代码再次训练一下MNIST数据集吧。</p>
<h2 id="10-小结"><a href="#10-小结" class="headerlink" title="10 小结"></a>10 小结</h2><p>至此，你已经完成了又一次漫长的学习之旅。你现在应该已经明白了神经网络的基本原理，高兴的话，你甚至有能力去动手实现一个，并用它解决一些问题。如果感到困难也不要气馁，这篇文章是一个重要的分水岭，如果你完全弄明白了的话，在真正的『小白』和装腔作势的『大牛』面前吹吹牛是完全没有问题的。</p>
<p>作为深度学习入门的系列文章，本文也是上半场的结束。在这个半场，你掌握了机器学习、神经网络的基本概念，并且有能力去动手解决一些简单的问题（例如手写数字识别，如果用传统的观点来看，其实这些问题也不简单）。而且，一旦掌握基本概念，后面的学习就容易多了。</p>
<p>在下半场，我们讲介绍更多『深度』学习的内容，我们已经讲了神经网络(Neutrol Network)，但是并没有讲深度神经网络(Deep Neutrol Network)。Deep会带来更加强大的能力，同时也带来更多的问题。如果不理解这些问题和它们的解决方案，也不能说你入门了『深度』学习。</p>
<p>目前业界有很多开源的神经网络实现，它们的功能也要强大的多，因此你并不需要事必躬亲的去实现自己的神经网络。我们在上半场不断的从头发明轮子，是为了让你明白神经网络的基本原理，这样你就能非常迅速的掌握这些工具。在下半场的文章中，我们改变了策略：不会再去从头开始去实现，而是尽可能应用现有的工具。</p>
<p>下一篇文章，我们介绍不同结构的神经网络，比如鼎鼎大名的卷积神经网络，它在图像和语音领域已然创造了诸多奇迹，在自然语言处理领域的研究也如火如荼。某种意义上说，它的成功大大提升了人们对于深度学习的信心。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/12/19/线性单元和梯度下降/" rel="next" title="深度学习系列（2）">
                <i class="fa fa-chevron-left"></i> 深度学习系列（2）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Gaochao</p>
              <p class="site-description motion-element" itemprop="description">Python NLP</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#神经网络和反向传播算法"><span class="nav-number">1.</span> <span class="nav-text"><a href="#&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x548C;&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7B97;&#x6CD5;" class="headerlink" title="&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x548C;&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7B97;&#x6CD5;"></a>&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x548C;&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7B97;&#x6CD5;</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-神经元"><span class="nav-number">1.1.</span> <span class="nav-text"><a href="#1-&#x795E;&#x7ECF;&#x5143;" class="headerlink" title="1. &#x795E;&#x7ECF;&#x5143;"></a>1. &#x795E;&#x7ECF;&#x5143;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-神经网络是啥"><span class="nav-number">1.2.</span> <span class="nav-text"><a href="#2-&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x662F;&#x5565;" class="headerlink" title="2. &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x662F;&#x5565;"></a>2. &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x662F;&#x5565;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-计算神经网络的输出"><span class="nav-number">1.3.</span> <span class="nav-text"><a href="#3-&#x8BA1;&#x7B97;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8F93;&#x51FA;" class="headerlink" title="3. &#x8BA1;&#x7B97;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8F93;&#x51FA;"></a>3. &#x8BA1;&#x7B97;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8F93;&#x51FA;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-神经网络的矩阵表示"><span class="nav-number">1.4.</span> <span class="nav-text"><a href="#4-&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x77E9;&#x9635;&#x8868;&#x793A;" class="headerlink" title="4. &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x77E9;&#x9635;&#x8868;&#x793A;"></a>4. &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x77E9;&#x9635;&#x8868;&#x793A;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-神经网络的训练"><span class="nav-number">1.5.</span> <span class="nav-text"><a href="#5-&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;" class="headerlink" title="5. &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;"></a>5. &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BAD;&#x7EC3;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-反向传播算法-Back-Propagation"><span class="nav-number">1.5.1.</span> <span class="nav-text"><a href="#5-1-&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7B97;&#x6CD5;-Back-Propagation" class="headerlink" title="5.1 &#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7B97;&#x6CD5;(Back Propagation)"></a>5.1 &#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7B97;&#x6CD5;(Back Propagation)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-反向传播算法的推导"><span class="nav-number">1.5.2.</span> <span class="nav-text"><a href="#5-2-&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7B97;&#x6CD5;&#x7684;&#x63A8;&#x5BFC;" class="headerlink" title="5.2 &#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7B97;&#x6CD5;&#x7684;&#x63A8;&#x5BFC;"></a>5.2 &#x53CD;&#x5411;&#x4F20;&#x64AD;&#x7B97;&#x6CD5;&#x7684;&#x63A8;&#x5BFC;</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-输出层权值训练"><span class="nav-number">1.5.2.1.</span> <span class="nav-text"><a href="#5-2-1-&#x8F93;&#x51FA;&#x5C42;&#x6743;&#x503C;&#x8BAD;&#x7EC3;" class="headerlink" title="5.2.1 &#x8F93;&#x51FA;&#x5C42;&#x6743;&#x503C;&#x8BAD;&#x7EC3;"></a>5.2.1 &#x8F93;&#x51FA;&#x5C42;&#x6743;&#x503C;&#x8BAD;&#x7EC3;</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-2-隐藏层权值训练"><span class="nav-number">1.5.2.2.</span> <span class="nav-text"><a href="#5-2-2-&#x9690;&#x85CF;&#x5C42;&#x6743;&#x503C;&#x8BAD;&#x7EC3;" class="headerlink" title="5.2.2 &#x9690;&#x85CF;&#x5C42;&#x6743;&#x503C;&#x8BAD;&#x7EC3;"></a>5.2.2 &#x9690;&#x85CF;&#x5C42;&#x6743;&#x503C;&#x8BAD;&#x7EC3;</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-神经网络的实现"><span class="nav-number">1.6.</span> <span class="nav-text"><a href="#6-&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x5B9E;&#x73B0;" class="headerlink" title="6 &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x5B9E;&#x73B0;"></a>6 &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x5B9E;&#x73B0;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-梯度检查"><span class="nav-number">1.7.</span> <span class="nav-text"><a href="#7-&#x68AF;&#x5EA6;&#x68C0;&#x67E5;" class="headerlink" title="7 &#x68AF;&#x5EA6;&#x68C0;&#x67E5;"></a>7 &#x68AF;&#x5EA6;&#x68C0;&#x67E5;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-神经网络实战——手写数字识别"><span class="nav-number">1.8.</span> <span class="nav-text"><a href="#8-&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x5B9E;&#x6218;&#x2014;&#x2014;&#x624B;&#x5199;&#x6570;&#x5B57;&#x8BC6;&#x522B;" class="headerlink" title="8 &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x5B9E;&#x6218;&#x2014;&#x2014;&#x624B;&#x5199;&#x6570;&#x5B57;&#x8BC6;&#x522B;"></a>8 &#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x5B9E;&#x6218;&#x2014;&#x2014;&#x624B;&#x5199;&#x6570;&#x5B57;&#x8BC6;&#x522B;</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-超参数的确定"><span class="nav-number">1.8.1.</span> <span class="nav-text"><a href="#8-1-&#x8D85;&#x53C2;&#x6570;&#x7684;&#x786E;&#x5B9A;" class="headerlink" title="8.1 &#x8D85;&#x53C2;&#x6570;&#x7684;&#x786E;&#x5B9A;"></a>8.1 &#x8D85;&#x53C2;&#x6570;&#x7684;&#x786E;&#x5B9A;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-模型的训练和评估"><span class="nav-number">1.8.2.</span> <span class="nav-text"><a href="#8-2-&#x6A21;&#x578B;&#x7684;&#x8BAD;&#x7EC3;&#x548C;&#x8BC4;&#x4F30;" class="headerlink" title="8.2 &#x6A21;&#x578B;&#x7684;&#x8BAD;&#x7EC3;&#x548C;&#x8BC4;&#x4F30;"></a>8.2 &#x6A21;&#x578B;&#x7684;&#x8BAD;&#x7EC3;&#x548C;&#x8BC4;&#x4F30;</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-代码实现"><span class="nav-number">1.8.3.</span> <span class="nav-text"><a href="#8-3-&#x4EE3;&#x7801;&#x5B9E;&#x73B0;" class="headerlink" title="8.3 &#x4EE3;&#x7801;&#x5B9E;&#x73B0;"></a>8.3 &#x4EE3;&#x7801;&#x5B9E;&#x73B0;</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-向量化编程"><span class="nav-number">1.9.</span> <span class="nav-text"><a href="#9-&#x5411;&#x91CF;&#x5316;&#x7F16;&#x7A0B;" class="headerlink" title="9 &#x5411;&#x91CF;&#x5316;&#x7F16;&#x7A0B;"></a>9 &#x5411;&#x91CF;&#x5316;&#x7F16;&#x7A0B;</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-小结"><span class="nav-number">1.10.</span> <span class="nav-text"><a href="#10-&#x5C0F;&#x7ED3;" class="headerlink" title="10 &#x5C0F;&#x7ED3;"></a>10 &#x5C0F;&#x7ED3;</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Gaochao</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">19.3k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
